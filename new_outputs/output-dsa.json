{
  "team_id": "aline123",
  "items": [
    {
      "team_id": "",
      "items": [
        {
          "title": "In Defense of Coding Interviews",
          "content": "HomeNil MamanoAboutResearchProjectsContactMedia KitPersonalBlogToggle theme← Back to all postsIn Defense of Coding InterviewsApril 16, 2025There is already a lot of discourse about everything wrong with coding interviews. Indeed, one of the first chapters in Beyond Cracking the Coding Interview is What's Broken About Coding Interviews? (it's one of the sneak peek free chapters in bctci.co/free-chapters). Here, I want to collect all the arguments for the contrary view: that there are no clear better alternatives to coding interviews at Big Tech companies. Disclaimers: I am one of the authors of Beyond Cracking the Coding Interview, a prep book for coding interviews. Thus, I am vested in coding interviews not going away. I love leetcoding and specialized in DS&A for my PhD, so I haven't personally experienced the dread that most people feel grinding it. I've been an interviewer at Google in the past, but I'm not currently working for Big Tech, and I don't have any inside knowledge. This is just my assessment. This post is only about Big Tech. I don't think coding interviews are a good idea for startups. This post contains \"Strong Opinions, Weakly Held\". I believe everything here, but I'm very receptive to pushback and opposing data. The rationale for coding interviews I think Big Tech companies understand that being cracked at DS&A is not really necessary to be a good SWE. I don't think coding interviews are about that at all. Imagine you are a Big Tech company, like Google. You receive a massive stream of applications, and you have to trim that down to a still large number of hires. Your hiring system needs to be scalable: you need to quickly train many interviewers you need a way to evaluate candidates that minimizes interviewer bias (not your bias, or a specific person's bias, but all the biases of a large, heterogeneous group) So, the first thing you do to scale--in true engineering fashion--is decoupling hiring and team matching. But that means you cannot hire for specific tech or domain experience: You don't know in what team candidates will end up, and your teams use a bunch of different languages and tech stacks (and a lot of it is internal anyway, so you definitely can't hire for that). So, you need a competence assessment that is independent of any particulars about the job, much like the role the SAT plays for college admissions. How do you do that? If you are a Big Tech company, what you actually want is candidates who can take any complex software system (that's not part of the candidate's previous expertise) and answer hard questions about it, like what's the best way to add a feature, how to optimize it, or how it should be refactored. In other words, the competence you want to assess is general problem-solving skills, and that's what coding interviews are designed for: you are given a tough problem that you have ideally never seen before (more on this later), and asked to showcase your thought process on how you approach it. When working as intended, I believe it gives more signal about your problem-solving skills and is easier to evaluate impartially than other popular interview formats, like talking about previous experience or take-home assignments. And there's an impartial way to evaluate them, by looking at the optimality of the solution. Yes, there's a lot more to being a SWE than problem-solving skills--and that's why Google also does system design and behavioral interviews, but you still want to hire for this trait. The two crucial flaws: memorization and cheating Hopefully, the rationale above covered one of the most common criticisms of coding interviews: that they do not reflect the day-to-day work of an engineer. Instead, I want to focus on what I think are the two biggest issues with coding interviews: Memorizing an absurd amount of leetcode problems gives you an edge. This is the classic reason why people hate coding interviews with a passion. It has led to an \"arms race\" where candidates have to memorize more and more problems to improve their odds, and interviewers keep asking about more niche topics. At the extreme, coding interviews end up feeling like a lottery, and candidates find prep a soul-sucking waste of time. Cheating has become easy with AI. This is a newer issue that's becoming more prevalent due to the fact that LLMs are pretty good at leetcoding. In real time, a cheater can feed the problem statement to an LLM (without obvious tales like \"select all\"), get a solution, and even a script for what to say. From the company's side, Issue (1) is not much of an issue. It definitely undermines the \"problem-solving\" part of the interview if a candidate is just recalling the question, but, statistically, if they do enough rounds, it's unlikely to happen every round. Some people (not me) also argue that the memorization is even good for the companies because it rewards hard work and dedication. For what it's worth, one thing we hoped to change about the interview prep discourse with BCtCI is that candidates should focus on improving their problem-solving skills rather than memorizing. See, for instance, how we teach binary search or how we approach hard problems. But yes, grinding is still necessary. Issue (1) also means that they'll lose a big chunk of candidates who are great SWEs but won't put up with grinding leetcode or that simply don't perform well under pressure (and, from personal experience, many great developers fall in this group). This sucks from the candidate's perspective, but if you are Google, you receive an overwhelming amount of applications from qualified candidates, so you are more OK with rejecting good candidates than accepting bad ones. Issue (2), on the other hand, has the potential to completely ruin coding interviews from the company's side. I'm seeing a quick rise of stories from frustrated interviewers who interviewed or even hired cheaters who could then not do the job (Exhibit A). I expect to see some kind of systematic response to this from Big Tech, but it's not clear what as of April 2025. This article includes some internal comments from Google execs: \\[Brian\\] Ong \\[Google’s vice president of recruiting\\] said candidates and Google employees have said they prefer virtual job interviews because scheduling a video call is easier than finding a time to meet in available conference rooms. The virtual interview process is about two weeks faster, he added. He said interviewers are instructed to probe candidates on their answers as a way to decipher whether they actually know what they’re talking about. “We definitely have more work to do to integrate how AI is now more prevalent in the interview process,” said Ong. He said his recruiting organization is working with Google’s software engineer steering committee to figure out how the company can refine its interviewing process. “Given we all work hybrid, I think it’s worth thinking about some fraction of the interviews being in person,” Pichai responded. “I think it’ll help both the candidates understand Google’s culture and I think it’s good for both sides.” I thought going back to in-person interviews would be a no-brainer for a company like Google, but my reading of these comments is that they don't seem too bothered for now. ~shrug~ Disclaimer: I haven't worked for a Big Tech company since before AI cheating went viral, so I don't have internal insight into what people in charge of hiring are actually thinking. Two related arguments that I don't subscribe to are (1) that leetcode-style interviews are no longer relevant because AI can solve them, and (2) that LLMs should be allowed during coding interviews because they are allowed on the job. The fact that AI can solve coding questions doesn't change that it still gives you the important signal that you want from humans: algorithmic thinking and general problem-solving skills. We just need humans to not cheat. I'll share my thoughts on how to improve coding interviews to address these issues. First, let's see why I think the alternatives are not better. The problems with the alternatives Take-home assignments Take-home assignments are even more subject to cheating, so that can't be the answer to cheating. Never mind LLMs, you don't even know who did the assignment. But take-home assignments have other flaws: They create an asymmetry between company and candidate, where the company asks for a ton of work from the candidate without putting any effort in. \"Oh, we have way too many candidates we need to filter down to a shortlist? Send all of them a complex task to do over the weekend.\" I prefer a model where both company and candidate have to put in time. I'm more OK with take-home assignments as the final stage of the process. They favor people who are unemployed and/or have a lot of free time to polish the assignment. Previous experience I find this too subjective to give signal about problem-solving skills, and it's more about being a good \"salesperson\". I also think it's more subject to bias: people with a similar background as yours are probably more likely to have similar interests, and thus you may find their side-projects more interesting. Trial periods This makes sense to me in smaller companies, where you find a candidate with the perfect profile for the task at hand. It doesn't scale to Big Tech companies. Other alternatives If there are other alternatives that fulfill the same purpose as coding interviews but don't suffer from the same issues, I'd love to hear about them. One idea I liked is going through a code review during the interview, but it's not clear that (1) it offers as much signal about problem-solving skills, and (2) it is easy to evaluate impartially. How to improve coding interviews Right now, FAANG interviewers focus too much on \"Did they solve the question or not?\" That's because they don't get much training on how to interview well (if at all), and it's the most straightforward way to pass on a hire/no hire recommendation to the hiring committee. This leads to many interviewers just pasting the prompt in and mostly sitting in silence. This is the ideal scenario for cheaters. The obvious things There are obvious ways to improve this situation: In-person interviews. These have other benefits, like allowing the candidate to get a better sense of the company culture. Not using publicly available questions, and actively scanning for leaks. Cheating detection software (privacy is a concern here -- would it be too crazy for a company to ship a laptop to the candidate just for the interview?). Stop asking questions that require knowing some niche trick that a normal person wouldn't be able to figure out on the spot. Those reinforce a focus on memorization. Low effort ways of countering cheating I also think that measures designed to throw LLMs off could be effective (at least in the short term) and require minimal effort, such as: Stating the question, or part of it, instead of writing the whole thing down Including a 'decoy' question and telling the candidate, \"Ignore that line, it is part of our anti-cheating measures.\" See LinkedIn discussion. A fundamental tradeoff Perhaps the most effective way to counter both memorization and cheating is to make coding interviews more open ended and conversational. To use a chess analogy, a cheater may make a great move, but if you ask them to explain why they did it, they may not be able to. The interviewer can use a coding question as a launching point, but then drill down on technical topics as they come up. So, e.g., if a candidate chooses to use a heap, the interviewer could go into: What made you think of using a heap? What properties are important for this problem? What are the tradeoffs of using a heap vs binary search trees? How would you go about implementing a heap that supports arbitrary priorities? Why is heapify faster than inserting one by one? If interviewers did that, it wouldn't even be necessary to ask tricky questions. They could even ask Fibonacci. The problem is that, the more open ended the interview is, the more difficult it is to evaluate candidates systematically. To start, you'd need better interviewers and better interviewer training. However, it seems to me that there is a fundamental tradeoff between how objective the evaluation is and how gameable the interview is by memorizing or cheating. I don't have a good solution to this--I would love to hear yours. More good things about coding interviews Only one thing to study An underrated upside of leetcode interviews is that you only need to study one thing for all the big companies. I feel like if every company asked different things, interview prep time would decrease for any specific company but increase overall. In fact, a likely outcome of the push for fewer leetcode-style interviews is an even worse compromise: coding interviews won't completely go away, so you'll still need to grind leetcode, but you'll also have to prep a bunch of specialized stuff for each company on top of that. See LinkedIn discussion. They are not based on pedigree Coding interviews act as a form of standardized testing, similar to the role of SAT for college admissions in the US. And, much like the SAT allows high-school students from all backgrounds to attend top colleges, coding interviews allow candidates from all backgrounds to get at the top companies. The leetcode grind is the same for everyone. If we kill coding interviews without a good alternative, it seems inevitable that Big Tech companies will give more weight to resume and referrals. We all agree that's a bad thing. Final thoughts The best question we got in our Reddit AMA for BCtCI was whether we'd use coding interviews ourselves if we were in charge of hiring. You can see Gayle's, Mike's (mikemroczka.com), and my answers. We all said no in its current form, but yes with caveats/improvements. My favorite answer was Mike's. He's less of a proponent of leetcode-style interviews than I am, but I think he strikes a thoughtful balance between DS&A and practical stuff: Best question so far. Yes, I would ask DS&A questions still, but not exclusively and not difficult ones. Many startups shouldn't ask them though, because most people are bad at discerning what a reasonable question is. I would do 4-5 rounds of interviews because less than that is hard to be significant, but more than that and you're wasting too much of a candidate's time (Netflix has a whopping 8 rounds!!). For a senior engineer role, I'd do something like this. Round 1: An online DS&A assessment to filter out people that can't do the simple things (easy & very simple medium questions only, not hard) Round 2: Live interview of DS&A (simple medium, not hard. essentially just making sure you didn't cheat on the previous round by asking you to explain your answers and code something new from scratch) Round 3: System design (no need for perfect answers, but I'd ask an uncommon question to ensure it was something they hadn't memorized) Round 4: Behavioral, with a focus on cross-team impact. This would just be a simple pass/fail and just a vibe check. It might also be skipped if the prior two rounds had good signal for emotional intelligence Round 5: Remote logging into a server and working on an actual bug that was fixed in our codebase before. There would be no time limit, but time on the server would be logged to weed people out who needed days to complete a simple task. This ends up testing a little bit of theory, practical knowledge, emotional intelligence, and the generalized SWE skillset. Full disclosure. This is my answer. Not the answer of every author. Again, I'd stress that the average startup wouldn't benefit from DS&A and shouldn't be asking them Want to leave a comment? You can post under the linkedin post or the X post.Stay in the loopI'd love to tell you when I publish a new post.Get notified when I write about DS&A or software engineering. Unsubscribe anytime if it's not your vibe.Subscribe nowWant to read more? Here are other posts:Lazy vs Eager AlgorithmsOctober 16, 2024DS&AExploring the tradeoffs between lazy and eager implementations of common algorithms.Read moreWall Game DB DesignMay 12, 2025Wall GameDesigning the DB for the Wall Game.Read moreDouble-Edge Cut ProblemMay 20, 2025ResearchWall GameAn optimal solution for a graph problem that comes up in the Wall Game.Read moreNil MamanoComputer scientist, software engineer, author.LinkedInTwitter© 2025 nilmamano.com. All rights reserved. Last updated May 2025. Privacy policyHome",
          "content_type": "other",
          "source_url": "https://nilmamano.com/blog/in-defense-of-coding-interviews?category=dsa",
          "author": "",
          "user_id": ""
        },
        {
          "title": "Get Binary Search Right Every Time, Explained Without Code",
          "content": "HomeNil MamanoAboutResearchProjectsContactMedia KitPersonalBlogToggle theme← Back to all postsGet Binary Search Right Every Time, Explained Without CodeApril 15, 2025One of the things that makes binary search tricky to implement is that you usually need to tweak the pointer manipulation logic in subtle ways based on the specifics of the problem. E.g., an implementation that works for finding a target in a sorted array when the target is present, may not work if the target is missing. Or, it may not be clear how to tweak the code to find the last occurrence of the target instead of the first one. And of course, there are plenty of less conventional applications of binary search where the input is not an array, like catching bike thieves. In Beyond Cracking the Coding Interview, we wanted to simplify this, so we went looking for a general binary search template. Going into it, I thought we might need at least two templates, but we ended up with just one, which we called the \"transition point recipe\", and which works for every problem we tried, including the 17 problems in the binary search chapter of the book. If you find one where it doesn't work, let me know! The transition point problem Here is the thesis of the transition point recipe: Every binary search problem can be reduced to the 'transition point problem'. In the 'transition point problem', you are given an array with just two values, say 1 and 2, where all the 1s come before the 2s, and you need to point where it changes. E.g., in the array \\[1, 1, 1, 1, 1, 2, 2, 2\\], the last 1 is at index 4 and the first 2 is at index 5. Knowing how to solve this specific problem is key to our recipe. The specific binary search implementation is not important, but there is an invariant we can follow that makes it quite easy: ensure that the left pointer is always at a 1 and the right pointer is always at a 2. We give code in the book, but remembering exact code in an interview is error prone. Instead, the four bullet points below are all I personally remember, and I feel confident that I can derive the rest easily. Start by handling some edge cases: The array is empty Every value is 1 Every value is 2 Initialize two pointers, left and right, to the first and last indices, respectively. For the main binary search loop, always maintain the invariant that the value at left is 1 and the value at right is 2. Let this invariant guide your pointer manipulation logic, so that you don't need to memorize any code. Stop when the left and right pointers are next to each other (i.e., left + 1 == right). Combining the invariant with the stopping condition, we get that, at the end, left will be at the last 1 and right will be at the first 2. These bullet points rely on two ideas to make binary search easier: (1) handling edge cases upfront, and (2) letting strong invariants guide the implementation. Notice how the invariant even guides the edge cases at the beginning, as they are the necessary ones to be able to initialize left and right in a way that satisfies it. The reduction Ok, so now, let's take for granted that we can solve the transition point problem. How does this help us solve other binary search problems? The idea is to come up with a (problem-specific) predicate, like < target, >= target, or x % 2 == 0, which splits the search range into two regions, the \"before\" region and the \"after\" region. This predicate is a function that takes an element of the search range and returns a boolean, and -- as you probably saw coming -- it is key that all the elements with true values come before the elements with false values (or the other way around). Then, we can use the solution to the transition point problem to find the transition point between the 'before' and 'after' regions. The only difference is that, instead of checking boolean values directly, we check the result of the predicate. You can even wrap the predicate in a function, which we called is\\_before(x) in the book, which tells you whether a given element is in the 'before' region. Then, it's really obvious that we are just solving the transition point problem every time. The only part that requires some thinking is choosing the right transition point. For example: if we want to find the first occurrence of target in a sorted array, we can use is\\_before(x) = x < target, which means that, if target is present, the first occurrence is the first element in the 'after' region (so, we can check/return the right pointer at the end). if we want to find the last occurrence of target in a sorted array, we can use is\\_before(x) = x <= target, which means that, if target is present, the last occurrence is the last element in the 'before' region (so, we can check/return the left pointer at the end). And so on for other problems. Practice You can try the transition-point recipe on all the problems from the binary search chapter of the book online at start.interviewing.io/beyond-ctci/part-vii-catalog/binary-search, even if you don't have the book. There, you can also find all our solutions using the recipe, in Python, JS, Java, and C++. By the way, the binary search chapter of the book is free -- it's in bctci.co/free-chapters. Want to leave a comment? You can post under the linkedin post or the X post.Stay in the loopI'd love to tell you when I publish a new post.Get notified when I write about DS&A or software engineering. Unsubscribe anytime if it's not your vibe.Subscribe nowWant to read more? Here are other posts:My family during the Spanish Civil WarMay 1, 2025PersonalMy grandparents' story during the Spanish Civil War.Read moreWall Game UI Design (+ Frontend Generators)May 2, 2025Wall GameThe specs of the UI for the Wall Game and renders from frontend generators.Read moreWhat Vibe Coding Actually Looks Like (prompts included)March 23, 2025SWEThe exact prompts used to create an interactive 3D torus visualization app with vibe coding.Read moreNil MamanoComputer scientist, software engineer, author.LinkedInTwitter© 2025 nilmamano.com. All rights reserved. Last updated May 2025. Privacy policyHome",
          "content_type": "other",
          "source_url": "https://nilmamano.com/blog/binary-search?category=dsa",
          "author": "",
          "user_id": ""
        },
        {
          "title": "Problem Solving BCtCI Style",
          "content": "HomeNil MamanoAboutResearchProjectsContactMedia KitPersonalBlogToggle theme← Back to all postsProblem Solving BCtCI StyleMarch 19, 2025Here's a thought: You don't want the first time you think about the question \"What should I do if I get stuck in a coding interview?\" to be when you are stuck in a coding interview. In a way, getting stuck in a coding interview is an opportunity. The main goal of the interview is to see your problem-solving thought process, and being stuck is the ideal time to showcase it. But you want to be prepared. It's valuable to have a plan for this exact scenario. We all dread blanking out in an interview, but having a plan makes it easy to simply focus on executing it. So, let's talk about what such a plan could look like in this blog post. In Beyond Cracking the Coding Interview, we go over all the steps in an interview, and our best tips to do well in each of them: In this blog post, I'll zoom in on the problem-solving step, \"Design the Algorithm,\" and illustrate the thought process with a problem. As you can see, we break it down into four steps: Minimally sketch the naive solution to establish a baseline. Identify upper and lower bounds using big O analysis to narrow down the range of possible solutions. Look for triggers (Keywords) that point to a specific approach. Employ boosters: problem-solving strategies that give you the \"boost\" you need when you are stuck. These are not revolutionary ideas -- it's what good problem solvers do and think about instinctively. One of the main goals of the book, and of this blog post, is to spell out the thought process of people who are really good at this in a relatable way so that anyone can reproduce it. We playfully call this the MIKE template (Minimally sketch brute force, Identify bounds, Keywords (triggers), Employ boosters) after Mike Mroczka, one of the authors of BCtCI. Rather than expanding on these now, we'll see them in action with the following problem. Problem Statement The problem is based on LeetCode 3458, which appeared in a recent contest. You can go and give it a try before reading on (it's labeled as medium, but I think it's on the harder end of medium). The thought process I'll walk through here is based on how I solved it during the contest. Given a string s, a substring of s is special if any character in it does not appear outside it. For example, if s is \"abcba\": \"bcb\" is a special substring because 'b' and 'c' do not appear in s outside \"bcb\". \"abc\" is not a special substring because 'a' appears in s outside \"abc\". Given a string s consisting of n lowercase English letters, determine the maximum number of disjoint special substrings. Two substrings are disjoint if they do not overlap. Example 1: s = \"abcba\" Output: 1 The special substrings are \"abcba\", \"bcb\", and \"c\". They all overlap with each other, so we can only pick 1. Example 2: s = \"banana\" Output: 2 The special substrings are \"b\", \"banana\", and \"anana\". We can pick \"b\" and \"anana\". Constraints: 2 <= n <= 10^5 s consists only of lowercase English letters. Digesting the problem First, we need to digest what the problem is asking. This problem follows a common pattern: it introduces a kind of esoteric definition, \"special substring\", and then asks us to do something with it. To make sure we understand what a special substring is, it's good to look at a few examples, starting with the provided ones. For instance, in \"abcba\", do you understand why \"a\" is not special but \"c\" is? Take some time to come up with your own examples. Rushing to solving a problem before understanding it well is a common but often costly mistake. Approach Sometimes, it helps to tackle just one part of the problem first, so we can start making progress. We can think of an algorithm with 2 parts: Part A: Find all the special substrings. Part B: Find the most non-overlapping special substrings. Let's start with part A. Part A: Find all the special substrings We'll walk through the MIKE template. M: Minimally sketch brute force The key here is to not overthink it. We just want to get the ball rolling and have a baseline we can improve upon. Since we don't want to spend too much time in an interview, you could even just describe the idea in a sentence and move on. But we prefer to briefly sketch it in very high-level pseudocode. We call it 'intended English': it's written like English, but with indentation to show the code structure: Algo 1: brute force T: O(n^4) for each possible substring start for each possible substring end # check if it is special for each letter inside the substring for each letter outside the substring if they match, it is not special Interviews often involve considering trade offs between algorithms, so it's a good habit to give them names and list their time/space complexity. In this case, the space complexity depends on how many special substrings we might find, which is not clear yet, so we'll leave it out for now. Sketching the brute force solution helps us ensure we understand the problem (and if we are solving for the wrong thing, we give the interviewer a chance to let us know). I: Identify upper and lower bounds We can use big O analysis to narrow down the range of possible solutions. An upper bound means \"we don't have to consider any solution that takes longer than this\", and a lower bound means the opposite: \"we don't have to consider any solution that takes less time than this\". In the book, we go over two ways of establishing an upper bound and two ways of establishing a lower bound: Upper bounds: Brute force upper bound: we just saw that we can find all special substrings in O(n^4) time, so we don't have to consider any solution that takes longer than that. TLE (Time Limit Exceeded) upper bound: here is where we use the problem constraints to establish an upper bound. The problem says that n <= 10^5, which usually means that O(n^2) solutions are too slow, but O(n log n) or faster solutions are fine. Lower bounds: Output-size lower bound: the space taken by the output is a lower bound for the time complexity, because that's how long it takes just to write the output. In our case, the output of the overall problem is just a number, so this lower bound is trivial: O(1). Bounds are not always useful! Task-based lower bound: some problems involve an inherent task that any solution must fulfill. The runtime of this task is a lower bound. In this case, we know we at least need to read every letter in the input, so we have a lower bound of O(n). In other words, we can rule out solutions that take O(log n) or O(1) time. Combining our findings, we can narrow down our search range to O(n log n) or O(n) algorithms (something like O(n log^2 n) would also be fine, it's just less common). K: Keywords (triggers) There are certain properties of problems that point to a specific approach. Here are some triggers we can identify for this problem: finding substrings -> sliding windows O(n log n) possible target complexity -> sorting or heaps Unfortunately, triggers are not a guarantee, and these triggers don't seem to help for this problem: In sliding windows, once you move past a character, you don't later go back. So, in Example 1, it would be impossible to find both \"abcba\" and \"bcb\": if you find \"abcba\" first, the right pointer would have to go back to find \"bcb\". But if you find \"bcb\" first, the left pointer would have to go back to find \"abcba\". Sorting doesn't seem like a good fit because the input order is important. Do you think I missed any other triggers? E: Employ boosters So, triggers didn't help, and brute force is still far from the target complexity. It's time to employ boosters. Here's an overview: The boosters are roughly ordered, but we don't always have to use them in order. In fact, here's a plot twist: what we did at the beginning, splitting the problem into two parts, is the third booster: Decrease the Difficulty -> Break Down the Problem. Booster 1: Brute force optimization The first booster is straightforward: take the brute force pseudocode we already have and try to optimize it. In the boosters diagram, we list three ways to go about it. One of them is the Data structure pattern. Many bottlenecks come from having to do some calculation inside a loop. In those situations, ask yourself, \"Do I know of any data structure which makes this type of operation faster?\" For this problem, we can use a hash set to optimize the innermost loop: Algo 2: set optimization T: O(n^3) for each possible substring start for each possible substring end # check if it is special dump the substring into a set for each letter outside the substring if it is in the set, it is not special If you have working code or pseudocode but think of an optimization or better approach, do NOT edit your code. Copy-paste it and work on a separate copy. This way, if you don't have time to finish or realize it's wrong, you'll still have the previous working version. Booster 2: Hunting for properties We got down to O(n^3) time, but we know we still need to bring this down to the target complexity. Let's say we don't know how to optimize the code further. Often, the breakthrough comes from uncovering some \"hidden\" observation or property not explicitly mentioned in the statement. Our second booster is to go hunting for those. In the book, we discuss a bunch of ways of doing this, but the most basic and effective one is to try to solve the problem manually with a non-trivial example. By non-trivial, we mean that is is not some weird edge case, which would not be helpful for figuring out a general algorithm. Let's actually do that: take s = \"mississippi\" and manually try to find all the special substrings. Don't overthink it. Don't think about algorithms yet. Just write them down. Done? Ok, now try to reverse-engineer what shortcuts your brain took. This is one property you may have noticed: Property 1 Property 1: a special substring must start at the first occurrence of a letter. You may have noticed this property when your brain skipped over the second, third, or fourth 'i's in mississippi and intuitively realized that there is no special substring starting at those. Writing down the property formalizes this instinct and ropes in the interviewer. Now that we have a property, we have to find a way to use it. Property 1 allows us to optimize the outer loop: it means we only have 26 = O(1) possible starts to check (problems where the input consists of only lowercase letters often have optimizations like this). As we iterate through the possible starts, we can track letters seen so far (e.g., in a hash set): Algo 3: selective start T: O(26 \\* n^2) = O(n^2) for each possible substring start i if seen s\\[i\\] before continue add s\\[i\\] to seen set for each possible substring end # check if it is special dump the substring into a set for each letter outside the substring if it is in the set, it is not special We like to write down the big O simplification (O(26 \\* n^2) = O(n^2)), so the interviewer doesn't think we missed steps. We haven't hit our target time complexity yet, so let's keep hunting for properties. Here is another one: Property 2 Property 2: of all the special substrings that start at a given letter, we only care about the shortest one. Our ultimate goal is to find the most non-overlapping special substrings. If we can choose between two special substrings, one of which contains the other, it is always \"optimal\" or, at least, \"safe\" to pick the smaller one. For instance, if s is \"baa\", we have two choices for special substrings starting at 'b': \"baa\" and \"b\". We should pick \"b\" so that the \"aa\" part can be in another disjoint special substring. Again, when we find a property, we need to think of how to apply it. Property 2 means that, for each starting point i, we can grow a substring one letter at a time, and stop as we find the first special substring. Let's break this down a bit more: say you start at index i. If you find a letter c that appears at some later point, we need to grow the substring up to that index. If you find a letter c that appears before i, we can stop the search. No substring starting at i can be special. For example, imagine i starts at the first 'b' in the following string: \"abbbbbabbba\" ^ i That means we need to grow the substring at least up to the last 'b' in the string: \"abbbbbabbba\" ^ ^ i need to grow up to here As we grow the substring, we hit an 'a', which appears before i, and we realize that no substring starting at i can be special. \"abbbbbabbba\" ^ ^ i invalid We can now add this logic to our algorithm. We can start the algorithm by computing the first and last index of each letter (this is an example of the preprocessing pattern in the boosters diagram -- it's common for properties from Booster 2 to enable optimizations from Booster 1). Then, as we grow each substring, we keep track of the farthest index we need to reach. (This is actually a common pattern in sliding window algorithms, where we maintain information about the window as it 'slides', rather than computing it from scratch every time the window moves. So, the 'sliding windows' trigger wasn't completely off). Algo 4: smallest special substring T: O(26 \\* n) = O(n) S: O(26 \\* n) = O(n) preprocessing: compute the first and last index of each letter for each possible substring start i for each index j starting at i if s\\[j\\] appears before i no special string starts at i else must\\_reach = max(must\\_reach, last occurrence of s\\[j\\]) if j reached must\\_reach: s\\[i\\]...s\\[j\\] is a special substring (the shortest one starting at s\\[i\\]) We got the time down to O(n). Since we hit the lower bound, we can be confident Part A is as good as it can be, and we can move on to Part B. Part B: Find the most non-overlapping special substrings Let's be honest: even if in the book we really emphasize developing your problem-solving skills by using the MIKE template and the boosters, knowing a bunch of leetcode questions DOES give you an edge in coding interviews. So, I'll tell you how I actually solved this problem in the contest. I realized that Part B is just a variation of a classic greedy problem: most non-overlapping intervals. Indeed, a substring can be seen as an interval of the string. The \"most non-overlapping intervals\" problem is in BCtCI, so I already knew that it can be solved with a greedy algorithm that sorts the intervals by their end time and then iterates through them, picking the ones that don't overlap with the previous one (here is a similar problem on leetcode). This algorithm fits within our target time complexity, so I didn't have to think beyond that. If I didn't already know the solution, I would have walked through the MIKE template again for Part B. Full implementation Here is a full implementation: # T: O(26 \\* n) = O(n) # S: O(26 \\* n) = O(n) def select\\_k\\_disjoint\\_special\\_substrings(s): special\\_substrings = find\\_special\\_substrings(s) # Part A return most\\_non\\_overlapping\\_intervals(special\\_substrings) # Part B def find\\_special\\_substrings(s): # Algo 4 # Preprocessing: compute the first and last index of each letter first\\_idx = {} last\\_idx = {} for i, char in enumerate(s): if char not in first\\_idx: first\\_idx\\[char\\] = i last\\_idx\\[char\\] = i special\\_substrings = \\[\\] for i in range(len(s)): if i != first\\_idx\\[s\\[i\\]\\]: continue must\\_reach = i for j in range(i, len(s)): if first\\_idx\\[s\\[j\\]\\] < i: break must\\_reach = max(must\\_reach, last\\_idx\\[s\\[j\\]\\]) if j == must\\_reach: special\\_substrings.append((i, j)) break return special\\_substrings def most\\_non\\_overlapping\\_intervals(intervals): # Classic Greedy intervals.sort(key=lambda x: x\\[1\\]) # Sort by endpoint count = 0 prev\\_end = -math.inf for l, r in intervals: if l > prev\\_end: count += 1 prev\\_end = r return count You may think that the bottleneck is the sorting, but it's not. Recall that there are only up to 26 special substrings (by Property 1). Sorting 26 intervals takes O(26 log 26) = O(1) time. Conclusion I wanted to give an overview of all the high-level ideas for problem-solving in leetcode-style interviews. We could dive a lot deeper into any of those ideas, so this blog post may feel a bit rushed, but the meta-point is that you should have a plan for when you are stuck in an interview (and you should be following it during your practice sessions so it becomes second nature). It's not important that you use the MIKE template -- your plan should work for you. But the ideas covered in this post should probably be part of it. If you have any comments, let me know on linkedin or X.Stay in the loopI'd love to tell you when I publish a new post.Get notified when I write about DS&A or software engineering. Unsubscribe anytime if it's not your vibe.Subscribe nowWant to read more? Here are other posts:Single-Edge Cut ProblemMay 15, 2025ResearchWall GameA linear-time algorithm for a graph problem that comes up in the Wall Game.Read moreWhat Vibe Coding Actually Looks Like (prompts included)March 23, 2025SWEThe exact prompts used to create an interactive 3D torus visualization app with vibe coding.Read moreWall Game DB DesignMay 12, 2025Wall GameDesigning the DB for the Wall Game.Read moreNil MamanoComputer scientist, software engineer, author.LinkedInTwitter© 2025 nilmamano.com. All rights reserved. Last updated May 2025. Privacy policyHome",
          "content_type": "other",
          "source_url": "https://nilmamano.com/blog/problem-solving-bctci-style?category=dsa",
          "author": "",
          "user_id": ""
        },
        {
          "title": "Heapify Analysis Without Math",
          "content": "HomeNil MamanoAboutResearchProjectsContactMedia KitPersonalBlogToggle theme← Back to all postsHeapify Analysis Without MathOctober 30, 2024I'm writing about heaps for Beyond Cracking the Coding Interview (beyondctci.com), and the most technical part is the analysis of heapify. It's easy to show that it takes O(n log n) time, where n is the number of nodes in the heap, but it's not straightforward to show that this is not tight and the method actually takes O(n). time. Every proof I have found online involves a summation over the levels of the heap that ends up looking something like the one in Wikipedia heap page: which is more math than I want to put in this book (the bulk of the audience consists of people trying to land a SWE job, not math enthusiasts). Below is the proof \"without complicated math\" I came up with that heapify takes O(n) time. If you are familiar with the classic proof, let me know if you find it easier - I might use it for the book. Also, please let me know if you've seen someone else proving it in a similar way. If you already know what heapify is, you can jump directly to the Proof. Heap Recap Heaps are binary trees with two special properties: They are complete binary trees: all the levels except the last one have the maximum number of nodes; the last level may not be full, but all the nodes are aligned to the left. (In particular, this implies that heaps have logarithmic height, which is key to the big O analysis.) The heap property: every node is smaller than its children (this is assuming a min-heap - it would be the opposite for a max-heap). I will focus on the heapify operation and its analysis, but if you want to learn heaps from scratch, the Algorithms with Attitude Youtube channel has a great video on it. He also covered the classic linear-time proof for heapify, if you want to compare it to mine. In any case, I left a full Python heap implementation at the bottom of this post. What's Heapify? Heapify (invented by Robert W. Floyd) converts a binary tree which is already complete, but may not have the heap property, into a proper heap. Heapify uses the \"bubble-down\" procedure, which starts at a node that may not satisfy the heap property, and recursively swaps it with the smallest of its two children until the heap property is restored: def bubble\\_down(self, idx): left\\_idx, right\\_idx = left\\_child(idx), right\\_child(idx) is\\_leaf = left\\_idx >= len(self.heap) if is\\_leaf: return # Leaves cannot be bubbled down. # Find the index of the smallest child child\\_idx = left\\_idx if right\\_idx < len(self.heap) and self.heap\\[right\\_idx\\] < self.heap\\[left\\_idx\\]: child\\_idx = right\\_idx if self.heap\\[child\\_idx\\] < self.heap\\[idx\\]: self.heap\\[idx\\], self.heap\\[child\\_idx\\] = self.heap\\[child\\_idx\\], self.heap\\[idx\\] self.bubble\\_down(child\\_idx) Heapify works by \"bubbling down\" every non-leaf (internal) node, from bottom to top: This figure shows the heapify steps for a min-heap. The first tree is the initial state, which doesn't yet have the min-heap property. Leaves are already at the bottom, so bubbling them down has no effect. The next 3 trees show the evolution after bubbling down the two nodes at depth 1 and then the node at depth 0. In the array-based heap implementation, heapify() looks like this: def heapify(self, arr): self.heap = arr for idx in range(len(self.heap) // 2, -1, -1): self.bubble\\_down(idx) The reason why we start bubbling down from the middle of the heap is that, in a complete tree, at least half the nodes are leaves, and we don't need to bubble those down. Here, we won't prove that it works, only that its analysis is O(n). Proof I'll start with a definition and a fact we'll use later: A perfect binary tree is a complete tree where the last level is full: Fact 1: In a perfect tree, the number of leaves is 1 more than the number of internal nodes. For instance: - Height 1: 1 leaf, 0 internal nodes, 1 total - Height 2: 2 leaves, 1 internal node, 3 total - Height 3: 4 leaves, 3 internal nodes, 7 total - Height 4: 8 leaves, 7 internal nodes, 15 total Fact 1 is true because the number of nodes at each level is a power of 2, so: the number of leaves is a power of 2, and the number of internal nodes is the sum of all the previous powers of 2. The sum of the first few powers of 2 add up to one less than the next power of 2. You can see that if you line them up like this: It's a bit like Zeno's paradox, where each power of 2 in the sum halves the remaining distance, but never quite gets to 64. With that out of the way, back to heapify: In the worst case, each node will get bubbled down all the way to a leaf. Thus, each node needs to move down O(log n) levels, so one might reasonably expect heapify to take O(n log n) time. This is correct in the 'upper bound' sense, but not tight: the total time is actually O(n). The intuition for why that is the case is that most nodes are in the deeper levels of the tree, where they don't need to travel a lot to get to the bottom. We'll actually prove a stronger claim: Main Claim: If you heapify a perfect tree, the number of 'bubble-down' swaps is smaller than n, the number of nodes. We'll assume the worst case, in which every node is bubbled down to a leaf position. If the claim is true and heapify does \\= len(self.heap) if is\\_leaf: return # Leaves cannot be bubbled down. # Find the index of the smallest child child\\_idx = left\\_idx if right\\_idx < len(self.heap) and self.heap\\[right\\_idx\\] < self.heap\\[left\\_idx\\]: child\\_idx = right\\_idx if self.heap\\[child\\_idx\\] < self.heap\\[idx\\]: self.heap\\[idx\\], self.heap\\[child\\_idx\\] = self.heap\\[child\\_idx\\], self.heap\\[idx\\] self.bubble\\_down(child\\_idx) def heapify(self, arr): self.heap = arr for idx in range(len(self.heap) // 2, -1, -1): self.bubble\\_down(idx) Stay in the loopI'd love to tell you when I publish a new post.Get notified when I write about DS&A or software engineering. Unsubscribe anytime if it's not your vibe.Subscribe nowWant to read more? Here are other posts:Reachability Problems and DFSJune 21, 2020DS&AAn extensive list of questions that can be solved with DFS.Read moreProblem Solving BCtCI StyleMarch 19, 2025DS&AA problem walkthrough using the concepts from Beyond Cracking the Coding Interview.Read moreWall Game UI Design (+ Frontend Generators)May 2, 2025Wall GameThe specs of the UI for the Wall Game and renders from frontend generators.Read moreNil MamanoComputer scientist, software engineer, author.LinkedInTwitter© 2025 nilmamano.com. All rights reserved. Last updated May 2025. Privacy policyHome",
          "content_type": "other",
          "source_url": "https://nilmamano.com/blog/heapify-analysis?category=dsa",
          "author": "",
          "user_id": ""
        },
        {
          "title": "Lazy vs Eager Algorithms",
          "content": "HomeNil MamanoAboutResearchProjectsContactMedia KitPersonalBlogToggle theme← Back to all postsLazy vs Eager AlgorithmsOctober 16, 2024Warning: I have not tested any code snippet below. Please let me know if you find a bug. Introduction Most algorithms have multiple valid implementations. For instance, in a binay tree problem, you have multiple ways of handling NULL nodes. I'm currently writing Beyond Cracking the Coding Interview (beyondctci.com), which means that my co-authors and I need to take a stance on what version of each algorithm to use. Ideally, we want to show the simplest version of each algorithm: Easy to recall for interview, Easy to explain to interviewers, Easy to debug by hand, Short, so that it is quick to code. In the book, we don't claim that the version we show is \"the best\" - we say to use the one that works best for you. But showing one in the book is an implicit endorsement. One particular decision that comes up again and again with recursive algorithms is choosing between the lazy version and the eager version of an algorithm. An eager recursive function expects 'valid' inputs and ensures to only call the recursive function with 'valid' inputs. We can also call it a clean (call) stack algorithm. A lazy recursive algorithm allows 'invalid' inputs, so it starts by validating the input. Then, it calls the recursive function without validating the inputs passed to it. We can also call it a dirty stack algorithm. What 'valid' means depends on the algorithm--we'll see plenty of examples. We'll also translate the concept of eager vs lazy to iterative algorithms. Lazy vs Eager Tree Traversals An eager tree traversal eagerly validates that the children are not NULL before passing them to the recursive function. A lazy tree traversal doesn't, so it needs to check if the current node is NULL before accessing it. For instance, here is eager vs lazy preorder traversal: class Node: def \\_\\_init\\_\\_(self, val, left=None, right=None): self.val = val self.left = left self.right = right def preorder\\_traversal\\_eager(root): res = \\[\\] # CANNOT be called with node == None def visit(node): res.append(node.val) if node.left: visit(node.left) if node.right: visit(node.right) if not root: return \\[\\] visit(root) return res def preorder\\_traversal\\_lazy(root): res = \\[\\] # CAN be called with node == None def visit(node): if not node: return res.append(node.val) visit(node.left) visit(node.right) visit(root) return res Both have the same runtime and space analysis. Even the constant factors probably don't change much, so it comes down to style preference. Which one do you prefer? Lazy vs Eager graph DFS An eager graph DFS eagerly checks that the neighbors are not already visited before passing them to the recursive function. A lazy graph DFS doesn't, so it needs to check if the current node is already visited. # Returns all nodes reachable from start def dfs\\_eager(adj\\_lists, start): res = \\[\\] visited = set() def visit(node): res.append(node) for neighbor in adj\\_lists\\[node\\]: if neighbor not in visited: visited.add(neighbor) visit(neighbor) visited.add(start) visit(start) return res def dfs\\_lazy(adj\\_lists, start): res = \\[\\] visited = set() def visit(node): if node in visited: return visited.add(node) res.append(node) for neighbor in adj\\_lists\\[node\\]: visit(neighbor) visit(start) return res For a graph DFS, we can also do a mix between lazy and eager: we can eagerly check if nodes are already visited, and lazily mark them as visited: def dfs\\_lazy(adj\\_lists, start): res = \\[\\] visited = set() def visit(node): visited.add(node) res.append(node) for neighbor in adj\\_lists\\[node\\]: if neighbor not in visited: visit(neighbor) visit(start) return res Again, they all have the same analysis. Which one do you prefer? Lazy vs Eager grid algorithms Consider the same DFS algorithm but on a grid of 0's and 1's. The 0's are walkable cells, the 1's are obstacles, and walkable cells next to each other are connected. This time, we need to check that the neighbors are not out of bounds, which we can do lazily or greedily. # Returns all cells reachable from (start\\_row, start\\_col). def grid\\_dfs\\_eager(grid, start\\_row, start\\_col): nr, nc = len(grid), len(grid\\[0\\]) res = \\[\\] visited = set() def visit(row, col): res.append((row, col)) for dir in ((-1, 0), (1, 0), (0, 1), (0, -1)): r, c = row + dir\\[0\\], col + dir\\[1\\] if 0 <= r < nr and 0 <= c < nc and grid\\[r\\]\\[c\\] == 0 and (r, c) not in visited: visited.add((r, c)) visit(r, c) # Assumes (start\\_row, start\\_col) is within bounds visited.add((start\\_row, start\\_col)) visit(start\\_row, start\\_col) return res def grid\\_dfs\\_lazy(grid, start\\_row, start\\_col): nr, nc = len(grid), len(grid\\[0\\]) res = \\[\\] visited = set() def visit(row, col): if row < 0 or row >= nr or col < 0 or col >= nc or grid\\[row\\]\\[col\\] == 1: return if (row, col) in visited: return visited.add((row, col)) res.append((row, col)) for dir in ((-1, 0), (1, 0), (0, 1), (0, -1)): visit(row + dir\\[0\\], col + dir\\[1\\]) visit(start\\_row, start\\_col) return res Lazy vs Eager Memoization DP In a lazy memoization DP (Dynamic Programming) algorithm, we call the recursive function for a subproblem without checking first if we have already computed that subproblem. In an eager algorithm, we only call the recursive function for subproblems that we still need to compute. # Returns all cells reachable from (start\\_row, start\\_col). def fibonacci\\_eager(n): memo = {} def fib\\_rec(i): if i <= 1: return 1 if i-1 in memo: prev = memo\\[i-1\\] else: prevprev = fib\\_rec(i-1) if i-2 in memo: prevprev = memo\\[i-2\\] else: prev = fib\\_rec(i-2) memo\\[i\\] = prev + prevprev return memo\\[i\\] return fib\\_rec(n) def fibonacci\\_lazy(n): memo = {} def fib\\_rec(i): if i <= 1: return 1 if i in memo: return memo\\[i\\] memo\\[i\\] = fib\\_rec(i-1) + fib\\_rec(i-2) return memo\\[i\\] return fib\\_rec(n) For memoization DP, I think lazy is cleaner and more conventional. Lazy vs Eager Iterative Tree traversals Consider a level-order traversal on a binary tree. A level-order traversal is an iterative algorithm that uses a queue data structure. A lazy version puts children in the queue without checking if they are NULL first. We can call it a dirty queue algorithm. An eager version checks for NULL nodes and avoids putting them in the queue. We can call it a clean queue algorithm. def level\\_order\\_traversal\\_eager(root): if not root: return \\[\\] res = \\[\\] Q = deque(\\[root\\]) while Q: node = Q.popleft() res.append(node.val) if node.left: Q.append(node.left) if node.right: Q.append(node.right) return res def level\\_order\\_traversal\\_lazy(root): res = \\[\\] Q = deque(\\[root\\]) while Q: node = Q.popleft() if not node: continue res.append(node.val) Q.append(node.left) Q.append(node.right) return res Eager Graph BFS is better than lazy Graph BFS This is the first exception where one is better than the other in terms of big O analysis. The lazy BFS allows adding already-visited nodes to the queue, while the eager one does not. We'll first look at the two versions, and then analyze them. def graph\\_bfs\\_eager(adj\\_lists, start): res = \\[\\] visited = set() visited.add(start) Q = deque(\\[start\\]) while Q: node = Q.popleft() res.append(node.val) for neighbor in adj\\_lists\\[node\\]: if neighbor not in visited: visited.add(neighbor) Q.append(neighbor) return res def graph\\_bfs\\_lazy(adj\\_lists, start): res = \\[\\] visited = set() Q = deque(\\[start\\]) while Q: node = Q.popleft() if node in visited: continue visited.add(node) res.append(node) for neighbor in adj\\_lists\\[node\\]: Q.append(neighbor) return res It may come as a surprise that these two are not equivalent like all the other examples. Let's say V is the number of nodes and E is the number of edges. To keep things simple, consider that the graph is connected, meaning that E is at least V-1 and at most O(V²). Both versions take O(E) time. The difference is in the space complexity: the eager version takes O(V) space because we never have the same node twice in the queue. The lazy version takes O(E) space because we allow the same nodes multiple times in the queue. To see this, consider a complete graph: When we visit start, we add A, B, C, D, E to the queue. Now the queue is: \\[A, B, C, D, E\\] When we visit A, we add start, B, C, D, E to the queue. Now the queue is: \\[B, C, D, E, start, B, C, D, E\\] When we visit B, we add start, A, C, D, E to the queue. Now the queue is: \\[C, D, E, start, B, C, D, E, start, A, C, D, E\\] And so on. By the time we finish popping the nodes added as neighbors of the start node, we've done V queue pops and V² queue appends, so the queue size is O(V²). So, why didn't this happen for other lazy algorithms we have seen? For tree traversals, each tree node has a single parent that it can be reached from, so we don't need to worry about the same node appearing twice in the call stack or in the level-order traversal queue. For graph DFS, every node in the call stack is marked visited, so if we call visit() on a node that is already in the call stack, we'll immediately return as we'll see it is marked as visited. Eager Dijkstra is better than Lazy Dijkstra, but harder to implement I wrote extensively about different Dijktsra implementations in this Dijkstra blog post. Dijkstra is similar to BFS, with the main difference that it uses a priority queue (PQ) instead of a queue to visit the nodes that are closer first (in terms of shortest paths). In BFS, when a node is added to the queue, its distance from the starting node is already established and there is never a reason to add it again to the queue. In Dijkstra, when a node is added to the PQ, we might later find a shorter path while it is still in the PQ. When that happens, we can do two things: Lazy Dijkstra: just add the node again with the new, improved distance. It will get popped before the previous occurrence because it has higher priority in the PQ. When a node with a \"stale\" distance gets popped off from the queue, we just ignore it. Eager Dijkstra (called textbook Dijkstra in the other blog post): instead of adding the node again, find the existing occurrence of it in the PQ, and update it with the new found distance. This guarantees that the same node never appears twice in the PQ. Both versions take O(E\\*log V) time, but eager is more space efficient, analogously to eager BFS: O(V) for eager Dijkstra vs O(E) for lazy Dijkstra. Here is lazy Dijkstra: def dijkstra\\_lazy(adj\\_lists, start): dist = defaultdict(int) dist\\[start\\] = 0 visited = set() PQ = \\[(0, start)\\] while PQ: \\_, node = heappop(PQ) # Only need the node, not the distance. if node in visited: continue # Not the first extraction. visited.add(node) for neighbor, weight in adj\\_lists\\[node\\]: if dist\\[node\\]+weight < dist\\[neighbor\\]: dist\\[neighbor\\] = dist\\[node\\]+weight # Neighbor may already be in the PQ; we add it anyway. heappush(PQ, (dist\\[neighbor\\], neighbor)) return dist Unfortunately, eager Dijkstra is not so easy to implement in Python because we are missing the decrease\\_key() operation in a heap (and Python does have a self-balancing BST data structure, which can also be used for eager Dijkstra). You can see a BST-based C++ implementation in my other blog post. The dijkstra\\_lazy() algorithm above is more or less standard and it has been known as \"lazy Dijkstra\" for a while. However, it is possible to make an even lazier version which has the same runtime and space analysis (but likely bigger constant factors). The idea is that instead of only adding to the PQ the neighbors for whom we find an improved distance, we can simply add all of them, and discard duplicates once we extract them from the PQ: def dijkstra\\_super\\_lazy(adj\\_lists, start): dist = defaultdict(int) dist\\[start\\] = 0 PQ = \\[(0, s)\\] while PQ: d, node = heappop(PQ) if dist\\[node\\] != math.inf: continue dist\\[node\\] = d for neighbor, weight in adj\\_lists\\[node\\]: heappush(PQ, (dist\\[node\\]+weight, neighbor)) return dist So, Lazy or Eager? We could keep looking at lazy vs eager algorithms, but I'll stop here. In aggregate, these are the pros and cons that I see: Pros of lazy algorithms Lazy algorithms require less code. This is because you only need to validate the parameters of the recursive function once at the beginning, instead of validating what you pass to each recursive call. This is specially true in binary tree problems, where you usually have two recursive calls. It doesn't apply as much for graphs. Lazy algorithms require less indentation. For instance, in graph problems, we don't need to do checks inside the for loop over the neighbors. Lazy algorithms do not require special handling for the first recursive call. You don't need to worry about things like checking if the root is NULL or marking the start node as visited. Lazy recursive functions have simpler preconditions. You can just pass anything to them, and they work. Pros of eager algorithms For a graph BFS, eager has a better space complexity. This is a case where eager is objectively better. (Eager Dijkstra is also better but it is not expected to be implemented in interviews. Your interviewer is probably expecting lazy Dijkstra.) Eager algorithms do fewer recursive calls or iterations. In a binary tree, the number of NULL nodes is always one more than the number of internal nodes. This means that a lazy traversal does twice as many recursive calls/iterations as the eager counterpart. This could make a big difference if you want to debug the code manually. For instance, in this picture, you can see that adding NULLs to the queue makes visualizing the steps more painful: Eager algorithm can 'feel safer'. A friend commented that, with a lazy algorithm, they feel like they are missing an edge case. My preference Here are my personal preferences for coding interviews (not those of the other authors of 'Beyond Cracking the Coding Interview'): Strong preferences: For BFS, use eager. This one is clear cut. For memoization DP, use lazy. It is much cleaner to code. For Dijkstra, use lazy Dijkstra (not super lazy Dijkstra). It is what is realistic to do in an interview and probably what the interviewer expects. Weak preferences: For binary tree traversals (iterative or recursive), use lazy. It is a bit cleaner. For graph DFS, use eager. It is a bit more standard, and aligned with a graph BFS. In the book, we'll definitely mention that some algorithms can be implemented in a lazy or eager way (in way less detail than here), and that you should choose the one that feels easier to you. But, we still need to pick one to show in the problem solutions. One idea is trying to be consistent throughout (e.g., doing all tree and graph traversals in an eager way). If you have an opinion on which one is better, please reach out! I'd love to hear it.Stay in the loopI'd love to tell you when I publish a new post.Get notified when I write about DS&A or software engineering. Unsubscribe anytime if it's not your vibe.Subscribe nowWant to read more? Here are other posts:Lifecycle of a CS research paper: my knight's tour paperMay 2, 2025ResearchThe backstory and thought process behind a fun paper from my PhD.Read moreDouble-Edge Cut ProblemMay 20, 2025ResearchWall GameAn optimal solution for a graph problem that comes up in the Wall Game.Read moreIterative Tree Traversals: A Practical GuideFebruary 4, 2020DS&AA guide to implementing preorder, inorder, and postorder tree traversals iteratively.Read moreNil MamanoComputer scientist, software engineer, author.LinkedInTwitter© 2025 nilmamano.com. All rights reserved. Last updated May 2025. Privacy policyHome",
          "content_type": "other",
          "source_url": "https://nilmamano.com/blog/lazy-vs-eager?category=dsa",
          "author": "",
          "user_id": ""
        },
        {
          "title": "Reachability Problems and DFS",
          "content": "HomeNil MamanoAboutResearchProjectsContactMedia KitPersonalBlogToggle theme← Back to all postsReachability Problems and DFSJune 21, 2020Introduction Depth-first search, or DFS, is a fundamental graph algorithm that can be used to solve reachability problems. This post shows how to adapt the basic DFS template to solve several problems of this kind. Reachability problems are often easier in undirected graphs. Below, we specify if the algorithm works for undirected graphs, directed graphs, or both. Prerequisites We assume that the reader is already familiar with the concept of DFS. Here is an excellent video introducing DFS with step-by-step animations. We also assume that the reader is familiar with the adjacency list representation of a graph, and we use big-O notation in the analysis. Coding conventions The algorithms below are in Python. n denotes the number of nodes. Nodes are identified with integers in the range 0..n-1. The graph G is a graph stored as an adjacency list: G is a list of n lists. For each v between 0 and n-1, G\\[v\\] is the list of neighbors of G. If the graph is given as an edge list instead, we can initialize it as follows: def makeAdjList(edgeList): n = max(max(edge) for edge in edgeList) + 1 G = \\[\\[\\] for v in range(n)\\] for u,v in edgeList: G\\[u\\].append(v) G\\[v\\].append(u) #omit this for directed graphs return G If the graph is given as an adjacency matrix, we can iterate through the rows of the adjacency matrix instead of through the adjacency lists. To iterate through the neighbors of a node v, instead of for u in G\\[v\\]: #u is a neighbor of v ... we do for u in range(n): if adjMatrix\\[v\\]\\[u\\]: #u is a neighbor of v ... Note that using an adjacency matrix affects the runtime analysis of DFS: O(n²) instead of O(m). Which nodes can be reached from node s? This is the simplest question that can be answered with DFS. The primary data structure in DFS is a list of booleans to keep track of already visited nodes (we call it vis). If we start a DFS search from a node s, the reachable nodes will be the ones for which vis is true. For this, G can be directed or undirected. We make use of a nested function in Python so that we do not need to pass G and vis as parameters (in Python nested functions have visibility over the variables in the scope where they are defined). def reachableNodes(G, s): #G is directed or undirected n = len(G) vis = n \\* \\[False\\] vis\\[s\\] = True #invariant: v is marked as visited when calling visit(v) def visit(v): for nbr in G\\[v\\]: if not vis\\[nbr\\]: vis\\[nbr\\] = True visit(nbr) visit(s) return \\[v for v in range(n) if vis\\[v\\]\\] DFS runs in O(m) time and O(n) space, where m is the number of edges. This is because each edge is considered twice, once from each endpoint, if the endpoints end up being visited, or zero times if the endpoints are not visited. Iterative version def reachableNodes(G, s): #G is directed or undirected n = len(G) vis = n \\* \\[False\\] stk = \\[s\\] #mark nodes as visited when removed from the stack, not when added while stk: v = stk.pop() if vis\\[v\\]: continue vis\\[v\\] = True for nbr in G\\[v\\]: if not vis\\[nbr\\]: stk.append(nbr) return \\[v for v in range(n) if vis\\[v\\]\\] The iterative version takes O(m) space instead of O(n) because nodes can be inserted into the stack multiple times (up to one time for each incident edge). Alternatively, we can mark the nodes as visited when we add them to the stack instead of when we remove them. This change reduces the space usage to the usual O(n). However, with this change, the algorithm is no longer DFS. It still works for answering reachability questions because the set visited nodes is the same, but the order in which they are visited is no longer consistent with a depth-first search order (it is closer to a BFS (breath-first search) order, but also not exactly a BFS order). The difference between marking nodes when they added vs removed from the stack is discussed in detail here. Since the recursive version is shorter and optimal in terms of space, we favor it from now on. That said, it should be easy to adapt the iterative version above to the problems below. Can node s reach node t? We use the same code from before, but we add early termination as soon as we see t. Now, the recursive function has a return value. def canReachNode(G, s, t): #G is directed or undirected n = len(G) vis = n \\* \\[False\\] vis\\[s\\] = True #returns True if the search reaches t def visit(v): if v == t: return True for nbr in G\\[v\\]: if not vis\\[nbr\\]: vis\\[nbr\\] = True if visit(nbr): return True return False return visit(s) Adding the early termination can make the DFS faster, but in the worst-case the time/space complexity is the same. Practice problems https://leetcode.com/problems/the-maze/ The hardest part on this problem is constructing the graph in the first place. Find a path from s to t The edges \"traversed\" in a DFS search form a tree called the \"DFS tree\". The DFS tree changes depending on where we start the search. The starting node is called the root. We can construct the DFS tree by keeping track of the predecessor of each node in the search (the root has no predecessor). If we construct the DFS tree rooted at s, we can follow the sequence of predecessors from t to s to find a path from s to t in reverse order. Instead of using the list vis to keep track of visited nodes, we know a node is unvisited if it has no predecessor yet. We indicate that a node has no predecessor with the special value -1. def findPath(G, s, t): #G is directed or undirected n = len(G) pred = n \\* \\[-1\\] pred\\[s\\] = None def visit(v): for nbr in G\\[v\\]: if pred\\[nbr\\] == -1: pred\\[nbr\\] = v visit(nbr) visit(s) #builds DFS tree path = \\[t\\] while path\\[-1\\] != s: p = pred\\[path\\[-1\\]\\] if p == -1: return None #cannot reach t from s path.append(p) path.reverse() return path Note that DFS does not find the shortest path form s to t. For that, we can use BFS (breath-first search). It just returns any path without repeated nodes. Is the graph connected? For undirected graphs, this is almost the same question as the first question (\"which nodes can be reached by s?\") because of the following property: An undirected graph is connected if and only if every node can be reached from s, where s is any of the nodes. Thus, the code is exactly the same as for the first question, with two differences: 1) we choose s to be 0 (could be anything), and 2) we change the last line to check if every entry in vis is true. def isConnected(G): #G is undirected n = len(G) vis = n \\* \\[False\\] vis\\[0\\] = True def visit(v): for nbr in G\\[v\\]: if not vis\\[nbr\\]: vis\\[nbr\\] = True visit(nbr) visit(0) return all(vis) For directed graphs, we need to take into account the direction of the edges. A directed graph is strongly connected if every node can reach every other node. We can use the following property: A directed graph is strongly connected if and only if s can reach every node and every node can reach s, where s is any of the nodes. We already know how to use DFS to check if s can reach every node. To check if every node can reach s, we can do a DFS starting from s, but in the reverse graph of G. The reverse graph of G is like G but reversing the directions of all the edges. def isConnected(G): #G is directed n = len(G) vis = n \\* \\[False\\] vis\\[0\\] = True #use 0 for start node def visit(G, v): for nbr in G\\[v\\]: if not vis\\[nbr\\]: vis\\[nbr\\] = True visit(G, nbr) visit(G, 0) #nodes reachable from s if not all(vis): return False Greverse = \\[\\[\\] for v in range(n)\\] for v in range(n): for nbr in G\\[v\\]: Greverse\\[nbr\\].append(v) vis = n \\* \\[False\\] #reset vis for the second search vis\\[0\\] = True visit(Greverse, 0) #nodes that can reach s return all(vis) The runtime is still O(m), but the space is now O(m) because we need to create and store the reverse graph. There are alternative algorithms (like Tarjan's algorithm) which can do this in O(n) space. How many connected components are there? We can use the typical DFS to answer this question for undirected graphs. We use a common pattern in DFS algorithms: an outer loop through all the nodes where we launch a search for every yet-unvisited node. def numConnectedComponents(G): #G is undirected n = len(G) vis = n \\* \\[False\\] def visit(v): for nbr in G\\[v\\]: if not vis\\[nbr\\]: vis\\[nbr\\] = True visit(nbr) numCCs = 0 for v in range(n): if not vis\\[v\\]: numCCs += 1 vis\\[v\\] = True visit(v) return numCCs The runtime is now O(n+m) because, if m < n, we still spend O(n) time iterating through the loop at the end. For directed graphs, instead of connected components, we talk about strongly connected components. A strongly connected component is a maximal subset of nodes where every node can reach every other node. If we want to find the number of strongly connected components, we can use something like Tarjan's algorithm, a DFS-based algorithm that requires some additional data structures. Practice problems https://leetcode.com/problems/number-of-connected-components-in-an-undirected-graph/ (Premium only) https://leetcode.com/problems/number-of-islands/ https://leetcode.com/problems/friend-circles/ Which nodes are in the same connected components? This question is more general than the previous two. We label each node v with a number CC\\[v\\] so that nodes with the same number belong to the same CC. Instead of having a list CC in addition to vis, we use the CC number -1 to indicate unvisited nodes. This way, we do not need vis def connectedComponents(G): #G is undirected n = len(G) CC = n \\* \\[-1\\] ##invariant: v is labeled with CC i>=0 def visit(v, i): for nbr in G\\[v\\]: if CC\\[nbr\\] == -1: CC\\[nbr\\] = i visit(nbr, i) i = 0 for v in range(n): if CC\\[v\\] == -1: CC\\[v\\] = i visit(v, i) i += 1 return CC For directed graphs, again we need Tarjan's algorithm or an equivalent algorithm. Practice problems https://leetcode.com/problems/max-area-of-island/ https://leetcode.com/problems/sentence-similarity-ii/ In the second problem, nodes are given by names, not indices, so they need to be converted. Is the graph acyclic? For undirected graphs, this question is simple. First, we consider the problem in each CC independently. This is very common pattern in graph problems. We do this with an outer loop through all the nodes where we launch a search for every yet-unvisited node. During the DFS search in each CC, if we find an edge to an already visited node that is not the predecessor in the search (the node we just came from), there is a cycle. Such edges in a DFS search are called back edges. We add one parameter to the recursive function visit to know the predecessor node. def hasCycles(G): #G is undirected n = len(G) vis = n \\* \\[False\\] #returns True if the search finds a back edge def visit(v, p): for nbr in G\\[v\\]: if vis\\[nbr\\] and nbr != p: return True if not vis\\[nbr\\]: vis\\[nbr\\] = True if visit(nbr, v): return True return False for v in range(n): if not vis\\[v\\]: vis\\[v\\] = True #the root of the search has no predecessor if visit(v, -1): return True return False For directed graphs, it is not as simple: the fact that a neighbor nbr is already visited during the DFS search does not mean that nbr can reach the current node. To check if a directed graph is acyclic, we can use the linear-time peel-off algorithm for finding a topological ordering. This algorithm detects if the graph is acyclic and finds a topological ordering if so, though we are only interested in the first part. Practice problems https://leetcode.com/problems/redundant-connection/ This problem is easier to solve using union-find, but it can be done with DFS. Is the graph a tree? Usually, we ask this question for undirected graphs. We can use this characterization of trees: An undirected graph is a tree if and only if it is connected and has exactly n-1 edges. We already saw how to check if the graph is connected with DFS, and counting the number of edges is straightforward: #for undirected graphs: m = sum(len(G\\[v\\]) for v in range(n)) / 2 #for directed graphs: m = sum(len(G\\[v\\]) for v in range(n)) Practice problems https://leetcode.com/problems/graph-valid-tree/ Is the graph bipartite? This is exactly the same question as whether the graph can be two-colored, so see the next section. Can the graph be two-colored? Two-coloring a graph means assigning colors to the nodes such that no two adjacent nodes have the same color, using only two colors. Usually, we consider coloring question for undirected graphs. We consider whether each CC can be colored independently from the others. We can color each CC using DFS. We use values 0 and 1 for the colors. The color of the start node can be anything, so we set it to 0. For the remaining nodes, the color has to be different from the parent, so we only have one option. Instead of having a vis array, we use the special color -1 to denote unvisited nodes. def is2Colorable(G): #G is undirected n = len(G) color = n \\* \\[-1\\] #returns True if we can color all the nodes reached from v #invariant: v has an assigned color def visit(v): for nbr in G\\[v\\]: if color\\[nbr\\] == color\\[v\\]: return False if color\\[nbr\\] == -1: color\\[nbr\\] = 1 if color\\[v\\] == 0 else 0 if not visit(nbr): return False return True for v in range(n): if color\\[v\\] == -1: color\\[v\\] = 0 if not visit(v): return False return True With 3 or more colors, the problem becomes a lot harder. Practice problems https://leetcode.com/problems/is-graph-bipartite/ What is the distance from a node s to every other node in a tree? We cannot use DFS to find the distance between nodes in a graph which can have cycles, because DFS is not guaranteed to follow the shortest path from the root to the other nodes. For that, BFS is more suitable (if the graph is unweighted). However, since trees are acyclic, there is a unique path between any two nodes, so DFS must use the unique path, which, by necessity, is the shortest path. Thus, we can use DFS to find distances in a tree. def getDistances(G, s): #G is undirected and a tree n = len(G) dists = n \\* \\[-1\\] dists\\[s\\] = 0 #invariant: v has an assigned distance def visit(v): for nbr in G\\[v\\]: #check nbr is not the predecessor if dists\\[nbr\\] != -1: continue dists\\[nbr\\] = dists\\[v\\] + 1 visit(nbr) visit(s) return dists Practice problems https://leetcode.com/problems/time-needed-to-inform-all-employees/ Find a spanning tree A spanning tree of a connected, undirected graph G is a subgraph which has the same nodes as G that is a tree. The edges traversed by a DFS search on a connected graph form a spanning tree (sometimes called a DFS tree). Thus, we do DFS and add the traversed edges to the resulting tree. def spanningTree(G): #G is undirected and connected n = len(G) vis = n \\* \\[False\\] vis\\[0\\] = True T = \\[\\[\\] for v in range(n)\\] def visit(v): for nbr in G\\[v\\]: if not vis\\[nbr\\]: vis\\[nbr\\] = True T\\[v\\].append(nbr) T\\[nbr\\].append(v) visit(nbr) visit(0) return T Conclusions DFS has many uses. We showed how to make minor modifications to the DFS template to answer reachability and connectivity questions. After DFS, the next algorithm to learn would be BFS (breath-first search). Like DFS, it can answer reachability questions. On top of that, it can also answer questions about distance in undirected graphs.Stay in the loopI'd love to tell you when I publish a new post.Get notified when I write about DS&A or software engineering. Unsubscribe anytime if it's not your vibe.Subscribe nowWant to read more? Here are other posts:Single-Edge Cut ProblemMay 15, 2025ResearchWall GameA linear-time algorithm for a graph problem that comes up in the Wall Game.Read moreThe Wall Game ProjectMay 14, 2025Wall GameAn introduction to the Wall Game, my new project.Read moreGet Binary Search Right Every Time, Explained Without CodeApril 15, 2025DS&AA binary search recipe that works for every problem, explained without code.Read moreNil MamanoComputer scientist, software engineer, author.LinkedInTwitter© 2025 nilmamano.com. All rights reserved. Last updated May 2025. Privacy policyHome",
          "content_type": "other",
          "source_url": "https://nilmamano.com/blog/reachability-problems-and-dfs?category=dsa",
          "author": "",
          "user_id": ""
        },
        {
          "title": "Breaking Down Dynamic Programming",
          "content": "HomeNil MamanoAboutResearchProjectsContactMedia KitPersonalBlogToggle theme← Back to all postsBreaking Down Dynamic ProgrammingFebruary 5, 2020Note: the approach in this guide later became the foundation for the dynamic programming chapter in Beyond Cracking the Coding Interview. Introduction When I was a TA for \"Algorithm Design and Analysis\", the students struggled with dynamic programming. To simplify/demystify it, I tried to break it down into a logical sequence of steps, each of which should not feel too intimidating on its own. This is explained in detail here. To complement the explanations, there are links to problems on leetcode.com, in case the reader wants to practice. The code snippets are in Python, but Leetcode accepts most popular languages. Overview: Recursive vs Iterative DP In short, dynamic programming (DP) is a technique for problems that seem hard to solve as a whole, but become easy if we know the solution to smaller subproblems. More technically, we can use it in problems where the (value of the) solution can be expressed as an equation which is a function of the input, and is expressed in terms of itself with smaller inputs. This is called a recurrence equation. The classic example is the Fibonacci recurrence: Fib(n) = Fib(n-1) + Fib(n-2). https://leetcode.com/problems/fibonacci-number/ A recurrence equation can be translated into code: def Fib(n): if n == 0 or n == 1: return 1 return Fib(n-1) + Fib(n-2) However, the above function has an exponential runtime. A recursive function becomes exponential when it is possible to reach the same subcall through different execution paths. In the Fibonacci case, we have the following nested calls: Fib(n) -> Fib(n-1) -> Fib(n-2), and Fib(n) -> Fib(n-2). Since Fib(n-2) is called twice all the work from this call is duplicated, which in turn means that subcalls made from Fib(n-2) will start to duplicate and grow exponentially. Dynamic programming is simply a workaround to this duplication issue. Instead of recomputing the solutions of the subproblems, we store them and then we recall them as needed. This guarantees that each subproblem is computed only once. There are two main approaches for DP. Recursive / Top-down DP We start with the code which is a literal translation of the recurrence equation, but then we add a dictionary / hash table to store results. memo = {} def Fib(n): if n == 0 or n == 1: return 1 if n in memo: return memo\\[n\\] res = Fib(n-1) + Fib(n-2) memo\\[n\\] = res return res There are three changes in the code above: declaring our dictionary for storing results, memo outside the recursive function (memo comes \"memorization\" or \"memoization\", a name used in the literature). before computing the result, we check if the solution has already been computed. This check can be done before or after the base case. before returning, we save the result in the memo table. Using a memoization table in this way solves the inefficiency (we will go deeper into the analysis part later). Iterative / Bottom-up DP Instead of starting from the largest input and recursively reaching smaller subproblems, we can directly compute the subproblems from smallest to largest. This way, we already have the solutions to the subproblems when we need them. For this approach, we change the dictionary for an array/vector, and we change recursive calls for a for loop. def Fib(n): if n == 0: return 1 memo = \\[0 for i in range(n+1)\\] memo\\[0\\], memo\\[1\\] = 1, 1 for i in range(2, n+1): memo\\[i\\] = memo\\[i-1\\] + memo\\[i-2\\] return memo\\[n\\] Most problems can be solved with both recursive and iterative DP. Here are some considerations for how to choose: Recursive DP matches the recurrence equation more directly, so it can be easier to implement. Both have the same runtime complexity, but the recursive version will generally have larger constant factors due to all the recursive function calling and due to using a hash table instead of an array. Iterative DP often allows for an optimization to reduce the space complexity (discussed later). Recursive DP in 5 Steps Choose what your subproblems are. Find the recurrence equation. Translate the recurrence equation into recursive code. Add memoization. (Optional) Reconstruct the solution. We already saw steps 1–4 with the Fibonacci example. Now, we will walk through all the steps in more detail using a more complicated example, the longest common subsequence problem: Given two strings s1 and s2, find the length of the longest string which is a subsequence of both s1 and s2. A string t is a subsequence of a string s if every char in t appears in order in s, but are not necessarily contiguous. For example, abc is a subsequence of axbyz, but ba is not (do not confuse subsequence with substring or subset). https://leetcode.com/problems/longest-common-subsequence/ Step 1: choose our subproblems. This varies from problem to problem, but when the input to the problem is a string, a natural way to obtain smaller problems is to look at shorter strings. Here we can use as a subproblem a prefix of s1 and a prefix of s2. Some notation: let n be the length of s1 and m the length of s2. Let LCS(i,j) be the solution for the LCS problem for the prefix of s1 of length n (s1\\[0..i-1\\]) and the prefix of s2 of length m (s2\\[0..j-1\\]). Then, our goal is to find LCS(n, m). Step 2: find the recurrence equation. Now we need to come up with an expression for LCS(i,j) as a function of LCS with smaller indices (as well as a base case). This is the hardest step of DP, and often it is here that we realize that we chose bad subproblems in Step 1. If that happens, hopefully we will discover some hint for what our subproblems should be. In order to derive the recurrence equation for LCS, we need the following observation: if the two strings end with the same character c, then, to maximize the length of the subsequence, it is \"safe\" to add c to the subsequence. In contrast, if both strings end with different characters, then at least one of them cannot appear in the subsequence. The complication is that we do not know which one. Thus, instead of guessing, we can simply consider both options. This observation yields the recurrence equation (excluding base case): LCS(i, j) = 1 + LCS(i-1, j-1) if s\\[i\\] == s\\[j\\] max(LCS(i, j-1), LCS(i-1, j)) otherwise This step is not intuitive at first, and requires practice. After having done a few problems, one starts to recognize the typical patterns in DP. For instance, using max among a set of options of which we do not know which one is the best is easily the most common pattern in DP. Step 3. Translate the recurrence equation into recursive code. This step is a very simple programming task. Pay attention to the base case. #outer call: LCS(len(s1), len(s2)) def LCS(i, j): if i == 0 or j == 0: return 0 if s1\\[i-1\\] == s2\\[j-1\\]: return 1 + LCS(i-1, j-1) else: return max(LCS(i, j-1), LCS(i-1, j)) If we draw the few first steps of the call graph, we will see that the same subproblem is reached twice. Thus, call graph blows up, leading to an exponential runtime. Step 4. Add memo table. This step should be automatic: one does not even need to understand the previous code in order to add the memo table. #outer call: memo = {} LCS(len(s1), len(s2)) def LCS(i, j): if i == 0 or j == 0: return 0 if (i,j) in memo: return memo\\[(i,j)\\] if s1\\[i-1\\] == s2\\[j-1\\]: res = 1 + LCS(i-1, j-1) else: res = max(LCS(i, j-1), LCS(i-1, j)) memo\\[(i,j)\\] = res return res The base case corresponds to when one of the strings is empty. The LCS of an empty string with another string is clearly an empty string. Incidentally, if we flip the check on the memo table, the code becomes a bit more streamlined (fewer lines + merging the two returns). I prefer this form (it does the same): def LCS(i, j): if i == 0 or j == 0: return 0 if (i,j) not in memo: if s1\\[i-1\\] == s2\\[j-1\\]: memo\\[(i,j)\\] = 1 + LCS(i-1, j-1) else: memo\\[(i,j)\\] = max(LCS(i, j-1), LCS(i-1, j)) return memo\\[(i,j)\\] We have eliminated the exponential blowup. In general, DP algorithms can be analyzed as follows: # of distinct subproblems times time per subproblem excluding recursive calls. For LCS, we get O(nm)\\*O(1)=O(nm). Step 5. Reconstruct the solution. We used DP to compute the length of the LCS. What if we want to find the LCS itself? A naive way to do it would be to store the entire result of each subproblem in the memoization table instead of just its length. While this works, it is clear that it will require a lot of memory to store O(nm) strings of length O(min(n,m)) each. We can do better. Step 5, \"Reconstruct the solution\", is how to reuse the table that we constructed in Step 4 to find the actual solution instead of just its length. I said that this step is optional because sometimes we just need the value of the solution, so there is no reconstruction needed. The good news is that we do not need to modify the code that we already wrote in Step 4. The reconstruction is a separate step that comes after. In addition, the reconstruction step is very similar (follows the same set of cases) as the step of building the memo table. In short, we use the memo table as an \"oracle\" to guide us in our choices. Based on the values in the memo table, we know which option is better, so we know how to reconstruct the solution. #outer calls memo = {} n, m = len(s1), len(s2) LCS(n, m) #build memo table sol = reconstructLCS(n, m) def reconstructLCS(i, j): if i == 0 or j == 0: return \"\" if s1\\[i-1\\] == s2\\[j-1\\]: return reconstructLCS(i-1, j-1) + s1\\[i-1\\] elif memo\\[(i-1,j)\\] >= memo\\[(i,j-1)\\]: return reconstructLCS(i-1, j) else: return reconstructLCS(i, j-1) In the code above, first we run LCS(n,m) to fill the memo table. Then, we use it in the reconstruction. The condition memo\\[(i-1,j)\\] >= memo\\[(i,j-1)\\] tells us that we can obtain a longer or equal LCS by discarding a char from s1 instead of from s2. Note that there is a single recursive call in the reconstruction function, so the complexity is just O(n+m). Iterative DP in 6 Steps Choose what your subproblems are. Find the recurrence equation. Design the memo table. Fill the memo table. (Optional) Reconstruct the solution. (Optional) Space optimization. The new/different steps are highlighted. Step 3. is to design the layout of the table/matrix where we are going to store the subproblem solutions. There is no coding in this step. By \"design\", I mean making the following choices: what are the dimensions of the table, and what does each index mean. Generally speaking, the table should have one dimension for each parameter of the recurrence equation. In the case of LCS, it will be a 2-dimensional table. where are the base cases. where is the cell with the final solution. what is the \\`\\`dependence relationship'' between cells (which cells do you need in order to compute each cell). which cells do not need to be filled (in the case of LCS, we need them all). Here is how I would lay out the table for LCS (you can find a different layout in the problems below): Next (Step 4), we fill the memo table with a nested for loop. If the layout is good, this should be easy. Before the main loop, we fill the base case entries. Then, we must make sure to iterate through the table in an order that respects the dependencies between cells. In the case of LCS, we can iterate both by rows or by columns. We obtain the following algorithm: def LCS(s1, s2): n, m = len(s1), len(s2) memo = \\[\\[0 for j in range(m+1)\\] for i in range(n+1)\\] for i in range(1, n+1): for j in range(1, m+1): if s1\\[i-1\\] == s2\\[j-1\\]: memo\\[i\\]\\[j\\] = 1 + memo\\[i-1\\]\\[j-1\\] else: memo\\[i\\]\\[j\\] = max(memo\\[i-1\\]\\[j\\], memo\\[i\\]\\[j-1\\]) return memo\\[n\\]\\[m\\] In the code above, the base case entries are filled implicitly when we initialize the table with zeros everywhere. If we need to reconstruct the solution, we can do it in the same way as for the recursive DP. The only difference is that memo is a matrix instead of dictionary. Space optimization Clearly, the space complexity of iterative DP is the size of the DP table. Often, we can do better. The idea is to only store the already-computed table entries that we will use to compute future entries. For instance, in the case of Fibonacci, we do not need to create an entire array -- keeping the last two numbers suffice. In the case of a 2-dimensional DP table, if we are filling the DP table by rows and each cell only depends on the previous row, we only need to keep the last row (and similarly if we iterated by columns). Here is the final version for LCS where we improve the space complexity from O(nm) to O(n+m): def LCS(s1, s2): n, m = len(s1), len(s2) lastRow = \\[0 for j in range(m+1)\\] for i in range(1,n+1): curRow = \\[0 for j in range(m+1)\\] for j in range(1,m+1): if s1\\[i-1\\] == s2\\[j-1\\]: curRow\\[j\\] = 1 + lastRow\\[j-1\\] else: curRow\\[j\\] = max(lastRow\\[j\\], curRow\\[j-1\\]) lastRow = curRow return lastRow\\[m\\] Note: this optimization is incompatible with reconstructing the solution, because that uses the entire table as an \"oracle\". DP Patterns Here are some typical patterns: For Step 1. The subproblems. If the input is a string or a list, the subproblems are usually prefixes or substrings/sublists, which can be specified as a pair of indices. If the input is a number, the subproblems are usually smaller numbers. Generally speaking, the number of subproblems will be linear or quadratic on the input size. For Step 2. The recurrence equation. Often, we use max or min to choose between options, or sum to aggregate subsolutions. The number of subproblems is most often constant, but sometimes it is linear on the subproblem size. In the latter case, we use an inner loop to aggregate/choose the best solution. Sometimes, the recurrence equation is not exactly for the original problem, but for a related but more constrained problem. See an example below, \"Longest Increasing Subsequence\". Practice Problems Here are some practice problems showcasing the patterns mentioned above. Follow the Leetcode links for the statements and example inputs. I jump directly to the solutions. I'd recommend trying to solve the problems before checking them. https://leetcode.com/problems/palindromic-substrings/ Here, the goal is to count the number of substrings of a string s which are palindromic. There is a trivial O(n³) time solution without DP: def countSubstrings(s): n = len(s) count = 0 for i in range(n): for j in range(i, n): if isPalindrome(s\\[i:j+1\\]): count += 1 return count We can improve this to O(n²) with DP. The subproblems are all the substrings of s. Let Pal(i, j) be true iff s\\[i..j\\] is a palindrome. We have the following recurrence equation (excluding base cases): Pal(i, j) = false if s\\[i\\] != s\\[j\\], PAl(i, j) = Pal(i+1, j-1) otherwise Based on this recurrence equation, we can design the following DP table: This type of \"diagonal\" DP tables are very common when the subproblems are substrings/sublists. In this case, the base cases are substrings of length 1 or 2. The goal is Pal(0,n-1). Given the dependency, the table can be filled by rows (starting from the last row), by columns (starting each column from the bottom), or by diagonals (i.e., from shortest to longest substrings). In the code below, I illustrate how to fill the table by diagonals. def countSubstrings(s): n = len(s) T = \\[\\[False for j in range(n)\\] for i in range(n)\\] for i in range(n): T\\[i\\]\\[i\\] = True for i in range(n-1): T\\[i\\]\\[i+1\\] = s\\[i\\] == s\\[i+1\\] for size in range(2, n+1): for i in range(0,n-size): j = i + size T\\[i\\]\\[j\\] = s\\[i\\] == s\\[j\\] and T\\[i+1\\]\\[j-1\\] count = 0 for row in T: for val in row: if val: count += 1 return count https://leetcode.com/problems/minimum-path-sum/ Here, a subproblem can be a grid with reduced width and height. Let T\\[i\\]\\[j\\] be the cheapest cost to reach cell (i,j). The goal is to find T\\[n-1\\]\\[m-1\\], where n and m are the dimensions of the grid. The base case is when either i or j are zero, in which case we do not have any choices for how to get there. In the general case, we have the recurrence equation T\\[i\\]\\[j\\] = grid\\[i\\]\\[j\\] + min(T\\[i-1\\]\\[j\\], T\\[i\\]\\[j-1\\]): to get to (i,j), we first need to get to either (i-1,j) or to (i,j-1). We use min to choose the best of the two. We convert this into an iterative solution: def minPathSum(grid): n, m = len(grid), len(grid\\[0\\]) T = \\[\\[0 for j in range(m)\\] for i in range(n)\\] T\\[0\\]\\[0\\] = grid\\[0\\]\\[0\\] for i in range(1, n): T\\[i\\]\\[0\\] = grid\\[i\\]\\[0\\] + T\\[i-1\\]\\[0\\] for j in range(1, m): T\\[0\\]\\[j\\] = grid\\[0\\]\\[j\\] + T\\[0\\]\\[j-1\\] for i in range(1, n): for j in range(1, m): T\\[i\\]\\[j\\] = grid\\[i\\]\\[j\\] + min(T\\[i-1\\]\\[j\\], T\\[i\\]\\[j-1\\]) return T\\[n-1\\]\\[m-1\\] https://leetcode.com/problems/unique-paths-ii/ This is similar to the previous problem, but we need to accumulate the solutions to the subproblems, instead of choosing between them. Problems about counting solutions can often be solved with DP. https://leetcode.com/problems/longest-increasing-subsequence/ This problem will illustrate a new trick: if you cannot find a recurrence equation for the original problem, try to find one for a more restricted version of the problem which nevertheless you enough information to compute the original problem. Here, the input is a list L of numbers, and we need to find the length of the longest increasing subsequence (a subsequence does not need to be contiguous). Again, the subproblems correspond to prefixes of the list. Let LIS(i) be the solution for the prefix of length i (L\\[0..i\\]). The goal is to find LIS(n-1), where n is the length of L. However, it is not easy to give a recurrence equation for LIS(i) as a function of smaller prefixes. In particular, the following is wrong (I will let the reader think why): LIS(i) = LIS(i-1) + 1 if L\\[i\\] > L\\[i-1\\], LIS(i) = LIS(i-1) otherwise Thus, we actually give a recurrence equation for a slightly modified type of subproblems: let LIS2(i) be the length of the LIS ending at index i. This constraint makes it easier to give a recurrence equation: LIS2(i) = 1 + max(LIS2(j)) over all j < i such that L\\[j\\] < L\\[i\\] In short, since we know that the LIS ends at L\\[i\\], we consider all candidate predecessors, which are the numbers smaller than it, and get the best one by using max. Crucially, this recurrence works for LIS2(i) but not for LIS(i). Here is a full solution: def LIS(L): n = len(L) T = \\[0 for i in range(n)\\] T\\[0\\] = 1 for i in range(1, n): T\\[i\\] = 1 for j in range(0, i): if L\\[j\\] < L\\[i\\]: T\\[i\\] = max(T\\[i\\], T\\[j\\] + 1) return max(T) At the end, we do not simply return T\\[n-1\\] because T is the table for LCS2, not LCS. We return max(T) because the LCS must end somewhere, so LCS(n-1) = LCS2(j) for some j < n. Note that the runtime is O(n²) even though the table has linear size. This is because we take linear time per subproblem. https://leetcode.com/problems/number-of-longest-increasing-subsequence/ A harder version of the previous problem. A similar approach works. First solve the LIS problem as before, and then do a second pass to count the solutions. https://leetcode.com/problems/shortest-common-supersequence/ This problem is similar to LCS, and it requires reconstruction. I should mention that not every problem that can be solved with DP fits into the mold discussed above. Despite that, it should be a useful starting framework. Here are many more practice problems: https://leetcode.com/tag/dynamic-programming/ Stay in the loopI'd love to tell you when I publish a new post.Get notified when I write about DS&A or software engineering. Unsubscribe anytime if it's not your vibe.Subscribe nowWant to read more? Here are other posts:Wall Game DB DesignMay 12, 2025Wall GameDesigning the DB for the Wall Game.Read moreLifecycle of a CS research paper: my knight's tour paperMay 2, 2025ResearchThe backstory and thought process behind a fun paper from my PhD.Read moreGet Binary Search Right Every Time, Explained Without CodeApril 15, 2025DS&AA binary search recipe that works for every problem, explained without code.Read moreNil MamanoComputer scientist, software engineer, author.LinkedInTwitter© 2025 nilmamano.com. All rights reserved. Last updated May 2025. Privacy policyHome",
          "content_type": "other",
          "source_url": "https://nilmamano.com/blog/breaking-down-dynamic-programming?category=dsa",
          "author": "",
          "user_id": ""
        },
        {
          "title": "Iterative Tree Traversals: A Practical Guide",
          "content": "HomeNil MamanoAboutResearchProjectsContactMedia KitPersonalBlogToggle theme← Back to all postsIterative Tree Traversals: A Practical GuideFebruary 4, 2020Introduction I don't know how often tree traversals come up in actual software projects, but they are popular in coding interviews and competitive programming. In this article, I share an approach for implementing tree traversal algorithms iteratively that I found to be simple to remember and implement, while being flexible enough to do anything that a recursive algorithm can (I also didn't like most suggestions I saw online). The main technique is given in section \"Iterative Postorder and Inorder Traversal\", but first I give some context. I also link to practice problems on leetcode.com for the reader to play with. I provide some solutions, but I suggest trying the problems out first. The code snippets are in C++, but leetcode accepts most languages. What are Tree Traversals Mathematically, trees are just connected acyclic graphs. However, in the context of tree traversals, we are usually working with rooted trees represented with a recursive structure such as the following (which is the default definition in Leetcode for binary trees). A leaf is a node with two null pointers as children: struct TreeNode { int val; TreeNode \\*left; TreeNode \\*right; TreeNode(int x) : val(x), left(NULL), right(NULL) {} }; A tree traversal is an algorithm that visits every node in a tree in a specific order (and does some computation with them, depending on the problem). For binary trees specifically, there are three important orders: Preorder: root before children. As we will see, this is the simplest to implement. Inorder: left child, then root, then right child. This traversal is most often used on binary search trees (BST). A BST is a rooted binary tree with the additional property that every node in the left subtree has a smaller value than the root, and every node in the right subtree has a larger value than the root. This traversal is called \"inorder\" because, when used on a BST, it will visit the nodes from smallest to largest. Postorder: children before root. It comes up in problems where we have to aggregate information about the entire subtree rooted at each node. Classic examples are computing the size, the height, or the sum of values of the tree. Because rooted trees are recursive data structures, algorithms on trees are most naturally expressed recursively. Here are the three traversals. I use the function process(node) as a placeholder for whatever computation the problem calls for. void preorderTraversal(TreeNode\\* root) { if (!root) return; process(root); preorderTraversal(root->left); preorderTraversal(root->right); } void inorderTraversal(TreeNode\\* root) { if (!root) return; inorderTraversal(root->left); process(root); inorderTraversal(root->right); } void postorderTraversal(TreeNode\\* root) { if (!root) return; postorderTraversal(root->left); postorderTraversal(root->right); process(root); } Side-note: in C++, pointers are implicitly converted to booleans: a pointer evaluates to true if and only if it is not null. So, in the code above, \"if (!root)\" is equivalent to \"if (root == NULL)\". Traversal problems on leetcode https://leetcode.com/problems/binary-tree-preorder-traversal/ https://leetcode.com/problems/binary-tree-inorder-traversal/ https://leetcode.com/problems/binary-tree-postorder-traversal/ Why / When to Use an Iterative Traversal If the recursive implementation is so simple, why bother with an iterative one? Of course, to avoid stack overflow. Most runtime engines/compilers set a limit on how many nested calls a program can make. For example, according to this article: Default stack size varies between 320k and 1024k depending on the version of Java and the system used. For a 64 bits Java 8 program with minimal stack usage, the maximum number of nested method calls is about 7000. If the height of the tree is larger than this limit, the program will crash with a stack overflow error. A recursive implementation is safe to use if: Somehow we know that the input trees will be small enough. The tree is balanced, which means that, for each node, the left and right subtrees have roughly the same height. In a balanced tree, the height is guaranteed to be logarithmic on the number of nodes (indeed, that is why balanced BSTs guarantee O(log n) search time), so any tree that fits in RAM (or even disk) will require a tiny number of recursive calls. However, if we are not in either of the cases above, an iterative solution is safer. Recursive and iterative traversals have the same runtime complexity, so this is not a concern when choosing either (all the problems shown in this article can be solved in linear time using either). The main approach for converting recursive implementations to iterative ones is to \"simulate\" the call stack with an actual stack where we push and pop the nodes explicitly. This works great \"out-of-the-box\" with preorder traversal. Incidentally, when implementing tree traversals we need to make an implementation choice about how to handle NULL pointers. We can be eager and filter them out before adding them to the stack, or we can be lazy and detect them once we extract them from the stack. Both are fine—what matters is to be deliberate and consistent about which approach we are using. I prefer the latter as it yields slightly shorter code, so I will use it in all the following examples. For comparison, here is the iterative preorder traversal with both approaches: //eager NULL checking void preorderTraversal(TreeNode\\* root) { stack stk; if (!root) return; stk.push(root); while (!stk.empty()) { TreeNode\\* node = stk.top(); stk.pop(); process(node); if (node->right) stk.push(node->right); if (node->left) stk.push(node->left); } } //lazy NULL checking void preorderTraversal(TreeNode\\* root) { stack stk; stk.push(root); while (!stk.empty()) { TreeNode\\* node = stk.top(); stk.pop(); if (!node) continue; process(node); stk.push(node->right); stk.push(node->left); } } Note that the right child is pushed to the stack before the left one. This is because we want the left child to be above in the stack so that it is processed first. Preorder traversal practice problems https://leetcode.com/problems/invert-binary-tree/ https://leetcode.com/problems/maximum-depth-of-binary-tree/ This problem asks to find the depth of a binary tree (follow the link for the description and examples). It requires passing information from each node to its children. We can do this by changing the stack to stack\\>, so that we can pass an int to each child, as in the solution below: int maxDepth(TreeNode\\* root) { int res = 0; stack\\> stk; stk.push({root, 1}); //node, depth while (!stk.empty()) { auto node = stk.top().first; int depth = stk.top().second; stk.pop(); if (!node) continue; res = max(res, depth); stk.push({node->left, depth+1}); stk.push({node->right, depth+1}); } return res; } In the code above, the {} notation is used to create pairs (e.g., {root, 0}). If one is not familiar with pairs in C++, or is using a language without the equivalent, a simple alternative is to use two separate stacks, one for the nodes and one for the info. The next two problems are similar: https://leetcode.com/problems/minimum-depth-of-binary-tree/ https://leetcode.com/problems/path-sum/ https://leetcode.com/problems/symmetric-tree/ A solution for the last one, this time using a stack with a pair of nodes: bool isSymmetric(TreeNode\\* root) { if (!root) return true; stack\\> stk; stk.push({root->left, root->right}); while (!stk.empty()) { TreeNode\\* l = stk.top().first; TreeNode\\* r = stk.top().second; stk.pop(); if (!l and !r) continue; if (!l or !r or l->val != r->val) return false; stk.push({l->left, r->right}); stk.push({l->right, r->left}); } return true; } Iterative Postorder and Inorder Traversal While iterative preorder traversal is straightforward, with postorder and inorder we run into a complication: we cannot simply swap the order of the lines as with the recursive implementation. In other words, the following does not yield a postorder traversal: ... stk.push(node->right); stk.push(node->left); process(node); ... The node is still processed before its children, which is not what we want. The workaround, once again emulating the recursive implementation, is to visit each node twice. We consider postorder traversal first. In the first visit, we only push the children onto the stack. In the second visit, we do the actual processing. The simplest way to do this is to enhance the stack with a \"visit number flag\". Implementation-wise, we change the stack to stack\\> so that we can pass the flag along with each node. The iterative postorder looks like this: void postorderTraversal(TreeNode\\* root) { stack\\> stk; //node, visit # stk.push({root, 0}); while (!stk.empty()) { TreeNode\\* node = stk.top().first; int visit = stk.top().second; stk.pop(); if (!node) continue; if (visit == 0) { stk.push({node, 1}); stk.push({node->right, 0}); stk.push({node->left, 0}); } else { //visit == 1 process(node); } } } Note the order in which the nodes are added to the stack when visit == 0. The parent ends up under its children, with the left child on top. Since it is the first time that the children are added to the stack, their visit-number flag is 0. For the parent, it is 1. For simplicity, I also follow the convention to always immediately call pop after extracting the top element from the stack. The same approach also works for inorder traversal (that's the point). Here is a version where we visit each node three times: one to push the left child, one to process the node, and one to push the right child. //3-visit version void inorderTraversal(TreeNode\\* root) { stack\\> stk; stk.push({root, 0}); while (!stk.empty()) { TreeNode\\* node = stk.top().first; int visit = stk.top().second; stk.pop(); if (!node) continue; if (visit == 0) { stk.push({node, 1}); stk.push({node->left, 0}); } else if (visit == 1) { stk.push({node, 2}); process(node); } else { //visit == 2 stk.push({node->right, 0}); } } } In fact, the second and third visits can be merged together: processing the node does not modify the stack, so the two visits are followed one after the other anyway. Here is my preferred version: //2-visit version void inorderTraversal(TreeNode\\* root) { stack\\> stk; stk.push({root, 0}); while (!stk.empty()) { TreeNode\\* node = stk.top().first; int visit = stk.top().second; stk.pop(); if (!node) continue; if (visit == 0) { stk.push({node, 1}); stk.push({node->left, 0}); } else { //visit == 1 process(node); stk.push({node->right, 0}); } } } For completeness, here is the version found in most of my top Google hits (see this for a nice explanation): void inorderTraversal(TreeNode\\* root) { stack stk; TreeNode\\* curr = root; while (curr or !stk.empty()) { while (curr) { stk.push(curr); curr = curr->left; } curr = stk.top(); stk.pop(); process(curr); curr = curr->right; } } While it is shorter, it cannot be easily converted to postorder traversal, so it is not as flexible. Also, I find it easier to follow the execution flow with the visit-number flag. Inorder traversal practice problems https://leetcode.com/problems/kth-smallest-element-in-a-bst/ A solution (follow the link for the statement and examples): int kthSmallest(TreeNode\\* root, int k) { int count = 1; stack\\> stk; stk.push({root, 0}); while (!stk.empty()) { auto node = stk.top().first; int visit = stk.top().second; stk.pop(); if (!node) continue; if (visit == 0) { stk.push({node, 1}); stk.push({node->left, 0}); } else { //visit == 1 if (count == k) return node->val; count++; stk.push({node->right, 0}); } } return -1; } https://leetcode.com/problems/validate-binary-search-tree/ A solution: bool isValidBST(TreeNode\\* root) { int lastVal; bool init = false; stack\\> stk; stk.push({root, 0}); while (!stk.empty()) { TreeNode\\* node = stk.top().first; int visit = stk.top().second; stk.pop(); if (!node) continue; if (visit == 0) { stk.push({node, 1}); stk.push({node->left, 0}); } else { //second visit if (!init) { init = true; lastVal = node->val; } else { if (node->val <= lastVal) return false; lastVal = node->val; } stk.push({node->right, 0}); } } return true; } Postorder traversal practice problems https://leetcode.com/problems/balanced-binary-tree/ This problem asks to check if a binary tree is balanced. It requires passing information back from the children to the parent node in a postorder traversal. Passing information from the children to the parent is easy with recursion. It can be done both with return values or with parameters passed by reference. For this problem we need to pass two things: a bool indicating if the subtree is balanced, and an int indicating its height. I use a reference parameter for the latter (returning a pair would be cleaner). bool isBalancedRec(TreeNode\\* root, int& height) { if (!root) { height = 0; return true; } int lHeight, rHeight; bool lBal = isBalancedRec(root->left, lHeight); bool rBal = isBalancedRec(root->right, rHeight); height = max(lHeight, rHeight) + 1; return lBal && rBal && abs(lHeight - rHeight) <= 1; } bool isBalanced(TreeNode\\* root) { int height; return isBalancedRec(root, height); } Passing information from the children to the parent in an iterative implementation is more intricate. There are three general approaches: Use a hash table mapping each node to the information. This is the easiest way, but also the most expensive. While the asymptotic runtime is still linear, hash tables generally have significant constant factors. bool isBalanced(TreeNode\\* root) { stack\\> stk; stk.push({root, 0}); unordered\\_map height; height\\[NULL\\] = 0; while (!stk.empty()) { TreeNode\\* node = stk.top().first; int visit = stk.top().second; stk.pop(); if (!node) continue; if (visit == 0) { stk.push({node, 1}); stk.push({node->right, 0}); stk.push({node->left, 0}); } else { // visit == 1 int lHeight = height\\[node->left\\], rHeight = height\\[node->right\\]; if (abs(lHeight - rHeight) > 1) return false; height\\[node\\] = max(lHeight, rHeight) + 1; } } return true; } Add a field to the definition of the node structure for the information needed. Then, we can read it from the parent node by traversing the children's pointers. In Leetcode we cannot modify the TreeNode data structure so, to illustrate this approach, I build a new tree first with a new struct: struct MyNode { int val; int height; MyNode \\*left; MyNode \\*right; MyNode(TreeNode\\* node): val(node->val), height(-1), left(NULL), right(NULL) { if (node->left) left = new MyNode(node->left); if (node->right) right = new MyNode(node->right); } }; bool isBalanced(TreeNode\\* root) { if (!root) return true; MyNode\\* myRoot = new MyNode(root); stack\\> stk; stk.push({myRoot, 0}); while (!stk.empty()) { MyNode\\* node = stk.top().first; int visit = stk.top().second; stk.pop(); if (!node) continue; if (visit == 0) { stk.push({node, 1}); stk.push({node->right, 0}); stk.push({node->left, 0}); } else { // visit == 1 int lHeight = 0, rHeight = 0; if (node->left) lHeight = node->left->height; if (node->right) rHeight = node->right->height; if (abs(lHeight - rHeight) > 1) return false; node->height = max(lHeight, rHeight) + 1; } } return true; } Pass the information through an additional stack. This is the most efficient, but one must be careful to keep both stacks in synch. When processing a node, that node first pops the information from its children, and then pushes its own info for its parent. Here is a solution (with eager NULL-pointer detection): bool isBalanced(TreeNode\\* root) { if (!root) return true; stack\\> stk; stk.push({root, 0}); stack heights; while (!stk.empty()) { TreeNode\\* node = stk.top().first; int visit = stk.top().second; stk.pop(); if (visit == 0) { stk.push({node, 1}); if (node->right) stk.push({node->right, 0}); if (node->left) stk.push({node->left, 0}); } else { // visit == 1 int rHeight = 0, lHeight = 0; if (node->right) { rHeight = heights.top(); heights.pop(); } if (node->left) { lHeight = heights.top(); heights.pop(); } if (abs(lHeight - rHeight) > 1) return false; heights.push(max(lHeight, rHeight) + 1); } } return true; } https://leetcode.com/problems/diameter-of-binary-tree/ This problem also requires passing information from the children to the parent in a postorder traversal. Here is a solution using the third approach again, but this time with lazy NULL-pointer detection. Note that we push a 0 to the depths stack when we extract a NULL pointer from the main stack, and during processing we always do two pops regardless of the number of non-NULL children: int diameterOfBinaryTree(TreeNode\\* root) { stack\\> stk; stk.push({root, 0}); stack depths; int res = 0; while (!stk.empty()) { TreeNode\\* node = stk.top().first; int visit = stk.top().second; stk.pop(); if (!node) { depths.push(0); continue; } if (visit == 0) { stk.push({node, 1}); stk.push({node->right, 0}); stk.push({node->left, 0}); } else { //visit == 1 int rDepth = depths.top(); depths.pop(); int lDepth = depths.top(); depths.pop(); int depth = max(lDepth, rDepth) + 1; depths.push(depth); int dia = lDepth + rDepth; res = max(res, dia); } } return res; } https://leetcode.com/problems/binary-tree-tilt/ https://leetcode.com/problems/most-frequent-subtree-sum/ https://leetcode.com/problems/maximum-product-of-splitted-binary-tree/ Traversals in n-ary Trees So far, we have looked at binary trees. In an n-ary tree, each node has an arbitrary number of children. struct Node { int val; vector children; Node(int val): val(val), children(0) {} }; For n-ary trees, preorder traversal is also straightforward, and inorder traversal is not defined. For postorder traversal, we can use a visit-number flag again. Two visits suffice for each node: one to push all the children into the stack, and another to process the node itself. I do not include the code here because it is very similar to the binary tree case. Consider a more complicated setting where we need to compute something at the node after visiting each child. Let's call this \"interleaved traversal\". I use process(node, i) as placeholder for the computation done before visiting the i-th child. Here is the recursive implementation and the corresponding iterative one using visit-number flags. //recursive void interleavedTraversal(Node\\* root) { if (!root) return; int n = root->children.size(); for (int i = 0; i < n; i++) { process(root, i); interleavedTraversal(root->children\\[i\\]); } } //iterative void interleavedTraversal(Node\\* root) { stack\\> stk; stk.push({root, 0}); while (!stk.empty()) { TreeNode\\* node = stk.top().first; int visit = stk.top().second; stk.pop(); if (!node) continue; int n = node->children.size(); if (visit < n) { stk.push({node, visit+1}); process(node, visit); stk.push({node->children\\[visit\\], 0}); } } } N-ary tree practice problems https://leetcode.com/problems/n-ary-tree-preorder-traversal/ https://leetcode.com/problems/n-ary-tree-postorder-traversal/ An Alternative Way of Passing the Visit Flag The common framework to all our solutions has been to pass a visit-number flag along with the nodes on the stack. User \"heiswyd\" on leetcode posted here an alternative way to pass the flag implicitly: initially, it pushes each node on the stack twice. Then, it can distinguish between the first visit and the second visit by checking whether the node that has just been extracted from the stack matches the node on top of the stack. This happens only when we extract the first of the two occurrences. Post-order traversal looks like this: void postorderTraversal(TreeNode\\* root) { stack stk; stk.push(root); stk.push(root); while (!stk.empty()) { TreeNode\\* node = stk.top(); stk.pop(); if (!node) continue; if (!stk.empty() and stk.top() == node) { stk.push(node->right); stk.push(node->right); stk.push(node->left); stk.push(node->left); } else { process(node); } } } It is cool, but I prefer passing the flag explicitly for clarity.Stay in the loopI'd love to tell you when I publish a new post.Get notified when I write about DS&A or software engineering. Unsubscribe anytime if it's not your vibe.Subscribe nowWant to read more? Here are other posts:Negative Binary Search and Choir RehearsalOctober 23, 2024ResearchA curious application of binary search.Read moreChoosing a tech stack in 2025May 7, 2025SWEWall GameHow would you build a Lichess clone in 2025? My process for picking a tech stack.Read moreWall Game DB DesignMay 12, 2025Wall GameDesigning the DB for the Wall Game.Read moreNil MamanoComputer scientist, software engineer, author.LinkedInTwitter© 2025 nilmamano.com. All rights reserved. Last updated May 2025. Privacy policyHome",
          "content_type": "other",
          "source_url": "https://nilmamano.com/blog/iterativetreetraversal?category=dsa",
          "author": "",
          "user_id": ""
        }
      ]
    }
  ]
}