{
  "team_id": "aline123",
  "items": [
    {
      "title": "In Defense of Coding Interviews",
      "content": "A collection of arguments in favor of coding interviews.\n\n# In Defense of Coding Interviews\n\nApril 16, 2025\n\n![In Defense of Coding Interviews](/blog/in-defense-of-coding-interviews/cover.png)\n\nThere is already a lot of discourse about everything wrong with coding interviews. Indeed, one of the first chapters in _Beyond Cracking the Coding Interview_ is _What's Broken About Coding Interviews?_ (it's one of the sneak peek free chapters in [bctci.co/free-chapters](https://bctci.co/free-chapters)).\n\nHere, I want to **collect all the arguments for the contrary view: that there are no clear better alternatives to coding interviews at Big Tech companies.**\n\n**Disclaimers:**\n\n*   I am one of the authors of _Beyond Cracking the Coding Interview_, a prep book for coding interviews. Thus, I am vested in coding interviews not going away.\n*   I love leetcoding and specialized in DS&A for my PhD, so I haven't personally experienced the _dread_ that most people feel grinding it.\n*   I've been an interviewer at Google in the past, but I'm not currently working for Big Tech, and I don't have any inside knowledge. This is just my assessment.\n*   This post is only about Big Tech. I don't think coding interviews are a good idea for startups.\n*   This post contains \"Strong Opinions, Weakly Held\". I believe everything here, but I'm very receptive to pushback and opposing data.\n\n## The rationale for coding interviews\n\nI think Big Tech companies understand that being cracked at DS&A is not really necessary to be a good SWE. I don't think coding interviews are about that at all.\n\nImagine you are a Big Tech company, like Google. You receive a massive stream of applications, and you have to trim that down to a still large number of hires. Your hiring system needs to be _scalable_:\n\n*   you need to quickly train many interviewers\n*   you need a way to evaluate candidates that minimizes interviewer bias (not _your_ bias, or a specific person's bias, but all the biases of a large, heterogeneous group)\n\nSo, the first thing you do to scale--in true engineering fashion--is decoupling hiring and team matching. But that means you cannot hire for specific tech or domain experience: You don't know in what team candidates will end up, and your teams use a bunch of different languages and tech stacks (and a lot of it is internal anyway, so you definitely can't hire for that).\n\nSo, you need a _competence assessment_ that is independent of any particulars about the job, much like the role the SAT plays for college admissions. How do you do that?\n\nIf you are a Big Tech company, what you actually want is candidates who can take any complex software system (that's not part of the candidate's previous expertise) and answer hard questions about it, like what's the best way to add a feature, how to optimize it, or how it should be refactored. In other words, the competence you want to assess is general problem-solving skills, and that's what coding interviews are designed for: you are given a tough problem that you have _ideally_ never seen before (more on this later), and asked to showcase your thought process on how you approach it. When working as intended, I believe it gives more _signal_ about your problem-solving skills and is easier to evaluate impartially than other popular interview formats, like talking about previous experience or take-home assignments. And there's an impartial way to evaluate them, by looking at the optimality of the solution.\n\nYes, there's a lot more to being a SWE than problem-solving skills--and that's why Google also does system design and behavioral interviews, but you still want to hire for this trait.\n\n## The two crucial flaws: memorization and cheating\n\nHopefully, the rationale above covered one of the most common criticisms of coding interviews: that they do not reflect the day-to-day work of an engineer. Instead, I want to focus on what _I_ think are the two biggest issues with coding interviews:\n\n1.  **Memorizing an absurd amount of leetcode problems gives you an edge.** This is the classic reason why people [_hate_](https://www.teamblind.com/post/We-wrote-the-official-sequel-to-Cracking-the-Coding-Interview-AMA-sosHtL28/44294147?commentsSort=top) coding interviews with a passion. It has led to an \"arms race\" where candidates have to memorize more and more problems to improve their odds, and interviewers keep asking about more niche topics. At the extreme, coding interviews end up feeling like a lottery, and candidates find prep a soul-sucking waste of time.\n    \n2.  **Cheating has become easy with AI.** This is a newer issue that's becoming more prevalent due to the fact that LLMs are pretty good at leetcoding. In real time, a cheater can feed the problem statement to an LLM (without obvious tales like \"select all\"), get a solution, and even a script for what to say.\n    \n\nFrom the company's side, Issue (1) is not much of an issue. It definitely undermines the \"problem-solving\" part of the interview if a candidate is just recalling the question, but, statistically, if they do enough rounds, it's unlikely to happen every round. Some people (not me) also argue that the memorization is even good for the companies because it rewards hard work and dedication.\n\nFor what it's worth, one thing we hoped to change about the interview prep discourse with BCtCI is that candidates should focus on improving their problem-solving skills rather than memorizing. See, for instance, how we [teach binary search](https://nilmamano.com/blog/binary-search) or how we [approach hard problems](https://nilmamano.com/blog/problem-solving-bctci-style). But yes, grinding is still necessary.\n\nIssue (1) also means that they'll lose a big chunk of candidates who are great SWEs but won't put up with grinding leetcode or that simply don't perform well under pressure (and, from personal experience, many great developers fall in this group). This sucks from the candidate's perspective, but if you are Google, you receive an overwhelming amount of applications from qualified candidates, so you are more OK with rejecting good candidates than accepting bad ones.\n\nIssue (2), on the other hand, has the potential to completely ruin coding interviews from the company's side. I'm seeing a quick rise of stories from frustrated interviewers who interviewed or even hired cheaters who could then not do the job (Exhibit [A](https://www.reddit.com/r/interviews/comments/1joh0w1/interview_coder_ai_is_a_complete_scam_and_total/)).\n\nI expect to see some kind of systematic response to this from Big Tech, but it's not clear what as of April 2025. [This article](https://www.cnbc.com/2025/03/09/google-ai-interview-coder-cheat.html) includes some internal comments from Google execs:\n\n> \\[Brian\\] Ong \\[Google’s vice president of recruiting\\] said candidates and Google employees have said they prefer virtual job interviews because scheduling a video call is easier than finding a time to meet in available conference rooms. The virtual interview process is about two weeks faster, he added.\n> \n> He said interviewers are instructed to probe candidates on their answers as a way to decipher whether they actually know what they’re talking about.\n> \n> “We definitely have more work to do to integrate how AI is now more prevalent in the interview process,” said Ong. He said his recruiting organization is working with Google’s software engineer steering committee to figure out how the company can refine its interviewing process.\n> \n> “Given we all work hybrid, I think it’s worth thinking about some fraction of the interviews being in person,” Pichai responded. “I think it’ll help both the candidates understand Google’s culture and I think it’s good for both sides.”\n\nI thought going back to in-person interviews would be a _no-brainer_ for a company like Google, but my reading of these comments is that they don't seem too bothered for now. ~shrug~\n\nDisclaimer: I haven't worked for a Big Tech company since before AI cheating went viral, so I don't have internal insight into what people in charge of hiring are actually thinking.\n\nTwo related arguments that I don't subscribe to are (1) that leetcode-style interviews are no longer relevant because AI can solve them, and (2) that LLMs should be allowed during coding interviews because they are allowed on the job. The fact that AI can solve coding questions doesn't change that it still gives you the important signal that you want from humans: algorithmic thinking and general problem-solving skills. We just need humans to not cheat.\n\nI'll share my thoughts on how to improve coding interviews to address these issues. First, let's see why I think the alternatives are not better.\n\n## The problems with the alternatives\n\n### Take-home assignments\n\nTake-home assignments are even more subject to cheating, so that can't be the answer to cheating. Never mind LLMs, you don't even know who did the assignment. But take-home assignments have other flaws:\n\n*   They create an asymmetry between company and candidate, where the company asks for a ton of work from the candidate without putting any effort in. \"Oh, we have way too many candidates we need to filter down to a shortlist? Send all of them a complex task to do over the weekend.\" I prefer a model where both company and candidate have to put in time. I'm more OK with take-home assignments as the final stage of the process.\n*   They favor people who are unemployed and/or have a lot of free time to polish the assignment.\n\n### Previous experience\n\nI find this too subjective to give signal about problem-solving skills, and it's more about being a good \"salesperson\". I also think it's more subject to bias: _people with a similar background as yours are probably more likely to have similar interests, and thus you may find their side-projects more interesting._\n\n## Trial periods\n\nThis makes sense to me in smaller companies, where you find a candidate with the perfect profile for the task at hand. It doesn't scale to Big Tech companies.\n\n### Other alternatives\n\nIf there are other alternatives that fulfill the same purpose as coding interviews but don't suffer from the same issues, I'd love to hear about them.\n\nOne idea I liked is going through a code review during the interview, but it's not clear that (1) it offers as much signal about problem-solving skills, and (2) it is easy to evaluate impartially.\n\n## How to improve coding interviews\n\nRight now, FAANG interviewers focus too much on \"Did they solve the question or not?\" That's because they don't get much training on how to interview well (if at all), and it's the most straightforward way to pass on a hire/no hire recommendation to the hiring committee. This leads to many interviewers just pasting the prompt in and mostly sitting in silence. This is the ideal scenario for cheaters.\n\n### The obvious things\n\nThere are obvious ways to improve this situation:\n\n*   In-person interviews. These have other benefits, like allowing the candidate to get a better sense of the company culture.\n*   Not using publicly available questions, and actively scanning for leaks.\n*   Cheating detection software (privacy is a concern here -- would it be too crazy for a company to ship a laptop to the candidate just for the interview?).\n*   Stop asking questions that require knowing some niche trick that a normal person wouldn't be able to figure out on the spot. Those reinforce a focus on memorization.\n\n### Low effort ways of countering cheating\n\nI also think that measures designed to throw LLMs off could be effective (at least in the short term) and require minimal effort, such as:\n\n*   Stating the question, or part of it, instead of writing the whole thing down\n*   Including a 'decoy' question and telling the candidate, \"Ignore that line, it is part of our anti-cheating measures.\"\n\nSee [LinkedIn discussion](https://www.linkedin.com/posts/nilmamano_if-coding-interview-cheating-is-a-big-issue-activity-7306108316126957569-3Pgy?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC6jqIwBADV70xmTEpmkxbAnN_32mssoxA8).\n\n### A fundamental tradeoff\n\nPerhaps the most effective way to counter both memorization and cheating is to make coding interviews more open ended and conversational. To use a chess analogy, a cheater may make a great move, but if you ask them to explain why they did it, they may not be able to.\n\nThe interviewer can use a coding question as a launching point, but then drill down on technical topics as they come up. So, e.g., if a candidate chooses to use a heap, the interviewer could go into:\n\n*   What made you think of using a heap? What properties are important for this problem?\n*   What are the tradeoffs of using a heap vs binary search trees?\n*   How would you go about implementing a heap that supports arbitrary priorities?\n*   Why is [heapify faster than inserting one by one](https://nilmamano.com/blog/heapify-analysis)?\n\nIf interviewers did that, it wouldn't even be necessary to ask tricky questions. They could even [ask _Fibonacci_](https://www.linkedin.com/posts/chrisverges_here-are-my-interview-questions-for-a-software-activity-7259700112145158144-YqnR?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC6jqIwBADV70xmTEpmkxbAnN_32mssoxA8).\n\nThe problem is that, the more open ended the interview is, the more difficult it is to evaluate candidates systematically. To start, you'd need better interviewers and better interviewer training. However, it seems to me that there is **a fundamental tradeoff between how objective the evaluation is and how _gameable_ the interview is by memorizing or cheating.**\n\nI don't have a good solution to this--I would love to hear yours.\n\n## More good things about coding interviews\n\n### Only one thing to study\n\nAn underrated upside of leetcode interviews is that you only need to study one thing for all the big companies. I feel like if every company asked different things, interview prep time would decrease for any specific company but increase overall.\n\nIn fact, a likely outcome of the push for fewer leetcode-style interviews is an even worse compromise: coding interviews won't completely go away, so you'll still need to grind leetcode, but you'll also have to prep a bunch of specialized stuff for each company on top of that.\n\nSee [LinkedIn discussion](https://www.linkedin.com/posts/nilmamano_an-underrated-upside-of-leetcode-interviews-activity-7312845744674091010-K0EU?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC6jqIwBADV70xmTEpmkxbAnN_32mssoxA8).\n\n### They are not based on pedigree\n\nCoding interviews act as a form of standardized testing, similar to the role of SAT for college admissions in the US. And, much like the SAT allows high-school students from all backgrounds to attend top colleges, coding interviews allow candidates from all backgrounds to get at the top companies. The leetcode grind is the same for everyone.\n\nIf we kill coding interviews without a good alternative, it seems inevitable that Big Tech companies will give more weight to resume and referrals. We all agree that's a bad thing.\n\n## Final thoughts\n\nThe best question we got in our [Reddit AMA](https://www.reddit.com/r/cscareerquestions/comments/1j4zsjj/we_wrote_the_official_sequel_to_ctci_cracking_the/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) for BCtCI was whether [we'd use coding interviews ourselves if we were in charge of hiring](https://www.reddit.com/r/cscareerquestions/comments/1j4zsjj/comment/mgdfddt/). You can see Gayle's, Mike's ([mikemroczka.com](https://www.mikemroczka.com/)), and my answers. We all said _no_ in its current form, but yes with caveats/improvements.\n\nMy favorite answer was Mike's. He's less of a proponent of leetcode-style interviews than I am, but I think he strikes a thoughtful balance between DS&A and practical stuff:\n\n> Best question so far. Yes, I would ask DS&A questions still, but not exclusively and not difficult ones. Many startups shouldn't ask them though, because most people are bad at discerning what a reasonable question is.\n> \n> I would do 4-5 rounds of interviews because less than that is hard to be significant, but more than that and you're wasting too much of a candidate's time (Netflix has a whopping 8 rounds!!). For a senior engineer role, I'd do something like this.\n> \n> Round 1: An online DS&A assessment to filter out people that can't do the simple things (easy & very simple medium questions only, not hard)\n> \n> Round 2: Live interview of DS&A (simple medium, not hard. essentially just making sure you didn't cheat on the previous round by asking you to explain your answers and code something new from scratch)\n> \n> Round 3: System design (no need for perfect answers, but I'd ask an uncommon question to ensure it was something they hadn't memorized)\n> \n> Round 4: Behavioral, with a focus on cross-team impact. This would just be a simple pass/fail and just a vibe check. It might also be skipped if the prior two rounds had good signal for emotional intelligence\n> \n> Round 5: Remote logging into a server and working on an actual bug that was fixed in our codebase before. There would be no time limit, but time on the server would be logged to weed people out who needed days to complete a simple task.\n> \n> This ends up testing a little bit of theory, practical knowledge, emotional intelligence, and the generalized SWE skillset.\n> \n> Full disclosure. This is my answer. Not the answer of every author. Again, I'd stress that the average startup wouldn't benefit from DS&A and shouldn't be asking them\n\n_Want to leave a comment? You can post under the [linkedin post](https://www.linkedin.com/posts/nilmamano_in-defense-of-coding-interviews-activity-7318682168845639680-beDg?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC6jqIwBADV70xmTEpmkxbAnN_32mssoxA8) or the [X post](https://x.com/Nil053/status/1912916866235846936)._",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/in-defense-of-coding-interviews?category=dsa",
      "author": "Nil Mamano",
      "user_id": ""
    },
    {
      "title": "Get Binary Search Right Every Time, Explained Without Code",
      "content": "A binary search recipe that works for every problem, explained without code.\n\n# Get Binary Search Right Every Time, Explained Without Code\n\nApril 15, 2025\n\n![Get Binary Search Right Every Time, Explained Without Code](/blog/binary-search/cover.png)\n\nOne of the things that makes binary search tricky to implement is that you usually need to tweak the pointer manipulation logic in subtle ways based on the specifics of the problem.\n\nE.g., an implementation that works for finding a target in a sorted array when the target is present, may not work if the target is missing. Or, it may not be clear how to tweak the code to find the last occurrence of the target instead of the first one. And of course, there are plenty of less conventional applications of binary search where the input is not an array, like [catching bike thieves](https://x.com/AlecStapp/status/1728953538301345889).\n\nIn [Beyond Cracking the Coding Interview](https://www.amazon.com/dp/195570600X), we wanted to simplify this, so we went looking for a general binary search template. Going into it, I thought we might need at least two templates, but we ended up with just one, which we called the \"transition point recipe\", and which works for every problem we tried, including the 17 problems in the binary search chapter of the book. If you find one where it doesn't work, let me know!\n\n## The transition point problem\n\nHere is the thesis of the transition point recipe:\n\nEvery binary search problem can be reduced to the 'transition point problem'.\n\nIn the 'transition point problem', you are given an array with just two values, say `1` and `2`, where all the `1`s come before the `2`s, and you need to point where it changes.\n\nE.g., in the array `[1, 1, 1, 1, 1, 2, 2, 2]`, the last `1` is at index `4` and the first `2` is at index `5`.\n\nKnowing how to solve this specific problem is key to our recipe. The specific binary search implementation is not important, but there is an invariant we can follow that makes it quite easy: ensure that the left pointer is always at a `1` and the right pointer is always at a `2`.\n\nWe give code in the book, but remembering exact code in an interview is error prone. Instead, the four bullet points below are all I _personally_ remember, and I feel confident that I can derive the rest easily.\n\n1.  Start by handling some edge cases:\n    *   The array is empty\n    *   Every value is `1`\n    *   Every value is `2`\n2.  Initialize two pointers, `left` and `right`, to the first and last indices, respectively.\n3.  For the main binary search loop, always maintain the _invariant_ that the value at `left` is `1` and the value at `right` is `2`. Let this invariant guide your pointer manipulation logic, so that you don't need to memorize any code.\n4.  Stop when the `left` and `right` pointers are next to each other (i.e., `left + 1 == right`).\n\nCombining the invariant with the stopping condition, we get that, at the end, `left` will be at the last `1` and `right` will be at the first `2`.\n\nThese bullet points rely on two ideas to make binary search easier: (1) handling edge cases upfront, and (2) letting strong invariants guide the implementation. Notice how the invariant even guides the edge cases at the beginning, as they are the necessary ones to be able to initialize `left` and `right` in a way that satisfies it.\n\n## The reduction\n\nOk, so now, let's take for granted that we can solve the transition point problem. How does this help us solve other binary search problems?\n\nThe idea is to come up with a (problem-specific) _predicate_, like `< target`, `>= target`, or `x % 2 == 0`, which splits the search range into two regions, the \"before\" region and the \"after\" region.\n\nThis predicate is a function that takes an element of the search range and returns a boolean, and -- as you probably saw coming -- it is key that all the elements with `true` values come before the elements with `false` values (or the other way around).\n\nThen, we can use the solution to the transition point problem to find the transition point between the 'before' and 'after' regions. The only difference is that, instead of checking boolean values directly, we check the result of the predicate.\n\nYou can even wrap the predicate in a function, which we called `is_before(x)` in the book, which tells you whether a given element is in the 'before' region. Then, it's really obvious that we are just solving the transition point problem every time.\n\nThe only part that requires some thinking is choosing the right transition point. For example:\n\n*   if we want to find the _first_ occurrence of `target` in a sorted array, we can use `is_before(x) = x < target`, which means that, if `target` is present, the first occurrence is the first element in the 'after' region (so, we can check/return the `right` pointer at the end).\n*   if we want to find the _last_ occurrence of `target` in a sorted array, we can use `is_before(x) = x <= target`, which means that, if `target` is present, the last occurrence is the last element in the 'before' region (so, we can check/return the `left` pointer at the end).\n\nAnd so on for other problems.\n\n![Binary search recipe](/blog/binary-search/meme.png)\n\n## Practice\n\nYou can try the transition-point recipe on all the problems from the binary search chapter of the book online at [start.interviewing.io/beyond-ctci/part-vii-catalog/binary-search](https://start.interviewing.io/beyond-ctci/part-vii-catalog/binary-search), even if you don't have the book. There, you can also find all our solutions using the recipe, in Python, JS, Java, and C++.\n\nBy the way, the binary search chapter of the book is free -- it's in [bctci.co/free-chapters](https://bctci.co/free-chapters).\n\n_Want to leave a comment? You can post under the [linkedin post](https://www.linkedin.com/posts/nilmamano_get-binary-search-right-every-time-explained-activity-7319072161481084932-74ga?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC6jqIwBADV70xmTEpmkxbAnN_32mssoxA8) or the [X post](https://x.com/Nil053/status/1913316583298011224)._",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/binary-search?category=dsa",
      "author": "Nil Mamano",
      "user_id": ""
    },
    {
      "title": "Problem Solving BCtCI Style",
      "content": "A problem walkthrough using the concepts from Beyond Cracking the Coding Interview.\n\n# Problem Solving BCtCI Style\n\nMarch 19, 2025\n\n![Problem Solving BCtCI Style](/blog/problem-solving-bctci-style/cover.png)\n\nHere's a thought: You don't want the first time you think about the question _\"What should I do if I get stuck in a coding interview?\"_ to be when you are stuck in a coding interview.\n\nIn a way, getting stuck in a coding interview is an opportunity. The main goal of the interview is to see your problem-solving thought process, and being stuck is the ideal time to showcase it.\n\nBut you want to be prepared. It's valuable to have a plan for this exact scenario. We all dread blanking out in an interview, but having a plan makes it easy to simply focus on executing it. So, let's talk about what such a plan could look like in this blog post.\n\nIn [Beyond Cracking the Coding Interview](https://www.amazon.com/dp/195570600X), we go over all the steps in an interview, and our best tips to do well in each of them:\n\n![Interview checklist, from the chapter 'Anatomy of a Coding Interview'](/blog/problem-solving-bctci-style/interview-checklist.png)\n\nIn this blog post, I'll zoom in on the problem-solving step, \"Design the Algorithm,\" and illustrate the thought process with a problem.\n\nAs you can see, we break it down into four steps:\n\n1.  **Minimally sketch the naive solution** to establish a baseline.\n2.  **Identify upper and lower bounds** using big O analysis to narrow down the range of possible solutions.\n3.  **Look for triggers (Keywords)** that point to a specific approach.\n4.  **Employ boosters**: problem-solving strategies that give you the \"boost\" you need when you are stuck.\n\nThese are not revolutionary ideas -- it's what good problem solvers do and think about instinctively. One of the main goals of the book, and of this blog post, is to spell out the thought process of people who are really good at this in a relatable way so that anyone can reproduce it.\n\nWe playfully call this the **MIKE template** (**M**inimally sketch brute force, **I**dentify bounds, **K**eywords (triggers), **E**mploy boosters) after [Mike Mroczka](https://www.mikemroczka.com/), one of the authors of BCtCI.\n\nRather than expanding on these now, we'll see them in action with the following problem.\n\n## Problem Statement\n\nThe problem is based on [LeetCode 3458](https://leetcode.com/problems/select-k-disjoint-special-substrings/description/), which appeared in a recent contest. You can go and give it a try before reading on (it's labeled as medium, but I think it's on the harder end of medium). The thought process I'll walk through here is based on how I solved it during the contest.\n\nGiven a string `s`, a substring of `s` is **special** if any character in it does not appear outside it.\n\nFor example, if `s` is `\"abcba\"`:\n\n*   `\"bcb\"` is a special substring because `'b'` and `'c'` do not appear in `s` outside `\"bcb\"`.\n*   `\"abc\"` is not a special substring because `'a'` appears in `s` outside `\"abc\"`.\n\nGiven a string `s` consisting of `n` lowercase English letters, determine the maximum number of disjoint **special** substrings. Two substrings are disjoint if they do not overlap.\n\n```\nExample 1: s = \"abcba\"\nOutput: 1\nThe special substrings are \"abcba\", \"bcb\", and \"c\". They all overlap with each other, so we can only pick 1.\n\nExample 2: s = \"banana\"\nOutput: 2\nThe special substrings are \"b\", \"banana\", and \"anana\". We can pick \"b\" and \"anana\".\n\n```\n\nConstraints:\n\n*   `2 <= n <= 10^5`\n*   `s` consists only of lowercase English letters.\n\n## Digesting the problem\n\nFirst, we need to digest what the problem is asking. This problem follows a common pattern: it introduces a kind of esoteric definition, \"special substring\", and then asks us to do something with it.\n\nTo make sure we understand what a special substring is, it's good to look at a few examples, starting with the provided ones. For instance, in `\"abcba\"`, do you understand why `\"a\"` is not special but `\"c\"` is?\n\nTake some time to come up with your own examples. Rushing to solving a problem before understanding it well is a common but often costly mistake.\n\n## Approach\n\nSometimes, it helps to tackle just one part of the problem first, so we can start making progress.\n\nWe can think of an algorithm with 2 parts:\n\n*   **Part A**: Find all the special substrings.\n*   **Part B**: Find the most non-overlapping special substrings.\n\nLet's start with part A.\n\n## Part A: Find all the special substrings\n\nWe'll walk through the MIKE template.\n\n## M: Minimally sketch brute force\n\nThe key here is to not overthink it. We just want to get the ball rolling and have a baseline we can improve upon.\n\nSince we don't want to spend too much time in an interview, you could even just describe the idea in a sentence and move on. But we prefer to briefly sketch it in very high-level pseudocode. We call it 'intended English': it's written like English, but with indentation to show the code structure:\n\n```\nAlgo 1: brute force\nT: O(n^4)\nfor each possible substring start\n  for each possible substring end\n    # check if it is special\n    for each letter inside the substring\n      for each letter outside the substring\n        if they match, it is not special\n\n```\n\nInterviews often involve considering trade offs between algorithms, so it's a good habit to give them names and list their time/space complexity.\n\nIn this case, the space complexity depends on how many special substrings we might find, which is not clear yet, so we'll leave it out for now.\n\nSketching the brute force solution helps us ensure we understand the problem (and if we are solving for the wrong thing, we give the interviewer a chance to let us know).\n\n## I: Identify upper and lower bounds\n\nWe can use big O analysis to narrow down the range of possible solutions. An upper bound means \"we don't have to consider any solution that takes longer than this\", and a lower bound means the opposite: \"we don't have to consider any solution that takes less time than this\". In the book, we go over two ways of establishing an upper bound and two ways of establishing a lower bound:\n\nUpper bounds:\n\n*   **Brute force upper bound**: we just saw that we can find all special substrings in `O(n^4)` time, so we don't have to consider any solution that takes longer than that.\n*   **TLE (Time Limit Exceeded) upper bound**: here is where we use the problem constraints to establish an upper bound. The problem says that `n <= 10^5`, which usually means that `O(n^2)` solutions are too slow, but `O(n log n)` or faster solutions are fine.\n\nLower bounds:\n\n*   **Output-size lower bound**: the _space_ taken by the output is a lower bound for the time complexity, because that's how long it takes just to write the output. In our case, the output of the overall problem is just a number, so this lower bound is trivial: `O(1)`. Bounds are not always useful!\n*   **Task-based lower bound**: some problems involve an inherent task that **any** solution must fulfill. The runtime of this task is a lower bound. In this case, we know we _at least_ need to read every letter in the input, so we have a lower bound of `O(n)`. In other words, we can rule out solutions that take `O(log n)` or `O(1)` time.\n\nCombining our findings, we can narrow down our search range to `O(n log n)` or `O(n)` algorithms (something like `O(n log^2 n)` would also be fine, it's just less common).\n\n## K: Keywords (triggers)\n\nThere are certain properties of problems that point to a specific approach. Here are some triggers we can identify for this problem:\n\n*   finding substrings `->` sliding windows\n*   `O(n log n)` possible target complexity `->` sorting or heaps\n\nUnfortunately, triggers are not a guarantee, and these triggers don't seem to help for this problem:\n\n*   In sliding windows, once you move past a character, you don't later go back. So, in Example 1, it would be impossible to find both `\"abcba\"` and `\"bcb\"`: if you find `\"abcba\"` first, the _right_ pointer would have to go back to find `\"bcb\"`. But if you find `\"bcb\"` first, the _left_ pointer would have to go back to find `\"abcba\"`.\n*   Sorting doesn't seem like a good fit because the input order is important.\n\nDo you think I missed any other triggers?\n\n## E: Employ boosters\n\nSo, triggers didn't help, and brute force is still far from the target complexity. It's time to employ boosters.\n\nHere's an overview:\n\n![Boosters overview](/blog/problem-solving-bctci-style/boosters.png)\n\nThe boosters are roughly ordered, but we don't always have to use them in order. In fact, here's a plot twist: what we did at the beginning, splitting the problem into two parts, is the third booster: **Decrease the Difficulty `->` Break Down the Problem.**\n\n## Booster 1: Brute force optimization\n\nThe first booster is straightforward: take the brute force pseudocode we already have and try to optimize it.\n\nIn the boosters diagram, we list three ways to go about it. One of them is the **Data structure pattern**. Many bottlenecks come from having to do some calculation inside a loop. In those situations, ask yourself,\n\n_\"Do I know of any data structure which makes this type of operation faster?\"_\n\nFor this problem, we can use a hash set to optimize the innermost loop:\n\n```\nAlgo 2: set optimization\nT: O(n^3)\nfor each possible substring start\n  for each possible substring end\n    # check if it is special\n    dump the substring into a set\n    for each letter outside the substring\n      if it is in the set, it is not special\n\n```\n\nIf you have working code or pseudocode but think of an optimization or better approach, do NOT edit your code. Copy-paste it and work on a separate copy. This way, if you don't have time to finish or realize it's wrong, you'll still have the previous working version.\n\n## Booster 2: Hunting for properties\n\nWe got down to `O(n^3)` time, but we know we still need to bring this down to the target complexity.\n\nLet's say we don't know how to optimize the code further. Often, the breakthrough comes from uncovering some \"hidden\" observation or _property_ not explicitly mentioned in the statement. Our second booster is to go hunting for those.\n\nIn the book, we discuss a bunch of ways of doing this, but the most basic and effective one is to try to solve the problem manually with a non-trivial example. By non-trivial, we mean that is is not some weird edge case, which would not be helpful for figuring out a general algorithm.\n\nLet's actually do that: take `s = \"mississippi\"` and **manually** try to find all the special substrings.\n\nDon't overthink it. Don't think about algorithms yet. Just write them down.\n\nDone? Ok, _now_ try to reverse-engineer what shortcuts your brain took. This is one property you may have noticed:\n\n### Property 1\n\n**Property 1:** a special substring must start at the first occurrence of a letter.\n\nYou may have noticed this property when your brain skipped over the second, third, or fourth `'i'`s in `mississippi` and intuitively realized that there is no special substring starting at those. Writing down the property _formalizes_ this instinct and ropes in the interviewer.\n\nNow that we have a property, we have to find a way to use it. **Property 1** allows us to optimize the outer loop: it means we only have `26 = O(1)` possible starts to check (problems where the input consists of only lowercase letters often have optimizations like this).\n\nAs we iterate through the possible starts, we can track letters seen so far (e.g., in a hash set):\n\n```\nAlgo 3: selective start\nT: O(26 * n^2) = O(n^2)\nfor each possible substring start i\n  if seen s[i] before\n    continue\n  add s[i] to seen set\n  for each possible substring end\n    # check if it is special\n    dump the substring into a set\n    for each letter outside the substring\n      if it is in the set, it is not special\n\n```\n\nWe like to write down the big O simplification (`O(26 * n^2) = O(n^2)`), so the interviewer doesn't think we missed steps.\n\nWe haven't hit our target time complexity yet, so let's keep hunting for properties. Here is another one:\n\n### Property 2\n\n**Property 2:** of all the special substrings that start at a given letter, we only care about the shortest one.\n\nOur ultimate goal is to find the most non-overlapping special substrings. If we can choose between two special substrings, one of which contains the other, it is always \"optimal\" or, at least, \"safe\" to pick the smaller one.\n\nFor instance, if `s` is `\"baa\"`, we have two choices for special substrings starting at `'b'`: `\"baa\"` and `\"b\"`. We should pick `\"b\"` so that the `\"aa\"` part can be in another disjoint special substring.\n\nAgain, when we find a property, we need to think of how to apply it. **Property 2** means that, for each starting point `i`, we can grow a substring one letter at a time, and stop as we find the first special substring.\n\nLet's break this down a bit more: say you start at index `i`.\n\n*   If you find a letter `c` that appears at some later point, we need to grow the substring up to that index.\n*   If you find a letter `c` that appears before `i`, we can stop the search. No substring starting at `i` can be special.\n\nFor example, imagine `i` starts at the first `'b'` in the following string:\n\n```\n\"abbbbbabbba\"\n  ^\n  i\n\n```\n\nThat means we need to grow the substring at least up to the last `'b'` in the string:\n\n```\n\"abbbbbabbba\"\n  ^       ^\n  i   need to grow up to here\n\n```\n\nAs we grow the substring, we hit an `'a'`, which appears before `i`, and we realize that no substring starting at `i` can be special.\n\n```\n\"abbbbbabbba\"\n  ^    ^\n  i invalid\n\n```\n\nWe can now add this logic to our algorithm. We can start the algorithm by computing the first and last index of each letter (this is an example of the **preprocessing pattern** in the boosters diagram -- it's common for properties from Booster 2 to enable optimizations from Booster 1).\n\nThen, as we grow each substring, we keep track of the farthest index we need to reach. (This is actually a common pattern in sliding window algorithms, where we maintain information about the window as it 'slides', rather than computing it from scratch every time the window moves. So, the 'sliding windows' trigger wasn't completely off).\n\n```\nAlgo 4: smallest special substring\nT: O(26 * n) = O(n)\nS: O(26 * n) = O(n)\npreprocessing: compute the first and last index of each letter\n\nfor each possible substring start i\n  for each index j starting at i\n    if s[j] appears before i\n      no special string starts at i\n    else\n      must_reach = max(must_reach, last occurrence of s[j])\n    if j reached must_reach:\n      s[i]...s[j] is a special substring (the shortest one starting at s[i])\n\n```\n\nWe got the time down to `O(n)`. Since we hit the lower bound, we can be confident Part A is as good as it can be, and we can move on to Part B.\n\n## Part B: Find the most non-overlapping special substrings\n\nLet's be honest: even if in the book we really [emphasize](https://bctci.co/question-landscape) developing your problem-solving skills by using the MIKE template and the boosters, knowing a bunch of leetcode questions DOES give you an edge in coding interviews. So, I'll tell you how I actually solved this problem in the contest. I realized that Part B is just a variation of a classic greedy problem: most non-overlapping intervals. Indeed, a substring can be seen as an interval of the string.\n\nThe \"most non-overlapping intervals\" problem is in BCtCI, so I already knew that it can be solved with a greedy algorithm that sorts the intervals by their end time and then iterates through them, picking the ones that don't overlap with the previous one ([here](https://leetcode.com/problems/non-overlapping-intervals/description/) is a similar problem on leetcode). This algorithm fits within our target time complexity, so I didn't have to think beyond that.\n\nIf I didn't already know the solution, I would have walked through the MIKE template again for Part B.\n\n## Full implementation\n\nHere is a full implementation:\n\n```py\n# T: O(26 * n) = O(n)\n# S: O(26 * n) = O(n)\ndef select_k_disjoint_special_substrings(s):\n    special_substrings = find_special_substrings(s)  # Part A\n    return most_non_overlapping_intervals(special_substrings)  # Part B\n\ndef find_special_substrings(s):  # Algo 4\n    # Preprocessing: compute the first and last index of each letter\n    first_idx = {}\n    last_idx = {}\n    for i, char in enumerate(s):\n        if char not in first_idx:\n            first_idx[char] = i\n        last_idx[char] = i\n\n    special_substrings = []\n    for i in range(len(s)):\n        if i != first_idx[s[i]]:\n            continue\n\n        must_reach = i\n        for j in range(i, len(s)):\n            if first_idx[s[j]] < i:\n                break\n            must_reach = max(must_reach, last_idx[s[j]])\n\n            if j == must_reach:\n                special_substrings.append((i, j))\n                break\n\n    return special_substrings\n\ndef most_non_overlapping_intervals(intervals):  # Classic Greedy\n    intervals.sort(key=lambda x: x[1])  # Sort by endpoint\n    count = 0\n    prev_end = -math.inf\n    for l, r in intervals:\n        if l > prev_end:\n            count += 1\n            prev_end = r\n    return count\n\n```\n\nYou may think that the bottleneck is the sorting, but it's not. Recall that there are only up to 26 special substrings (by Property 1). Sorting `26` intervals takes `O(26 log 26) = O(1)` time.\n\n## Conclusion\n\nI wanted to give an overview of all the high-level ideas for problem-solving in leetcode-style interviews. We could dive a lot deeper into any of those ideas, so this blog post may feel a bit rushed, but the meta-point is that **you should have a plan for when you are stuck in an interview** (and you should be following it during your practice sessions so it becomes second nature). It's not important that you use the MIKE template -- _your_ plan should work for _you_. But the ideas covered in this post should probably be part of it.\n\nIf you have any comments, let me know on [linkedin](https://linkedin.com/in/nilmamano/) or [X](https://x.com/Nil053).",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/problem-solving-bctci-style?category=dsa",
      "author": "Nil Mamano",
      "user_id": ""
    },
    {
      "title": "Lazy vs Eager Algorithms",
      "content": "Exploring the tradeoffs between lazy and eager implementations of common algorithms.\n\n# Lazy vs Eager Algorithms\n\nOctober 16, 2024\n\n![Lazy vs Eager Algorithms](/blog/lazy-vs-eager/cover.png)\n\nWarning: I have not tested any code snippet below. Please let me know if you find a bug.\n\n## Introduction\n\nMost algorithms have multiple valid implementations. For instance, in a binay tree problem, you have multiple ways of handling NULL nodes. I'm currently writing **Beyond Cracking the Coding Interview** ([beyondctci.com](https://www.beyondctci.com/)), which means that my co-authors and I need to take a stance on what version of each algorithm to use. Ideally, we want to show the simplest version of each algorithm:\n\n*   Easy to recall for interview,\n*   Easy to explain to interviewers,\n*   Easy to debug by hand,\n*   Short, so that it is quick to code.\n\nIn the book, we don't claim that the version we show is \"the best\" - we say to use the one that works best for you. But showing one in the book is an implicit endorsement.\n\nOne particular decision that comes up again and again with recursive algorithms is choosing between the **lazy** version and the **eager** version of an algorithm.\n\n*   An **eager** recursive function expects 'valid' inputs and ensures to only call the recursive function with 'valid' inputs. We can also call it a **clean** (call) **stack** algorithm.\n*   A **lazy** recursive algorithm allows 'invalid' inputs, so it starts by validating the input. Then, it calls the recursive function without validating the inputs passed to it. We can also call it a **dirty stack** algorithm.\n\nWhat 'valid' means depends on the algorithm--we'll see plenty of examples. We'll also translate the concept of eager vs lazy to iterative algorithms.\n\n## Lazy vs Eager Tree Traversals\n\nAn **eager** tree traversal eagerly validates that the children are not NULL before passing them to the recursive function. A **lazy** tree traversal doesn't, so it needs to check if the current node is NULL before accessing it.\n\nFor instance, here is eager vs lazy preorder traversal:\n\n```python\nclass Node:\n  def __init__(self, val, left=None, right=None):\n    self.val = val\n    self.left = left\n    self.right = right\n\ndef preorder_traversal_eager(root):\n  res = []\n\n  # CANNOT be called with node == None\n  def visit(node):\n    res.append(node.val)\n    if node.left:\n      visit(node.left)\n    if node.right:\n      visit(node.right)\n\n  if not root:\n    return []\n  visit(root)\n  return res\n\ndef preorder_traversal_lazy(root):\n  res = []\n\n  # CAN be called with node == None\n  def visit(node):\n    if not node:\n      return\n    res.append(node.val)\n    visit(node.left)\n    visit(node.right)\n\n  visit(root)\n  return res\n\n```\n\nBoth have the same runtime and space analysis. Even the constant factors probably don't change much, so it comes down to style preference. Which one do you prefer?\n\n## Lazy vs Eager graph DFS\n\nAn **eager** graph DFS eagerly checks that the neighbors are not already visited before passing them to the recursive function. A **lazy** graph DFS doesn't, so it needs to check if the current node is already visited.\n\n```python\n# Returns all nodes reachable from start\ndef dfs_eager(adj_lists, start):\n  res = []\n  visited = set()\n\n  def visit(node):\n    res.append(node)\n    for neighbor in adj_lists[node]:\n      if neighbor not in visited:\n        visited.add(neighbor)\n        visit(neighbor)\n\n  visited.add(start)\n  visit(start)\n  return res\n\ndef dfs_lazy(adj_lists, start):\n  res = []\n  visited = set()\n\n  def visit(node):\n    if node in visited:\n      return\n    visited.add(node)\n    res.append(node)\n    for neighbor in adj_lists[node]:\n      visit(neighbor)\n\n  visit(start)\n  return res\n\n```\n\nFor a graph DFS, we can also do a mix between lazy and eager: we can eagerly check if nodes are already visited, and lazily mark them as visited:\n\n```python\ndef dfs_lazy(adj_lists, start):\n  res = []\n  visited = set()\n\n  def visit(node):\n    visited.add(node)\n    res.append(node)\n    for neighbor in adj_lists[node]:\n      if neighbor not in visited:\n        visit(neighbor)\n\n  visit(start)\n  return res\n\n```\n\nAgain, they all have the same analysis. Which one do you prefer?\n\n## Lazy vs Eager grid algorithms\n\nConsider the same DFS algorithm but on a grid of 0's and 1's. The 0's are walkable cells, the 1's are obstacles, and walkable cells next to each other are connected. This time, we need to check that the neighbors are not out of bounds, which we can do lazily or greedily.\n\n```python\n# Returns all cells reachable from (start_row, start_col).\ndef grid_dfs_eager(grid, start_row, start_col):\n  nr, nc = len(grid), len(grid[0])\n  res = []\n  visited = set()\n\n  def visit(row, col):\n    res.append((row, col))\n    for dir in ((-1, 0), (1, 0), (0, 1), (0, -1)):\n      r, c = row + dir[0], col + dir[1]\n      if 0 <= r < nr and 0 <= c < nc and grid[r][c] == 0 and (r, c) not in visited:\n        visited.add((r, c))\n        visit(r, c)\n\n  # Assumes (start_row, start_col) is within bounds\n  visited.add((start_row, start_col))\n  visit(start_row, start_col)\n  return res\n\ndef grid_dfs_lazy(grid, start_row, start_col):\n  nr, nc = len(grid), len(grid[0])\n  res = []\n  visited = set()\n\n  def visit(row, col):\n    if row < 0 or row >= nr or col < 0 or col >= nc or grid[row][col] == 1:\n      return\n    if (row, col) in visited:\n      return\n    visited.add((row, col))\n    res.append((row, col))\n    for dir in ((-1, 0), (1, 0), (0, 1), (0, -1)):\n      visit(row + dir[0], col + dir[1])\n\n  visit(start_row, start_col)\n  return res\n\n```\n\n## Lazy vs Eager Memoization DP\n\nIn a **lazy** memoization DP (Dynamic Programming) algorithm, we call the recursive function for a subproblem without checking first if we have already computed that subproblem. In an **eager** algorithm, we only call the recursive function for subproblems that we still need to compute.\n\n```python\n# Returns all cells reachable from (start_row, start_col).\ndef fibonacci_eager(n):\n  memo = {}\n\n  def fib_rec(i):\n    if i <= 1:\n      return 1\n    if i-1 in memo:\n      prev = memo[i-1]\n    else:\n      prevprev = fib_rec(i-1)\n    if i-2 in memo:\n      prevprev = memo[i-2]\n    else:\n      prev = fib_rec(i-2)\n    memo[i] = prev + prevprev\n    return memo[i]\n\n  return fib_rec(n)\n\ndef fibonacci_lazy(n):\n  memo = {}\n\n  def fib_rec(i):\n    if i <= 1:\n      return 1\n    if i in memo:\n      return memo[i]\n    memo[i] = fib_rec(i-1) + fib_rec(i-2)\n    return memo[i]\n\n  return fib_rec(n)\n\n```\n\nFor memoization DP, I think **lazy** is cleaner and more conventional.\n\n## Lazy vs Eager Iterative Tree traversals\n\nConsider a level-order traversal on a binary tree. A level-order traversal is an iterative algorithm that uses a queue data structure.\n\n*   A **lazy** version puts children in the queue without checking if they are NULL first. We can call it a **dirty queue** algorithm.\n*   An **eager** version checks for NULL nodes and avoids putting them in the queue. We can call it a **clean queue** algorithm.\n\n```python\ndef level_order_traversal_eager(root):\n  if not root:\n    return []\n  res = []\n  Q = deque([root])\n  while Q:\n    node = Q.popleft()\n    res.append(node.val)\n    if node.left:\n      Q.append(node.left)\n    if node.right:\n      Q.append(node.right)\n  return res\n\ndef level_order_traversal_lazy(root):\n  res = []\n  Q = deque([root])\n  while Q:\n    node = Q.popleft()\n    if not node:\n      continue\n    res.append(node.val)\n    Q.append(node.left)\n    Q.append(node.right)\n  return res\n\n```\n\n## Eager Graph BFS is better than lazy Graph BFS\n\nThis is the first exception where one is better than the other in terms of big O analysis. The **lazy** BFS allows adding already-visited nodes to the queue, while the **eager** one does not. We'll first look at the two versions, and then analyze them.\n\n```python\ndef graph_bfs_eager(adj_lists, start):\n  res = []\n  visited = set()\n  visited.add(start)\n  Q = deque([start])\n\n  while Q:\n    node = Q.popleft()\n    res.append(node.val)\n    for neighbor in adj_lists[node]:\n      if neighbor not in visited:\n        visited.add(neighbor)\n        Q.append(neighbor)\n  return res\n\ndef graph_bfs_lazy(adj_lists, start):\n  res = []\n  visited = set()\n  Q = deque([start])\n\n  while Q:\n    node = Q.popleft()\n    if node in visited:\n      continue\n    visited.add(node)\n    res.append(node)\n    for neighbor in adj_lists[node]:\n      Q.append(neighbor)\n  return res\n\n```\n\nIt may come as a surprise that these two are **not** equivalent like all the other examples.\n\nLet's say `V` is the number of nodes and `E` is the number of edges. To keep things simple, consider that the graph is connected, meaning that `E` is at least `V-1` and at most `O(V²)`.\n\nBoth versions take `O(E)` time. The difference is in the space complexity: the eager version takes `O(V)` space because we never have the same node twice in the queue. The lazy version takes `O(E)` space because we allow the same nodes multiple times in the queue.\n\nTo see this, consider a complete graph:\n\n![Complete graph](/blog/lazy-vs-eager/completegraph.png)\n\n1.  When we visit start, we add A, B, C, D, E to the queue. Now the queue is: `[A, B, C, D, E]`\n2.  When we visit A, we add start, B, C, D, E to the queue. Now the queue is: `[B, C, D, E, start, B, C, D, E]`\n3.  When we visit B, we add start, A, C, D, E to the queue. Now the queue is: `[C, D, E, start, B, C, D, E, start, A, C, D, E]`\n4.  And so on.\n\nBy the time we finish popping the nodes added as neighbors of the start node, we've done `V` queue pops and `V²` queue appends, so the queue size is `O(V²)`.\n\nSo, why didn't this happen for other lazy algorithms we have seen?\n\n*   For tree traversals, each tree node has a single parent that it can be reached from, so we don't need to worry about the same node appearing twice in the call stack or in the level-order traversal queue.\n*   For graph DFS, **every node in the call stack** is marked visited, so if we call `visit()` on a node that is already in the call stack, we'll immediately return as we'll see it is marked as visited.\n\n## Eager Dijkstra is better than Lazy Dijkstra, but harder to implement\n\nI wrote extensively about different Dijktsra implementations in [this Dijkstra blog post](https://nilmamano.com/blog/dijkstra.html).\n\nDijkstra is similar to BFS, with the main difference that it uses a priority queue (PQ) instead of a queue to visit the nodes that are closer first (in terms of shortest paths).\n\nIn BFS, when a node is added to the queue, its distance from the starting node is already established and there is never a reason to add it again to the queue. In Dijkstra, when a node is added to the PQ, we might later find a shorter path while it is still in the PQ. When that happens, we can do two things:\n\n*   **Lazy Dijkstra**: just add the node again with the new, improved distance. It will get popped before the previous occurrence because it has higher priority in the PQ. When a node with a \"stale\" distance gets popped off from the queue, we just ignore it.\n*   **Eager Dijkstra** (called textbook Dijkstra in the other blog post): instead of adding the node again, find the existing occurrence of it in the PQ, and update it with the new found distance. This guarantees that the same node never appears twice in the PQ.\n\nBoth versions take `O(E*log V)` time, but eager is more space efficient, analogously to eager BFS: `O(V)` for eager Dijkstra vs `O(E)` for lazy Dijkstra.\n\nHere is lazy Dijkstra:\n\n```python\ndef dijkstra_lazy(adj_lists, start):\n  dist = defaultdict(int)\n  dist[start] = 0\n  visited = set()\n  PQ = [(0, start)]\n  while PQ:\n    _, node = heappop(PQ)  # Only need the node, not the distance.\n    if node in visited:\n      continue  # Not the first extraction.\n    visited.add(node)\n    for neighbor, weight in adj_lists[node]:\n      if dist[node]+weight < dist[neighbor]:\n        dist[neighbor] = dist[node]+weight\n        # Neighbor may already be in the PQ; we add it anyway.\n        heappush(PQ, (dist[neighbor], neighbor))\n  return dist\n\n```\n\nUnfortunately, eager Dijkstra is not so easy to implement in Python because we are missing the `decrease_key()` operation in a heap (and Python does have a self-balancing BST data structure, which can also be used for eager Dijkstra). You can see a BST-based C++ implementation in my other blog post.\n\nThe `dijkstra_lazy()` algorithm above is more or less standard and it has been known as \"lazy Dijkstra\" for a while. However, it is possible to make an even lazier version which has the same runtime and space analysis (but likely bigger constant factors). The idea is that instead of only adding to the PQ the neighbors for whom we find an improved distance, we can simply add all of them, and discard duplicates once we extract them from the PQ:\n\n```python\ndef dijkstra_super_lazy(adj_lists, start):\n  dist = defaultdict(int)\n  dist[start] = 0\n  PQ = [(0, s)]\n  while PQ:\n    d, node = heappop(PQ)\n    if dist[node] != math.inf: continue\n    dist[node] = d\n    for neighbor, weight in adj_lists[node]:\n      heappush(PQ, (dist[node]+weight, neighbor))\n  return dist\n\n```\n\n## So, Lazy or Eager?\n\nWe could keep looking at lazy vs eager algorithms, but I'll stop here. In aggregate, these are the pros and cons that I see:\n\n### Pros of lazy algorithms\n\n*   **Lazy algorithms require less code.** This is because you only need to validate the parameters of the recursive function once at the beginning, instead of validating what you pass to each recursive call. This is specially true in binary tree problems, where you usually have two recursive calls. It doesn't apply as much for graphs.\n*   **Lazy algorithms require less indentation.** For instance, in graph problems, we don't need to do checks inside the for loop over the neighbors.\n*   **Lazy algorithms do not require special handling for the first recursive call.** You don't need to worry about things like checking if the root is NULL or marking the start node as visited.\n*   **Lazy recursive functions have simpler preconditions.** You can just pass anything to them, and they work.\n\n### Pros of eager algorithms\n\n*   **For a graph BFS, eager has a better space complexity.** This is a case where eager is objectively better. (Eager Dijkstra is also better but it is not expected to be implemented in interviews. Your interviewer is probably expecting lazy Dijkstra.)\n*   **Eager algorithms do fewer recursive calls or iterations.** In a binary tree, the number of NULL nodes is always one more than the number of internal nodes. This means that a lazy traversal does twice as many recursive calls/iterations as the eager counterpart. This could make a big difference if you want to debug the code manually. For instance, in this picture, you can see that adding NULLs to the queue makes visualizing the steps more painful:\n\n![Evolution of the queue during level-order traversal](/blog/lazy-vs-eager/levelorder.png)\n\n*   **Eager algorithm can 'feel safer'.** A friend commented that, with a lazy algorithm, they feel like they are missing an edge case.\n\n### My preference\n\nHere are my personal preferences for coding interviews (not those of the other authors of 'Beyond Cracking the Coding Interview'):\n\n**Strong preferences:**\n\n*   For BFS, use eager. This one is clear cut.\n*   For memoization DP, use lazy. It is much cleaner to code.\n*   For Dijkstra, use lazy Dijkstra (not super lazy Dijkstra). It is what is realistic to do in an interview and probably what the interviewer expects.\n\n**Weak preferences:**\n\n*   For binary tree traversals (iterative or recursive), use lazy. It is a bit cleaner.\n*   For graph DFS, use eager. It is a bit more standard, and aligned with a graph BFS.\n\nIn the book, we'll definitely mention that some algorithms can be implemented in a lazy or eager way (in way less detail than here), and that you should choose the one that feels easier to you. But, we still need to pick one to show in the problem solutions. One idea is trying to be consistent throughout (e.g., doing all tree and graph traversals in an eager way). If you have an opinion on which one is better, please reach out! I'd love to hear it.",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/lazy-vs-eager?category=dsa",
      "author": "Nil Mamano",
      "user_id": ""
    },
    {
      "title": "Actually Implementing Dijkstra's Algorithm",
      "content": "A comprehensive guide to different implementations of Dijkstra's algorithm, with code.\n\n# Actually Implementing Dijkstra's Algorithm\n\nJune 22, 2020\n\n![Actually Implementing Dijkstra's Algorithm](/blog/implementing-dijkstra/cover.png)\n\n## Introduction\n\nDijkstra's algorithm for the shortest-path problem is one of the most important graph algorithms, so it is often covered in algorithm classes. However, going from the pseudocode to an actual implementation is made difficult by the fact that it relies on a priority queue with a \"decrease key\" operation. While most programming languages offer a priority queue data structure as part of their standard library, this operation is generally not supported (e.g., in C++, Java or Python). In this blog, we go over the different ways to implement Dijkstra's algorithm with and without this operation, and the implications of using each. All in all, we consider 5 versions of Dijkstra (names mostly made up by me):\n\n*   **Textbook Dijkstra**: the version commonly taught in textbooks where we assume that we have a priority queue with the \"decrease key\" operation. As we said, this often does not hold true in reality.\n*   **Linear-search Dijkstra**: the most naive implementation, but which is actually optimal for dense graphs.\n*   **Lazy Dijkstra**: practical version which does not use the \"decrease key\" operation at all, at the cost of using some extra space.\n*   **BST Dijkstra**: version which uses a self-balancing binary search tree to implement the priority queue functionality, including the \"decrease key\" operation.\n*   **Theoretical Dijkstra**: version that uses a Fibonacci heap for the priority queue in order to achieve the fastest possible runtime in terms of big-O notation. This is actually impractical due to the complexity and high constant factors of the Fibonacci heap.\n\nRoughly, each of the 5 versions corresponds to a different data structure used to implement the priority queue. Throughout the post, let `n` be the number of nodes and `m` the number of edges. Here is summary of the resulting runtime and space complexities:\n\n*   **Textbook Dijkstra**: indexed binary heap. Runtime: `O(m*log n)`; space: `O(n)`.\n*   **Linear-search Dijkstra**: unordered array. Runtime: `O(n²)`; space: `O(n)`.\n*   **Lazy Dijkstra**: binary heap. Runtime: `O(m*log n)`; space: `O(m)`.\n*   **BST Dijkstra**: self-balancing BST. Runtime: `O(m*log n)`; space: `O(n)`.\n*   **Theoretical Dijkstra**: Fibonacci heap. Runtime: `O(m + n*log n)`; space: `O(n)`.\n\nWe provide implementations in Python and C++. The initial sections are mostly background. If you are already familiar with Dijkstra's algorithm, you can skip to the code snippets.\n\n## The shortest-path problem\n\nThe input consists of a graph `G` and a special node `s`. The edges of `G` are directed and have non-negative weights. The edge weights represent the \"lengths\" of the edges. The goal is to find the distance from `s` to every other node in `G`. The distance from `s` to another node is the length of the shortest path from `s` to that node, and the length of a path is the sum of the lengths of its edges. If a node is unreachable from `s`, then we say that the distance is infinite.\n\nMore precisely, this is known as the \"single-source shortest-path\" (SSSP) problem, because we find the distance from one node to every other node. Related problems include the \"all-pairs shortest paths\" problem and the single-source single-destination problem. Dijkstra's algorithm is a really efficient algorithm for the SSSP problem when the edges are non-negative. Dijkstra's algorithm does not work in the presence of negative edges (zero-weight edges are fine). If `G` contains negative edges, we should use the Bellman-Ford algorithm instead.\n\nThe constraint that the edges are directed is not important: if `G` is undirected, we can simply replace every undirected edge `{u,v}` with a pair of directed edges `(u,v)` and `(v,u)` in opposite directions and with the weight of the original edge.\n\nTo simplify things, we make a couple of assumptions that do not make any actual difference:\n\n*   Nodes not reachable by `s` play no role in the algorithm, so we assume that `s` can reach every node. This is so that, in the analysis, we can assume that `n=O(m)`.\n*   We assume that the distance from `s` to every node is unique. This allows us to talk about \"the\" shortest path to a node, when in general there could be many.\n\n## The graph's representation\n\nA graph is a mathematical concept. In the context of graph algorithms, we need to specify how the graph is represented as a data structure. For Dijkstra's algorithm, the most convenient representation is the adjacency list. The valuable thing about the adjacency list representation is that it allows us to iterate through the out-going edges of a node efficiently.\n\nIn the version of the adjacency list that we use, each node is identified with an index from `0` to `n-1`. The adjacency list contains one list for each node. For each node `u` between `0` and `n-1`, the list `G[u]` contains one entry for each neighbor of `u`. In a directed graph, if we have an edge `(u,v)` from `u` to `v`, we say that `v` is a neighbor of `u`, but `u` is not a neighbor of `v`. Since the graph is weighted, the entry for each neighbor `v` consists of a pair of values, `(v, l)`: the destination node `v`, and the length `l` of the edge `(u,v)`.\n\n## Dijkstra's algorithm idea\n\nOne of the data structures that we maintain is a list `dist` where `dist[u]` is the best distance known for `u` so far. At the beginning, `dist[s] = 0`, and for every other node `dist[u] = infinity`. These distances improve during the algorithm as we consider new paths. Our goal is to get to the point where `dist` contains the correct distance for every node.\n\nDuring the algorithm, the `dist` list is only updated through an operation called \"relaxing\" an edge.\n\n```python\ndef relax(u,v,l): #l is the length of the edge (u,v)\n    if dist[u] + l < dist[v]:\n        dist[v] = dist[u] + l\n\n```\n\nIn words, relaxing an edge `(u,v)` means checking if going to `u` first and then using the edge `(u,v)` is shorter than the best distance known for `v`. If it is shorter, then we update `dist[v]` to the new, better value.\n\nDijkstra's algorithm is based on the following observations:\n\n*   if `dist[u]` is correct **and** the shortest path from `s` to `v` ends in the edge `(u,v)`, then if we relax the edge `(u,v)`, we will find the correct distance to `v`. If either of the conditions are not satisfied, relaxing `(u,v)` may improve `dist[v]`, but it will not be the correct distance.\n*   To find the correct distance to `v`, we need to relax all the edges in the shortest path from `s` to `v`, in order. If we do it in order, each node in the path will have the correct distance when we relax the edge to the next node, satisfying the conditions.\n\nDijkstra's algorithm is efficient because every edge is relaxed only once (unlike other algorithms like Bellman-Ford, which relaxes the edges multiple times). To relax every edge only once, we must relax the out-going edges of each node only after we have found the correct distance for that node.\n\nAt the beginning, only `s` has the correct distance, so we relax its edges. This updates the entries in `dist` for its neighbors. The neighbor of `s` that is closest to `s`, say, `x`, has the correct distance at this point. This is because every other path from `s` to `x` starts with a longer edge, and, since the graph does not have negative-weight edges, additional edges can only increase the distance. Next, since `x` has the correct distance, we can relax its out-going edges. After that, the node `y` with the 3rd smallest distance in `dist` (after `s` and `x`) has the correct distance because the node before `y` in the shortest path from `s` to `y` must be either `s` or `x`. It cannot be any other node because simply reaching any node that is not `s` or `x` is already more expensive than the distance we have found for `y`. We continue relaxing the out-going edges of nodes, always taking the next node with the smallest found distance. By generalizing the argument above, when we relax the out-going edges of each node, that node already has the correct distance. We finish after we have gone through all the nodes. At that point, `dist` contains the correct distance for every node.\n\n```\nHigh-level pseudocode of Dijkstra's algorithm\n\ndijkstra(G, s):\n    dist = list of length n initialized with INF everywhere except for a 0 at position s\n    mark every node as unvisited\n    while there are unvisited nodes:\n        u = unvisited node with smallest distance in dist\n        mark u as visited\n        for each edge (u,v):\n            relax((u,v))\n\n```\n\nIn order to implement Dijkstra's algorithm, we need to decide the data structures used to find the unvisited node with the smallest distance at each iteration.\n\n## Priority queues\n\nPriority queues are data structures that are useful in many applications, including Dijkstra's algorithm.\n\nIn a normal queue, we can insert new elements and extract the oldest element. A priority queue is similar, but we can associate a priority with each element. Then, instead of extracting the oldest element, we extract the one with highest priority. Depending on the context, \"highest priority\" can mean the element with the smallest or largest priority value. In this context, we will consider that the highest priority is the element with the smallest priority value.\n\nA priority queue is an _abstract_ data structure. That means that it only specifies which operations it supports, but not how they are implemented. There actually exist many ways to implement a priority queue. To make matters more confusing, different priority queues implementations support different sets of operations. The only agreed part is that they must support two basic operations:\n\n*   `insert(e, k)`: insert element `e` with priority `k`.\n*   `extract_min()`: remove and return the element with the smallest priority value.\n\nFor Dijkstra's algorithm, we can use a priority queue to maintain the nodes, using `dist[u]` as the priority for a node `u`. Then, at each iteration we can extract the unvisited node with the smallest distance. However, there is a problem: when we relax an edge, the value `dist[u]` may decrease. Thus, we need the priority queue to support a third operation which is not commonly supported:\n\n*   `change_priority(e, k)`: set the priority of `e` to `k` (assuming that `e` is in the priority queue).\n\nA related operation is removing elements that are not the most prioritary:\n\n*   `remove(e)`: remove `e` (assuming that `e` is in the priority queue).\n\nIf a priority queue implements remove, we can use it to obtain the same functionality as `change-priority(e, k)`: we can first call `remove(e)` and then reinsert the element with the new key by calling `insert(e, k)`.\n\n## Pseudocode with a priority queue\n\nAssuming that we have a priority queue data structure that supports `insert`, `extract-min`, and `change-priority`, Dijkstra's pseudocode would be as follows.\n\nThe priority queue contains the unvisited nodes, prioritized by distance from `s`. At the beginning, the priority queue contains all the nodes, and they are removed as they are visited.\n\n```\nDijkstra pseudocode (with a priority queue)\n\ndijkstra(G, s):\n    dist = list of length n initialized with INF everywhere except for a 0 at position s\n    PQ = empty priority queue\n    for each node u: PQ.insert(u, dist[u])\n    while not PQ.empty():\n        u = PQ.extract_min()\n        for each edge (u,v) of length l:\n            if dist[u]+l < dist[v]:\n                dist[v] = dist[u]+l\n                PQ.change_priority(v, dist[v])\n\n```\n\nA common variation is to add them to the priority queue when they are reached for the first time, instead of adding all the nodes at the beginning. The only change is how the priority queue is initialized and the if-else cases at the end:\n\n```\nDijkstra pseudocode (with deferred insertions to the PQ)\n\ndijkstra(G, s):\n    dist = list of length n initialized with INF everywhere except for a 0 at position s\n    PQ = empty priority queue\n    PQ.insert(s, 0)\n    while not PQ.empty():\n        u = PQ.extract_min()\n        for each edge (u,v) of length l:\n            if dist[u]+l < dist[v]:\n                dist[v] = dist[u]+l\n                if v in PQ: PQ.change_priority(v, dist[v])\n                else: PQ.insert(v, dist[v])\n\n```\n\nIt does not change the runtime or space complexity, but there is also no downside to deferring insertions to the PQ. On average, the PQ will contains fewer elements.\n\n## Analysis of Dijkstra's algorithm\n\nUsually, we analyze the algorithms _after_ implementing them. However, in order to choose the best data structure for the priority queue, we need to analyze how much we use each type of operation. Thus, it is convenient to define the runtime in terms of the priority queue operations, without specifying yet how they are done. Let `T_ins`, `T_min`, and `T_change` be the time per `insert`, `extract_min`, and `change_priority` operation, respectively, on a priority queue containing `n` elements.\n\nThe main `while` loop has `n` iterations, and the total number of iterations of the inner `for` loop, across all `n` iterations, is `m`. This is because each edge is relaxed once.\n\nThe runtime is dominated by the priority queue operations, so it is `O(n*T_ins + n*T_min + m*T_change)`. These operations dominate the runtime because everything else combined (like updating the `dist` list) takes `O(n+m)` time.\n\n## Linear-search Dijkstra for dense graphs\n\nThe simplest way to simulate the `extract_min` functionality of a priority queue is to iterate through the entire `dist` list to find the smallest value among the non-visited entries. If we do this, we don't need a priority queue. We call this **linear-search Dijkstra**. We get `T_ins = O(1)`, `T_min = O(n)`, and `T_change = O(1)`. Plugging those in, the total runtime of linear-search Dijkstra is `O(n + n*n + m) = O(n²)`, where we simplify out the `m` term because `n² > m` in any graph. More precisely, a directed graph with `n` nodes has at most `n*(n-1)=O(n²)` edges.\n\nA graph with \"close to\" `n*(n-1)` edges is called dense. **Linear-search Dijkstra is actually optimal for dense graphs.** This is because Dijkstra's algorithm must take `O(m)` time just to relax all edges, so it cannot be faster than `O(m)`, and, in dense graphs that is already proportional to `O(n²)`.\n\nHere is a Python implementation:\n\n```python\ndef linearSearchDijkstra(G, s):\n    n = len(G)\n    INF = 9999999\n    dist = [INF for node in range(n)]\n    dist[s] = 0\n    vis = [False for node in range(n)]\n    for i in range(n):\n        u = -1\n        for v in range(n):\n            if not vis[v] and (u == -1 or dist[v] < dist[u]):\n                u = v\n        if dist[u] == INF: break #no more reachable nodes\n        vis[u] = True\n        for v, l in G[u]:\n            if dist[u] + l < dist[v]:\n                dist[v] = dist[u] + l\n    return dist\n\n```\n\nAnd C++. We omit the includes and \"`using namespace std;`\".\n\n```cpp\nvector<int> linearSearchDijkstra(const vector<vector<pair<int,int>>>& G, int s) {\n    int n = G.size();\n    vector<int> dist(n, INT_MAX);\n    dist[s] = 0;\n    vector<int> vis(n, false);\n    for (int i = 0; i < n; i++) {\n        int u = -1;\n        for (int v = 0; v < n; v++)\n            if (not vis[v] and (u == -1 or dist[v] < dist[u]))\n                u = v;\n        if (dist[u] == INT_MAX) break; //no more reachable nodes\n        vis[u] = true;\n        for (auto edge : G[u]) {\n            int v = edge.first, l = edge.second;\n            if (dist[u]+l < dist[v])\n                dist[v] = dist[u]+l;\n        }\n    }\n    return dist;\n}\n\n```\n\n## Priority queues for sparse graphs\n\nThe `O(n²)` time from the implementation above is slow if the graph `G` is sparse, meaning that the number of edges is small relative to `O(n²)`. Recall that the time is `O(n*T_ins + n*T_min + m*T_change)`. If `m` is more similar to `n` than to `n²`, then we would be happy to trade a slower `change_priority` time for a faster `extract_min` time.\n\nThe best possible answer in terms of big-O notation is to use a priority queue implementation based on a data structure known as a **Fibonacci Heap**. A Fibonacci heap containing at most `n` elements achieves the following times:\n\n*   `insert`: `O(log n)` amortized time.\n*   `extract_min`: `O(log n)` amortized time.\n*   `change_priority`: `O(1)` amortized time.\n\nAmortized time means that it could take more time, but, if we average out the times for that operation across the execution of an algorithm, each one takes that time on average.\n\nUsing a Fibonacci heap, we get a total time of `O(n*log n + m)` for Dijkstra's algorithm. This is really fast in terms of big-O notation, but Fibonacci heaps have larger constant factors than other data structures, making them slower in practice.\n\nThe most common way to implement a priority queue is with a **binary heap**. It is simple and fast in practice. Binary heaps support `insert` and `extract_min` in `O(log n)` like a Fibonacci heap. However, they do not support the `change_priority` operation.\n\nIt is possible to modify a binary heap to to support the `change_priority` operation in `O(log n)` time. The result is sometimes called an \"indexed priority queue\". Using an indexed priority queue, we would get a total runtime of `O(n*log n + m*log n) = O(m*log n)`. This is slightly worse than with a Fibonacci heap, and faster in practice.\n\nIn any case, the priority queues provided by languages like C++, Python, and Java, do not support the `change_priority` operation. This creates a disconnect between the pseudocode taught in classrooms and the actual code that we can write.\n\nThe goal of this post is to illustrate the options to deal with this issue. There are 3:\n\n*   **Textbook Dijkstra**: find or implement our own indexed priority queue.\n*   **Lazy Dijkstra**: we implement Dijkstra without using the `change_priority` operation at all.\n*   **BST Dijkstra**: we use a self-balancing binary search tree as the priority queue.\n\nWe will cover the latter two options. The first option is an interesting exercise in data structures (I [implemented](https://github.com/nmamano/StableDistricting/blob/master/src/graphmatching/BinaryHeap.java) it once for a project), but it is more about the inner workings of binary heaps than it is about Dijkstra's algorithm.\n\nAll three options have a runtime of `O(m*log n)`. Note that for dense graphs, this becomes `O(n² log n)` time, so they are all worse than the naive linear-search Dijkstra. In terms of space, lazy Dijkstra is worse than the others, as it needs `O(m)` space, as opposed to `O(n)` for the other options.\n\n## Lazy Dijkstra\n\nWe implement Dijkstra using a priority queue that does not support the change-priority operation. We need the following change: when we find a shorter distance to a node that is already in the priority-queue, instead of using the \"change-priority\" operation, we simply use an \"insert\" operation and add a copy of the node in the priority queue with the new distance. Then, when we extract a node from the priority queue, we ignore it if it is not the first time we extract that node. We call this version of Dijkstra \"lazy Dijkstra\" because we \"postpone\" the removal of the pre-existing copy of the node.\n\nHere is a Python version. The logical structure of a binary heap is a binary tree, but, internally [the tree is represented as an array](https://en.wikipedia.org/wiki/Binary_heap#Heap_implementation) for efficiency reasons. Python is a bit whack because, instead of having a priority queue module that encapsulates the implementation, we have the [heapq](https://docs.python.org/3/library/heapq.html) module, which provides priority queue operations that can be used directly on a list representing a binary heap. `heapq` offers functions `heappop` (equivalent to `extract_min`) and `heappush` (equivalent to `insert`). These functions receive a normal Python list as a parameter, and this list is assumed to represent a binary heap. In Python, if the priority queue contains tuples, then the first element in the tuple is the priority. Thus, in the implementation we insert tuples to the priority queue with the distance first and the node second.\n\n```python\ndef lazyDijkstra(G, s):\n    n = len(G)\n    INF = 9999999\n    dist = [INF for u in range(n)]\n    dist[s] = 0\n    vis = [False for u in range(n)]\n    PQ = [(0, s)]\n    while len(PQ) > 0:\n        _, u = heappop(PQ) #only need the node, not the distance\n        if vis[u]: continue #not first extraction\n        vis[u] = True\n        for v, l in G[u]:\n            if dist[u]+l < dist[v]:\n                dist[v] = dist[u]+l\n                heappush(PQ, (dist[u]+l, v))\n    return dist\n\n```\n\nHere is a C++ version:\n\n```cpp\nvector<int> lazyDijkstra(const vector<vector<pair<int,int>>>& G, int s) {\n    int n = G.size();\n    vector<int> dist(n, INT_MAX);\n    dist[s] = 0;\n    vector<int> vis(n, false);\n    //PQ of (distance, node) pairs prioritized by smallest distance\n    priority_queue<pair<int, int>, vector<pair<int, int>>, greater<pair<int, int>>> PQ;\n    PQ.push({0, s});\n    while (not PQ.empty()) {\n        int u = PQ.top().second;\n        PQ.pop();\n        if (vis[u]) continue; //not first extraction\n        vis[u] = true;\n        for (auto edge : G[u]) {\n            int v = edge.first, l = edge.second;\n            if (dist[u]+l < dist[v]) {\n                dist[v] = dist[u]+l;\n                PQ.push({dist[v], v});\n            }\n        }\n    }\n    return dist;\n}\n\n```\n\nAnalysis: since nodes can be added to the priority queue multiple times, in lazy Dijkstra the maximum number of elements in the priority queue increases from `O(n)` to `O(m)`. As a result, we do `O(m)` `extract_min` and `insert` operations. The total runtime is `O(m*log m)`. This can be simplified to `O(m*log n)`, because `log m < log (n²) = 2 log n = O(log n)`. Thus, in terms of big-O notation, **lazy Dijkstra is equally fast as textbook Dijkstra** (Dijkstra with an indexed priority queue). The only thing that got worse is the space used by the priority queue.\n\n## BST Dijkstra\n\nSelf-balancing binary search trees, like red-black trees or AVL trees, are a type of data structure that maintains a set of elements ordered according to values associated with the elements, known as the elements' keys. They support a few operations, all in `O(log n)` time. For our use case, we are interested in the following ones:\n\n*   Insert an element with a given key.\n*   Find the element with the smallest/largest key.\n*   Given a key, find if there is an element with that key, and optionally remove it.\n\nThese operations allow us to use a self-balancing BST to implement a priority queue. With the third operation, we can even implement the `change_priority` operation, as we mentioned.\n\nPython does not actually have a self-balancing binary search tree module (why?!), so we cannot implement this version of Dijkstra either without finding or implementing our own self-balancing BST.\n\nHere is a C++ version. In C++, the set data structure is implemented as a self-balancing BST:\n\n```cpp\nvector<int> bstDijkstra(const vector<vector<pair<int,int>>>& G, int s) {\n    int n = G.size();\n    vector<int> dist (n, INT_MAX);\n    dist[s] = 0;\n    //self-balancing BST of (distance, node) pairs, sorted by smallest distance\n    set<pair<int, int>> PQ;\n    PQ.insert({0, s});\n    while (not PQ.empty()) {\n        int u = PQ.begin()->second; //extract-min\n        PQ.erase(PQ.begin());\n        for (auto edge : G[u]) {\n            int v = edge.first, l = edge.second;\n            if (dist[u]+l < dist[v]) {\n                //erase and insert instead of change-priority\n                PQ.erase({dist[v], v});\n                dist[v] = dist[u]+l;\n                PQ.insert({dist[v], v});\n            }\n        }\n    }\n    return dist;\n}\n\n```\n\nAnalysis: in a sense, BST Dijkstra combines the best of both worlds: it has the same runtime and space complexity as textbook Dijkstra, without needing the extra space of Lazy Dijkstra, but it uses a much more ubiquitous data structure, a self-balancing BST. However, in practice, self-balancing BSTs are slower than binary heaps. This has to do with the fact that heaps can be implemented on top of an array, while BSTs use recursive tree data structures with child pointers. The array has much better [locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference). For sparse graphs, I'd expect the performance of the different versions to be ordered as follows:\n\nTextbook Dijkstra > Lazy Dijkstra > BST Dijkstra > Theoretical Dijkstra > Linear-search Dijkstra\n\n## Practice problems\n\nHere are some problems on leetcode:\n\n*   [Network Delay Time](https://leetcode.com/problems/network-delay-time/)\n*   [Find the City With the Smallest Number of Neighbors at a Threshold Distance](https://leetcode.com/problems/find-the-city-with-the-smallest-number-of-neighbors-at-a-threshold-distance/)\n*   [Reachable Nodes In Subdivided Graph](https://leetcode.com/problems/reachable-nodes-in-subdivided-graph/)\n*   [Path with Maximum Minimum Value](https://leetcode.com/problems/path-with-maximum-minimum-value/) (Premium only)",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/implementing-dijkstra?category=dsa",
      "author": "Nil Mamano",
      "user_id": ""
    },
    {
      "title": "Reachability Problems and DFS",
      "content": "An extensive list of questions that can be solved with DFS.\n\n# Reachability Problems and DFS\n\nJune 21, 2020\n\n![Reachability Problems and DFS](/blog/reachability-problems-and-dfs/cover.png)\n\n## Introduction\n\nDepth-first search, or DFS, is a fundamental graph algorithm that can be used to solve **reachability** problems. This post shows how to adapt the basic DFS template to solve several problems of this kind. Reachability problems are often easier in undirected graphs. Below, we specify if the algorithm works for undirected graphs, directed graphs, or both.\n\n### Prerequisites\n\nWe assume that the reader is already familiar with the concept of DFS. [Here](https://www.youtube.com/watch?v=7fujbpJ0LB4) is an excellent video introducing DFS with step-by-step animations. We also assume that the reader is familiar with the adjacency list representation of a graph, and we use big-O notation in the analysis.\n\n### Coding conventions\n\nThe algorithms below are in Python. `n` denotes the number of nodes. Nodes are identified with integers in the range `0..n-1`. The graph `G` is a graph stored as an adjacency list: `G` is a list of `n` lists. For each `v` between `0` and `n-1`, `G[v]` is the list of neighbors of `G`.\n\nIf the graph is given as an edge list instead, we can initialize it as follows:\n\n```python\ndef makeAdjList(edgeList):\n    n = max(max(edge) for edge in edgeList) + 1\n    G = [[] for v in range(n)]\n    for u,v in edgeList:\n        G[u].append(v)\n        G[v].append(u) #omit this for directed graphs\n    return G\n\n```\n\nIf the graph is given as an adjacency matrix, we can iterate through the rows of the adjacency matrix instead of through the adjacency lists. To iterate through the neighbors of a node `v`, instead of\n\n```python\n    for u in G[v]:\n        #u is a neighbor of v\n        ...\n\n```\n\nwe do\n\n```python\n    for u in range(n):\n        if adjMatrix[v][u]:\n            #u is a neighbor of v\n            ...\n\n```\n\nNote that using an adjacency matrix affects the runtime analysis of DFS: `O(n²)` instead of `O(m)`.\n\n## Which nodes can be reached from node s?\n\nThis is the simplest question that can be answered with DFS. The primary data structure in DFS is a list of booleans to keep track of already visited nodes (we call it `vis`). If we start a DFS search from a node `s`, the reachable nodes will be the ones for which `vis` is true.\n\nFor this, `G` can be directed or undirected. We make use of a nested function in Python so that we do not need to pass `G` and `vis` as parameters (in Python nested functions have visibility over the variables in the scope where they are defined).\n\n```python\ndef reachableNodes(G, s): #G is directed or undirected\n    n = len(G)\n    vis = n * [False]\n    vis[s] = True\n\n    #invariant: v is marked as visited when calling visit(v)\n    def visit(v):\n        for nbr in G[v]:\n            if not vis[nbr]:\n                vis[nbr] = True\n                visit(nbr)\n\n    visit(s)\n    return [v for v in range(n) if vis[v]]\n\n```\n\nDFS runs in `O(m)` time and `O(n)` space, where `m` is the number of edges. This is because each edge is considered twice, once from each endpoint, if the endpoints end up being visited, or zero times if the endpoints are not visited.\n\n### Iterative version\n\n```python\ndef reachableNodes(G, s): #G is directed or undirected\n    n = len(G)\n    vis = n * [False]\n    stk = [s]\n    #mark nodes as visited when removed from the stack, not when added\n    while stk:\n        v = stk.pop()\n        if vis[v]: continue\n        vis[v] = True\n        for nbr in G[v]:\n            if not vis[nbr]:\n                stk.append(nbr)\n    return [v for v in range(n) if vis[v]]\n\n```\n\nThe iterative version takes `O(m)` space instead of `O(n)` because nodes can be inserted into the stack multiple times (up to one time for each incident edge). Alternatively, we can mark the nodes as visited when we add them to the stack instead of when we remove them. This change reduces the space usage to the usual `O(n)`. However, with this change, the algorithm is no longer DFS. It still works for answering reachability questions because the set visited nodes is the same, but the order in which they are visited is no longer consistent with a depth-first search order (it is closer to a BFS (breath-first search) order, but also not exactly a BFS order).\n\nThe difference between marking nodes when they added vs removed from the stack is discussed in detail [here](https://11011110.github.io/blog/2013/12/17/stack-based-graph-traversal.html). Since the recursive version is shorter and optimal in terms of space, we favor it from now on. That said, it should be easy to adapt the iterative version above to the problems below.\n\n## Can node s reach node t?\n\nWe use the same code from before, but we add early termination as soon as we see `t`. Now, the recursive function has a return value.\n\n```python\ndef canReachNode(G, s, t): #G is directed or undirected\n    n = len(G)\n    vis = n * [False]\n    vis[s] = True\n\n    #returns True if the search reaches t\n    def visit(v):\n        if v == t: return True\n        for nbr in G[v]:\n            if not vis[nbr]:\n                vis[nbr] = True\n                if visit(nbr): return True\n        return False\n\n    return visit(s)\n\n```\n\nAdding the early termination can make the DFS faster, but in the worst-case the time/space complexity is the same.\n\n### Practice problems\n\n*   [https://leetcode.com/problems/the-maze/](https://leetcode.com/problems/the-maze/)\n\nThe hardest part on this problem is constructing the graph in the first place.\n\n## Find a path from s to t\n\nThe edges \"traversed\" in a DFS search form a tree called the \"DFS tree\". The DFS tree changes depending on where we start the search. The starting node is called the root. We can construct the DFS tree by keeping track of the predecessor of each node in the search (the root has no predecessor). If we construct the DFS tree rooted at `s`, we can follow the sequence of predecessors from `t` to `s` to find a path from `s` to `t` in reverse order.\n\nInstead of using the list `vis` to keep track of visited nodes, we know a node is unvisited if it has no predecessor yet. We indicate that a node has no predecessor with the special value `-1`.\n\n```python\ndef findPath(G, s, t): #G is directed or undirected\n    n = len(G)\n    pred = n * [-1]\n    pred[s] = None\n    def visit(v):\n        for nbr in G[v]:\n            if pred[nbr] == -1:\n                pred[nbr] = v\n                visit(nbr)\n    visit(s) #builds DFS tree\n    path = [t]\n    while path[-1] != s:\n        p = pred[path[-1]]\n        if p == -1: return None #cannot reach t from s\n        path.append(p)\n    path.reverse()\n    return path\n\n```\n\nNote that DFS does _not_ find the shortest path form `s` to `t`. For that, we can use BFS (breath-first search). It just returns any path without repeated nodes.\n\n## Is the graph connected?\n\nFor undirected graphs, this is almost the same question as the first question (\"which nodes can be reached by `s`?\") because of the following property:\n\n_An undirected graph is connected if and only if every node can be reached from `s`, where `s` is any of the nodes._\n\nThus, the code is exactly the same as for the first question, with two differences: 1) we choose `s` to be `0` (could be anything), and 2) we change the last line to check if every entry in `vis` is true.\n\n```python\ndef isConnected(G): #G is undirected\n    n = len(G)\n    vis = n * [False]\n    vis[0] = True\n    def visit(v):\n        for nbr in G[v]:\n            if not vis[nbr]:\n                vis[nbr] = True\n                visit(nbr)\n    visit(0)\n    return all(vis)\n\n```\n\nFor directed graphs, we need to take into account the direction of the edges. A directed graph is **strongly connected** if every node can reach every other node. We can use the following property:\n\n_A directed graph is strongly connected if and only if `s` can reach every node and every node can reach `s`, where `s` is any of the nodes._\n\nWe already know how to use DFS to check if `s` can reach every node. To check if every node can reach `s`, we can do a DFS starting from `s`, **but in the reverse graph of G**. The reverse graph of `G` is like `G` but reversing the directions of all the edges.\n\n```python\ndef isConnected(G): #G is directed\n    n = len(G)\n    vis = n * [False]\n    vis[0] = True #use 0 for start node\n    def visit(G, v):\n        for nbr in G[v]:\n            if not vis[nbr]:\n                vis[nbr] = True\n                visit(G, nbr)\n    visit(G, 0) #nodes reachable from s\n    if not all(vis): return False\n    Greverse = [[] for v in range(n)]\n    for v in range(n):\n        for nbr in G[v]:\n            Greverse[nbr].append(v)\n    vis = n * [False] #reset vis for the second search\n    vis[0] = True\n    visit(Greverse, 0) #nodes that can reach s\n    return all(vis)\n\n```\n\nThe runtime is still `O(m)`, but the space is now `O(m)` because we need to create and store the reverse graph. There are alternative algorithms (like Tarjan's algorithm) which can do this in `O(n)` space.\n\n## How many connected components are there?\n\nWe can use the typical DFS to answer this question for undirected graphs. We use a common pattern in DFS algorithms: an outer loop through all the nodes where we launch a search for every yet-unvisited node.\n\n```python\ndef numConnectedComponents(G): #G is undirected\n    n = len(G)\n    vis = n * [False]\n    def visit(v):\n        for nbr in G[v]:\n            if not vis[nbr]:\n                vis[nbr] = True\n                visit(nbr)\n    numCCs = 0\n    for v in range(n):\n        if not vis[v]:\n            numCCs += 1\n            vis[v] = True\n            visit(v)\n    return numCCs\n\n```\n\nThe runtime is now `O(n+m)` because, if `m < n`, we still spend `O(n)` time iterating through the loop at the end.\n\nFor directed graphs, instead of connected components, we talk about **strongly connected components**. A strongly connected component is a maximal subset of nodes where every node can reach every other node.\n\nIf we want to find the number of strongly connected components, we can use something like [Tarjan's algorithm](https://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm), a DFS-based algorithm that requires some additional data structures.\n\n### Practice problems\n\n*   [https://leetcode.com/problems/number-of-connected-components-in-an-undirected-graph/](https://leetcode.com/problems/number-of-connected-components-in-an-undirected-graph/) (Premium only)\n*   [https://leetcode.com/problems/number-of-islands/](https://leetcode.com/problems/number-of-islands/)\n*   [https://leetcode.com/problems/friend-circles/](https://leetcode.com/problems/friend-circles/)\n\n## Which nodes are in the same connected components?\n\nThis question is more general than the previous two. We label each node `v` with a number `CC[v]` so that nodes with the same number belong to the same CC. Instead of having a list `CC` in addition to `vis`, we use the CC number `-1` to indicate unvisited nodes. This way, we do not need `vis`\n\n```python\ndef connectedComponents(G): #G is undirected\n    n = len(G)\n    CC = n * [-1]\n\n    ##invariant: v is labeled with CC i>=0\n    def visit(v, i):\n        for nbr in G[v]:\n            if CC[nbr] == -1:\n                CC[nbr] = i\n                visit(nbr, i)\n\n    i = 0\n    for v in range(n):\n        if CC[v] == -1:\n            CC[v] = i\n            visit(v, i)\n            i += 1\n    return CC\n\n```\n\nFor directed graphs, again we need Tarjan's algorithm or an equivalent algorithm.\n\n### Practice problems\n\n*   [https://leetcode.com/problems/max-area-of-island/](https://leetcode.com/problems/max-area-of-island/)\n*   [https://leetcode.com/problems/sentence-similarity-ii/](https://leetcode.com/problems/sentence-similarity-ii/)\n\nIn the second problem, nodes are given by names, not indices, so they need to be converted.\n\n## Is the graph acyclic?\n\nFor undirected graphs, this question is simple. First, we consider the problem in each CC independently. This is very common pattern in graph problems. We do this with an outer loop through all the nodes where we launch a search for every yet-unvisited node.\n\nDuring the DFS search in each CC, if we find an edge to an already visited node that is not the predecessor in the search (the node we just came from), there is a cycle. Such edges in a DFS search are called **back edges**. We add one parameter to the recursive function `visit` to know the predecessor node.\n\n```python\ndef hasCycles(G): #G is undirected\n    n = len(G)\n    vis = n * [False]\n\n    #returns True if the search finds a back edge\n    def visit(v, p):\n        for nbr in G[v]:\n            if vis[nbr] and nbr != p: return True\n            if not vis[nbr]:\n                vis[nbr] = True\n                if visit(nbr, v): return True\n        return False\n\n    for v in range(n):\n        if not vis[v]:\n            vis[v] = True\n            #the root of the search has no predecessor\n            if visit(v, -1): return True\n    return False\n\n```\n\nFor directed graphs, it is not as simple: the fact that a neighbor `nbr` is already visited during the DFS search does not mean that `nbr` can reach the current node. To check if a directed graph is acyclic, we can use the linear-time [peel-off algorithm](https://en.wikipedia.org/wiki/Topological_sorting#Kahn's_algorithm) for finding a topological ordering. This algorithm detects if the graph is acyclic and finds a topological ordering if so, though we are only interested in the first part.\n\n### Practice problems\n\n*   [https://leetcode.com/problems/redundant-connection/](https://leetcode.com/problems/redundant-connection/)\n\nThis problem is easier to solve using union-find, but it can be done with DFS.\n\n## Is the graph a tree?\n\nUsually, we ask this question for undirected graphs. We can use this characterization of trees:\n\n_An undirected graph is a tree if and only if it is connected and has exactly `n-1` edges._\n\nWe already saw how to check if the graph is connected with DFS, and counting the number of edges is straightforward:\n\n```python\n    #for undirected graphs:\n    m = sum(len(G[v]) for v in range(n)) / 2\n    #for directed graphs:\n    m = sum(len(G[v]) for v in range(n))\n\n```\n\n### Practice problems\n\n*   [https://leetcode.com/problems/graph-valid-tree/](https://leetcode.com/problems/graph-valid-tree/)\n\n## Is the graph bipartite?\n\nThis is exactly the same question as whether the graph can be two-colored, so see the next section.\n\n## Can the graph be two-colored?\n\nTwo-coloring a graph means assigning colors to the nodes such that no two adjacent nodes have the same color, using only two colors. Usually, we consider coloring question for undirected graphs.\n\nWe consider whether each CC can be colored independently from the others. We can color each CC using DFS. We use values `0` and `1` for the colors. The color of the start node can be anything, so we set it to `0`. For the remaining nodes, the color has to be different from the parent, so we only have one option.\n\nInstead of having a `vis` array, we use the special color `-1` to denote unvisited nodes.\n\n```python\ndef is2Colorable(G): #G is undirected\n    n = len(G)\n    color = n * [-1]\n\n    #returns True if we can color all the nodes reached from v\n    #invariant: v has an assigned color\n    def visit(v):\n        for nbr in G[v]:\n            if color[nbr] == color[v]: return False\n            if color[nbr] == -1:\n                color[nbr] = 1 if color[v] == 0 else 0\n                if not visit(nbr): return False\n        return True\n\n    for v in range(n):\n        if color[v] == -1:\n            color[v] = 0\n            if not visit(v): return False\n    return True\n\n```\n\nWith 3 or more colors, the problem becomes [a lot harder](https://en.wikipedia.org/wiki/Graph_coloring#Algorithms).\n\n### Practice problems\n\n*   [https://leetcode.com/problems/is-graph-bipartite/](https://leetcode.com/problems/is-graph-bipartite/)\n\n## What is the distance from a node s to every other node in a tree?\n\nWe cannot use DFS to find the distance between nodes in a graph which can have cycles, because DFS is not guaranteed to follow the shortest path from the root to the other nodes. For that, BFS is more suitable (if the graph is unweighted). However, since trees are acyclic, there is a unique path between any two nodes, so DFS must use the unique path, which, by necessity, is the shortest path. Thus, we can use DFS to find distances in a tree.\n\n```python\ndef getDistances(G, s): #G is undirected and a tree\n    n = len(G)\n    dists = n * [-1]\n    dists[s] = 0\n\n    #invariant: v has an assigned distance\n    def visit(v):\n        for nbr in G[v]:\n            #check nbr is not the predecessor\n            if dists[nbr] != -1: continue\n            dists[nbr] = dists[v] + 1\n            visit(nbr)\n    visit(s)\n    return dists\n\n```\n\n### Practice problems\n\n*   [https://leetcode.com/problems/time-needed-to-inform-all-employees/](https://leetcode.com/problems/time-needed-to-inform-all-employees/)\n\n## Find a spanning tree\n\nA spanning tree of a connected, undirected graph `G` is a subgraph which has the same nodes as `G` that is a tree. The edges traversed by a DFS search on a connected graph form a spanning tree (sometimes called a DFS tree). Thus, we do DFS and add the traversed edges to the resulting tree.\n\n```python\ndef spanningTree(G): #G is undirected and connected\n    n = len(G)\n    vis = n * [False]\n    vis[0] = True\n    T = [[] for v in range(n)]\n\n    def visit(v):\n        for nbr in G[v]:\n            if not vis[nbr]:\n                vis[nbr] = True\n                T[v].append(nbr)\n                T[nbr].append(v)\n                visit(nbr)\n    visit(0)\n    return T\n\n```\n\n## Conclusions\n\nDFS has many uses. We showed how to make minor modifications to the DFS template to answer reachability and connectivity questions.\n\nAfter DFS, the next algorithm to learn would be BFS (breath-first search). Like DFS, it can answer reachability questions. On top of that, it can also answer questions about distance in undirected graphs.",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/reachability-problems-and-dfs?category=dsa",
      "author": "Nil Mamano",
      "user_id": ""
    },
    {
      "title": "Breaking Down Dynamic Programming",
      "content": "A step-by-step guide to understanding and implementing dynamic programming algorithms with practical examples.\n\n# Breaking Down Dynamic Programming\n\nFebruary 5, 2020\n\n![Breaking Down Dynamic Programming](/blog/breaking-down-dynamic-programming/cover.png)\n\nNote: the approach in this guide later became the foundation for the dynamic programming chapter in [Beyond Cracking the Coding Interview](https://www.amazon.com/dp/195570600X).\n\n## Introduction\n\nWhen I was a TA for \"Algorithm Design and Analysis\", the students struggled with dynamic programming. To simplify/demystify it, I tried to break it down into a logical sequence of steps, each of which should not feel too intimidating on its own. This is explained in detail here. To complement the explanations, there are links to problems on [leetcode.com](https://leetcode.com/), in case the reader wants to practice. The code snippets are in Python, but Leetcode accepts most popular languages.\n\n## Overview: Recursive vs Iterative DP\n\nIn short, dynamic programming (DP) is a technique for problems that seem hard to solve as a whole, but become easy if we know the solution to smaller subproblems. More technically, we can use it in problems where the (value of the) solution can be expressed as an equation which is a function of the input, and is expressed in terms of itself with smaller inputs. This is called a _recurrence equation_. The classic example is the Fibonacci recurrence: `Fib(n) = Fib(n-1) + Fib(n-2)`.\n\n*   [https://leetcode.com/problems/fibonacci-number/](https://leetcode.com/problems/fibonacci-number/)\n\nA recurrence equation can be translated into code:\n\n```python\ndef Fib(n):\n  if n == 0 or n == 1:\n    return 1\n  return Fib(n-1) + Fib(n-2)\n\n```\n\nHowever, the above function has an exponential runtime. A recursive function becomes exponential when it is possible to reach the same subcall through different execution paths. In the Fibonacci case, we have the following nested calls: `Fib(n) -> Fib(n-1) -> Fib(n-2)`, and `Fib(n) -> Fib(n-2)`. Since `Fib(n-2)` is called twice all the work from this call is duplicated, which in turn means that subcalls made from `Fib(n-2)` will start to duplicate and grow exponentially.\n\nDynamic programming is simply a workaround to this duplication issue. Instead of recomputing the solutions of the subproblems, we store them and then we recall them as needed. This guarantees that each subproblem is computed only once.\n\nThere are two main approaches for DP.\n\n### Recursive / Top-down DP\n\nWe start with the code which is a literal translation of the recurrence equation, but then we add a dictionary / hash table to store results.\n\n```python\nmemo = {}\n\ndef Fib(n):\n  if n == 0 or n == 1:\n    return 1\n  if n in memo:\n    return memo[n]\n  res = Fib(n-1) + Fib(n-2)\n  memo[n] = res\n  return res\n\n```\n\nThere are three changes in the code above:\n\n1.  declaring our dictionary for storing results, `memo` outside the recursive function (memo comes \"memorization\" or \"memoization\", a name used in the literature).\n2.  before computing the result, we check if the solution has already been computed. This check can be done before or after the base case.\n3.  before returning, we save the result in the `memo` table.\n\nUsing a memoization table in this way solves the inefficiency (we will go deeper into the analysis part later).\n\n### Iterative / Bottom-up DP\n\nInstead of starting from the largest input and recursively reaching smaller subproblems, we can directly compute the subproblems from smallest to largest. This way, we already have the solutions to the subproblems when we need them. For this approach, we change the dictionary for an array/vector, and we change recursive calls for a for loop.\n\n```python\ndef Fib(n):\n  if n == 0: return 1\n  memo = [0 for i in range(n+1)]\n  memo[0], memo[1] = 1, 1\n  for i in range(2, n+1):\n    memo[i] = memo[i-1] + memo[i-2]\n  return memo[n]\n\n```\n\nMost problems can be solved with both recursive and iterative DP. Here are some considerations for how to choose:\n\n*   Recursive DP matches the recurrence equation more directly, so it can be easier to implement.\n*   Both have the same runtime complexity, but the recursive version will generally have larger constant factors due to all the recursive function calling and due to using a hash table instead of an array.\n*   Iterative DP often allows for an optimization to reduce the space complexity (discussed later).\n\n## Recursive DP in 5 Steps\n\n1.  Choose what your subproblems are.\n2.  Find the recurrence equation.\n3.  Translate the recurrence equation into recursive code.\n4.  Add memoization.\n5.  (Optional) Reconstruct the solution.\n\nWe already saw steps 1–4 with the Fibonacci example. Now, we will walk through all the steps in more detail using a more complicated example, the _longest common subsequence problem_:\n\nGiven two strings `s1` and `s2`, find the length of the longest string which is a subsequence of both `s1` and `s2`. A string `t` is a _subsequence_ of a string `s` if every char in `t` appears **in order** in `s`, but are **not necessarily contiguous**. For example, `abc` is a subsequence of `axbyz`, but `ba` is not (do not confuse subsequence with substring or subset).\n\n*   [https://leetcode.com/problems/longest-common-subsequence/](https://leetcode.com/problems/longest-common-subsequence/)\n\nStep 1: choose our subproblems. This varies from problem to problem, but when the input to the problem is a string, a natural way to obtain smaller problems is to look at shorter strings. Here we can use as a subproblem a _prefix_ of `s1` and a prefix of `s2`.\n\nSome notation: let `n` be the length of `s1` and `m` the length of `s2`. Let `LCS(i,j)` be the solution for the LCS problem for the prefix of `s1` of length `n` (`s1[0..i-1]`) and the prefix of `s2` of length `m` (`s2[0..j-1]`). Then, our goal is to find `LCS(n, m)`.\n\nStep 2: find the recurrence equation. Now we need to come up with an expression for `LCS(i,j)` as a function of `LCS` with smaller indices (as well as a base case). This is the hardest step of DP, and often it is here that we realize that we chose bad subproblems in Step 1. If that happens, hopefully we will discover some hint for what our subproblems should be.\n\nIn order to derive the recurrence equation for LCS, we need the following observation: if the two strings end with the same character `c`, then, to maximize the length of the subsequence, it is \"safe\" to add `c` to the subsequence. In contrast, if both strings end with different characters, then _at least_ one of them cannot appear in the subsequence. The complication is that we do not know which one. Thus, instead of guessing, we can simply consider both options.\n\nThis observation yields the recurrence equation (excluding base case):\n\n```python\nLCS(i, j) = 1 + LCS(i-1, j-1)                 if s[i] == s[j]\n            max(LCS(i, j-1), LCS(i-1, j))     otherwise\n\n```\n\nThis step is not intuitive at first, and requires practice. After having done a few problems, one starts to recognize the typical patterns in DP. For instance, using `max` among a set of options of which we do not know which one is the best is easily the most common pattern in DP.\n\nStep 3. Translate the recurrence equation into recursive code. This step is a very simple programming task. Pay attention to the base case.\n\n```python\n#outer call:\nLCS(len(s1), len(s2))\n\ndef LCS(i, j):\n  if i == 0 or j == 0:\n    return 0\n  if s1[i-1] == s2[j-1]:\n    return 1 + LCS(i-1, j-1)\n  else:\n    return max(LCS(i, j-1), LCS(i-1, j))\n\n```\n\nIf we draw the few first steps of the call graph, we will see that the same subproblem is reached twice. Thus, call graph blows up, leading to an exponential runtime.\n\nStep 4. Add memo table. This step should be automatic: one does not even need to understand the previous code in order to add the memo table.\n\n```python\n#outer call:\nmemo = {}\nLCS(len(s1), len(s2))\n\ndef LCS(i, j):\n  if i == 0 or j == 0:\n    return 0\n  if (i,j) in memo:\n    return memo[(i,j)]\n  if s1[i-1] == s2[j-1]:\n    res = 1 + LCS(i-1, j-1)\n  else:\n    res = max(LCS(i, j-1), LCS(i-1, j))\n  memo[(i,j)] = res\n  return res\n\n```\n\nThe base case corresponds to when one of the strings is empty. The LCS of an empty string with another string is clearly an empty string.\n\nIncidentally, if we flip the check on the memo table, the code becomes a bit more streamlined (fewer lines + merging the two returns). I prefer this form (it does the same):\n\n```python\ndef LCS(i, j):\n  if i == 0 or j == 0:\n    return 0\n  if (i,j) not in memo:\n    if s1[i-1] == s2[j-1]:\n      memo[(i,j)] = 1 + LCS(i-1, j-1)\n    else:\n      memo[(i,j)] = max(LCS(i, j-1), LCS(i-1, j))\n  return memo[(i,j)]\n\n```\n\nWe have eliminated the exponential blowup. In general, DP algorithms can be analyzed as follows: # of distinct subproblems times time per subproblem excluding recursive calls. For LCS, we get `O(nm)*O(1)=O(nm)`.\n\nStep 5. Reconstruct the solution.\n\nWe used DP to compute the length of the LCS. What if we want to find the LCS itself? A naive way to do it would be to store the entire result of each subproblem in the memoization table instead of just its length. While this works, it is clear that it will require a lot of memory to store `O(nm)` strings of length `O(min(n,m))` each. We can do better.\n\nStep 5, \"Reconstruct the solution\", is how to reuse the table that we constructed in Step 4 to find the actual solution instead of just its length. I said that this step is optional because sometimes we just need the _value_ of the solution, so there is no reconstruction needed.\n\nThe good news is that we do not need to modify the code that we already wrote in Step 4. The reconstruction is a separate step that comes after. In addition, the reconstruction step is very similar (follows the same set of cases) as the step of building the memo table. In short, we use the memo table as an \"oracle\" to guide us in our choices. Based on the values in the memo table, we know which option is better, so we know how to reconstruct the solution.\n\n```python\n#outer calls\nmemo = {}\nn, m = len(s1), len(s2)\nLCS(n, m) #build memo table\nsol = reconstructLCS(n, m)\n\ndef reconstructLCS(i, j):\n  if i == 0 or j == 0:\n    return \"\"\n  if s1[i-1] == s2[j-1]:\n    return reconstructLCS(i-1, j-1) + s1[i-1]\n  elif memo[(i-1,j)] >= memo[(i,j-1)]:\n    return reconstructLCS(i-1, j)\n  else:\n    return reconstructLCS(i, j-1)\n\n```\n\nIn the code above, first we run `LCS(n,m)` to fill the memo table. Then, we use it in the reconstruction. The condition `memo[(i-1,j)] >= memo[(i,j-1)]` tells us that we can obtain a longer or equal LCS by discarding a char from `s1` instead of from `s2`.\n\nNote that there is a single recursive call in the reconstruction function, so the complexity is just `O(n+m)`.\n\n## Iterative DP in 6 Steps\n\n1.  Choose what your subproblems are.\n2.  Find the recurrence equation.\n3.  **Design the memo table.**\n4.  **Fill the memo table.**\n5.  (Optional) Reconstruct the solution.\n6.  **(Optional) Space optimization.**\n\nThe new/different steps are highlighted. Step 3. is to design the layout of the table/matrix where we are going to store the subproblem solutions. There is no coding in this step. By \"design\", I mean making the following choices:\n\n*   what are the dimensions of the table, and what does each index mean. Generally speaking, the table should have one dimension for each parameter of the recurrence equation. In the case of LCS, it will be a 2-dimensional table.\n*   where are the base cases.\n*   where is the cell with the final solution.\n*   what is the \\`\\`dependence relationship'' between cells (which cells do you need in order to compute each cell).\n*   which cells do not need to be filled (in the case of LCS, we need them all).\n\nHere is how I would lay out the table for LCS (you can find a different layout in the problems below):\n\n![LCS table](/blog/breaking-down-dynamic-programming/lcstable.svg)\n\nNext (Step 4), we fill the memo table with a nested for loop. If the layout is good, this should be easy. Before the main loop, we fill the base case entries. Then, we must make sure to iterate through the table in an order that respects the dependencies between cells. In the case of LCS, we can iterate both by rows or by columns.\n\nWe obtain the following algorithm:\n\n```python\ndef LCS(s1, s2):\n  n, m = len(s1), len(s2)\n  memo = [[0 for j in range(m+1)] for i in range(n+1)]\n  for i in range(1, n+1):\n    for j in range(1, m+1):\n      if s1[i-1] == s2[j-1]:\n        memo[i][j] = 1 + memo[i-1][j-1]\n      else:\n        memo[i][j] = max(memo[i-1][j], memo[i][j-1])\n  return memo[n][m]\n\n```\n\nIn the code above, the base case entries are filled implicitly when we initialize the table with zeros everywhere.\n\nIf we need to reconstruct the solution, we can do it in the same way as for the recursive DP. The only difference is that memo is a matrix instead of dictionary.\n\n### Space optimization\n\nClearly, the space complexity of iterative DP is the size of the DP table. Often, we can do better. The idea is to only store the already-computed table entries that we will use to compute future entries. For instance, in the case of Fibonacci, we do not need to create an entire array -- keeping the last two numbers suffice. In the case of a 2-dimensional DP table, if we are filling the DP table by rows and each cell only depends on the previous row, we only need to keep the last row (and similarly if we iterated by columns). Here is the final version for LCS where we improve the space complexity from `O(nm)` to `O(n+m)`:\n\n```python\ndef LCS(s1, s2):\n  n, m = len(s1), len(s2)\n  lastRow = [0 for j in range(m+1)]\n  for i in range(1,n+1):\n    curRow = [0 for j in range(m+1)]\n    for j in range(1,m+1):\n      if s1[i-1] == s2[j-1]:\n        curRow[j] = 1 + lastRow[j-1]\n      else:\n        curRow[j] = max(lastRow[j], curRow[j-1])\n    lastRow = curRow\n  return lastRow[m]\n\n```\n\nNote: this optimization is incompatible with reconstructing the solution, because that uses the entire table as an \"oracle\".\n\n## DP Patterns\n\nHere are some typical patterns:\n\n### For Step 1. The subproblems.\n\n*   If the input is a string or a list, the subproblems are usually prefixes or substrings/sublists, which can be specified as a pair of indices.\n*   If the input is a number, the subproblems are usually smaller numbers.\n*   Generally speaking, the number of subproblems will be linear or quadratic on the input size.\n\n### For Step 2. The recurrence equation.\n\n*   Often, we use `max` or `min` to choose between options, or sum to aggregate subsolutions.\n*   The number of subproblems is most often constant, but sometimes it is linear on the subproblem size. In the latter case, we use an inner loop to aggregate/choose the best solution.\n*   Sometimes, the recurrence equation is not exactly for the original problem, but for a related but more constrained problem. See an example below, \"Longest Increasing Subsequence\".\n\n## Practice Problems\n\nHere are some practice problems showcasing the patterns mentioned above. Follow the Leetcode links for the statements and example inputs. I jump directly to the solutions. I'd recommend trying to solve the problems before checking them.\n\n*   [https://leetcode.com/problems/palindromic-substrings/](https://leetcode.com/problems/palindromic-substrings/)\n\nHere, the goal is to count the number of substrings of a string `s` which are palindromic. There is a trivial `O(n³)` time solution without DP:\n\n```python\ndef countSubstrings(s):\n  n = len(s)\n  count = 0\n  for i in range(n):\n      for j in range(i, n):\n          if isPalindrome(s[i:j+1]):\n              count += 1\n  return count\n\n```\n\nWe can improve this to `O(n²)` with DP. The subproblems are all the substrings of `s`. Let `Pal(i, j)` be true iff `s[i..j]` is a palindrome. We have the following recurrence equation (excluding base cases):\n\n```python\nPal(i, j) = false           if s[i] != s[j],\nPAl(i, j) = Pal(i+1, j-1)   otherwise\n\n```\n\nBased on this recurrence equation, we can design the following DP table:\n\n![Table for substring count](/blog/breaking-down-dynamic-programming/palindrometable.svg)\n\nThis type of \"diagonal\" DP tables are very common when the subproblems are substrings/sublists. In this case, the base cases are substrings of length 1 or 2. The goal is `Pal(0,n-1)`.\n\nGiven the dependency, the table can be filled by rows (starting from the last row), by columns (starting each column from the bottom), or by diagonals (i.e., from shortest to longest substrings). In the code below, I illustrate how to fill the table by diagonals.\n\n```python\ndef countSubstrings(s):\n    n = len(s)\n    T = [[False for j in range(n)] for i in range(n)]\n    for i in range(n):\n        T[i][i] = True\n    for i in range(n-1):\n        T[i][i+1] = s[i] == s[i+1]\n\n    for size in range(2, n+1):\n        for i in range(0,n-size):\n            j = i + size\n            T[i][j] = s[i] == s[j] and T[i+1][j-1]\n\n    count = 0\n    for row in T:\n        for val in row:\n            if val:\n                count += 1\n    return count\n\n```\n\n*   [https://leetcode.com/problems/minimum-path-sum/](https://leetcode.com/problems/minimum-path-sum/)\n\nHere, a subproblem can be a grid with reduced width and height. Let `T[i][j]` be the cheapest cost to reach cell `(i,j)`. The goal is to find `T[n-1][m-1]`, where `n` and `m` are the dimensions of the grid. The base case is when either `i` or `j` are zero, in which case we do not have any choices for how to get there. In the general case, we have the recurrence equation `T[i][j] = grid[i][j] + min(T[i-1][j], T[i][j-1])`: to get to `(i,j)`, we first need to get to either `(i-1,j)` or to `(i,j-1)`. We use `min` to choose the best of the two. We convert this into an iterative solution:\n\n```python\ndef minPathSum(grid):\n    n, m = len(grid), len(grid[0])\n    T = [[0 for j in range(m)] for i in range(n)]\n\n    T[0][0] = grid[0][0]\n    for i in range(1, n):\n        T[i][0] = grid[i][0] + T[i-1][0]\n    for j in range(1, m):\n        T[0][j] = grid[0][j] + T[0][j-1]\n\n    for i in range(1, n):\n        for j in range(1, m):\n            T[i][j] = grid[i][j] + min(T[i-1][j], T[i][j-1])\n\n    return T[n-1][m-1]\n\n```\n\n*   [https://leetcode.com/problems/unique-paths-ii/](https://leetcode.com/problems/unique-paths-ii/)\n\nThis is similar to the previous problem, but we need to accumulate the solutions to the subproblems, instead of choosing between them. Problems about _counting_ solutions can often be solved with DP.\n\n*   [https://leetcode.com/problems/longest-increasing-subsequence/](https://leetcode.com/problems/longest-increasing-subsequence/)\n\nThis problem will illustrate a new trick: if you cannot find a recurrence equation for the original problem, try to find one for a more restricted version of the problem which nevertheless you enough information to compute the original problem.\n\nHere, the input is a list `L` of numbers, and we need to find the length of the longest increasing subsequence (a subsequence does not need to be contiguous). Again, the subproblems correspond to prefixes of the list. Let `LIS(i)` be the solution for the prefix of length `i` (`L[0..i]`). The goal is to find `LIS(n-1)`, where `n` is the length of `L`. However, it is not easy to give a recurrence equation for `LIS(i)` as a function of smaller prefixes. In particular, **the following is wrong** (I will let the reader think why):\n\n```python\nLIS(i) = LIS(i-1) + 1   if L[i] > L[i-1],\nLIS(i) = LIS(i-1)       otherwise\n\n```\n\nThus, we actually give a recurrence equation for a slightly modified type of subproblems: let `LIS2(i)` be the length of the LIS **ending at index i**. This constraint makes it easier to give a recurrence equation:\n\n```python\nLIS2(i) = 1 + max(LIS2(j)) over all j < i such that L[j] < L[i]\n\n```\n\nIn short, since we know that the LIS ends at `L[i]`, we consider all candidate predecessors, which are the numbers smaller than it, and get the best one by using `max`. Crucially, this recurrence works for `LIS2(i)` but not for `LIS(i)`. Here is a full solution:\n\n```python\ndef LIS(L):\n  n = len(L)\n  T = [0 for i in range(n)]\n  T[0] = 1\n  for i in range(1, n):\n      T[i] = 1\n      for j in range(0, i):\n          if L[j] < L[i]:\n              T[i] = max(T[i], T[j] + 1)\n  return max(T)\n\n```\n\nAt the end, we do not simply return `T[n-1]` because `T` is the table for `LCS2`, not `LCS`. We return `max(T)` because the LCS must end _somewhere_, so `LCS(n-1) = LCS2(j)` for some `j < n`.\n\nNote that the runtime is `O(n²)` even though the table has linear size. This is because we take linear time per subproblem.\n\n*   [https://leetcode.com/problems/number-of-longest-increasing-subsequence/](https://leetcode.com/problems/number-of-longest-increasing-subsequence/)\n\nA harder version of the previous problem. A similar approach works. First solve the LIS problem as before, and then do a second pass to count the solutions.\n\n*   [https://leetcode.com/problems/shortest-common-supersequence/](https://leetcode.com/problems/shortest-common-supersequence/)\n\nThis problem is similar to LCS, and it requires reconstruction.\n\nI should mention that not _every_ problem that can be solved with DP fits into the mold discussed above. Despite that, it should be a useful starting framework. Here are many more practice problems:\n\n*   https://leetcode.com/tag/dynamic-programming/",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/breaking-down-dynamic-programming?category=dsa",
      "author": "Nil Mamano",
      "user_id": ""
    },
    {
      "title": "Iterative Tree Traversals: A Practical Guide",
      "content": "A guide to implementing preorder, inorder, and postorder tree traversals iteratively.\n\n# Iterative Tree Traversals: A Practical Guide\n\nFebruary 4, 2020\n\n![Iterative Tree Traversals: A Practical Guide](/blog/iterative-tree-traversals/cover.png)\n\n## Introduction\n\nI don't know how often tree traversals come up in actual software projects, but they are popular in coding interviews and competitive programming. In this article, I share an approach for implementing tree traversal algorithms iteratively that I found to be simple to remember and implement, while being flexible enough to do anything that a recursive algorithm can (I also didn't like most suggestions I saw online). The main technique is given in section [\"Iterative Postorder and Inorder Traversal\"](#iterative-postorder-and-inorder-traversal), but first I give some context. I also link to practice problems on [leetcode.com](https://leetcode.com) for the reader to play with. I provide some solutions, but I suggest trying the problems out first. The code snippets are in C++, but leetcode accepts most languages.\n\n## What are Tree Traversals\n\nMathematically, trees are just connected acyclic graphs. However, in the context of tree traversals, we are usually working with **rooted trees** represented with a recursive structure such as the following (which is the default definition in Leetcode for binary trees). A leaf is a node with two null pointers as children:\n\n```cpp\nstruct TreeNode {\n  int val;\n  TreeNode *left;\n  TreeNode *right;\n  TreeNode(int x) : val(x), left(NULL), right(NULL) {}\n};\n\n```\n\nA tree traversal is an algorithm that visits every node in a tree in a specific order (and does some computation with them, depending on the problem). For binary trees specifically, there are three important orders:\n\n*   **Preorder:** root before children. As we will see, this is the simplest to implement.\n*   **Inorder:** left child, then root, then right child. This traversal is most often used on _binary search trees_ (BST). A BST is a rooted binary tree with the additional property that every node in the left subtree has a smaller value than the root, and every node in the right subtree has a larger value than the root. This traversal is called \"inorder\" because, when used on a BST, it will visit the nodes from smallest to largest.\n*   **Postorder:** children before root. It comes up in problems where we have to aggregate information about the entire subtree rooted at each node. Classic examples are computing the size, the height, or the sum of values of the tree.\n\n![Tree traversals](/blog/iterative-tree-traversals/traversals.svg)\n\nBecause rooted trees are recursive data structures, algorithms on trees are most naturally expressed recursively. Here are the three traversals. I use the function `process(node)` as a placeholder for whatever computation the problem calls for.\n\n```cpp\nvoid preorderTraversal(TreeNode* root) {\n  if (!root) return;\n  process(root);\n  preorderTraversal(root->left);\n  preorderTraversal(root->right);\n}\n\nvoid inorderTraversal(TreeNode* root) {\n  if (!root) return;\n  inorderTraversal(root->left);\n  process(root);\n  inorderTraversal(root->right);\n}\n\nvoid postorderTraversal(TreeNode* root) {\n  if (!root) return;\n  postorderTraversal(root->left);\n  postorderTraversal(root->right);\n  process(root);\n}\n\n```\n\nSide-note: in C++, pointers are implicitly converted to booleans: a pointer evaluates to true if and only if it is not null. So, in the code above, \"`if (!root)`\" is equivalent to \"`if (root == NULL)`\".\n\n### Traversal problems on leetcode\n\n*   [https://leetcode.com/problems/binary-tree-preorder-traversal/](https://leetcode.com/problems/binary-tree-preorder-traversal/)\n*   [https://leetcode.com/problems/binary-tree-inorder-traversal/](https://leetcode.com/problems/binary-tree-inorder-traversal/)\n*   [https://leetcode.com/problems/binary-tree-postorder-traversal/](https://leetcode.com/problems/binary-tree-postorder-traversal/)\n\n## Why / When to Use an Iterative Traversal\n\nIf the recursive implementation is so simple, why bother with an iterative one? Of course, to avoid stack overflow. Most runtime engines/compilers set a limit on how many nested calls a program can make. For example, according to [this article](https://freecontent.manning.com/stack-safe-recursion-in-java/):\n\n> _Default stack size varies between 320k and 1024k depending on the version of Java and the system used. For a 64 bits Java 8 program with minimal stack usage, the maximum number of nested method calls is about 7000._\n\nIf the height of the tree is larger than this limit, the program will crash with a **stack overflow error**. A recursive implementation is safe to use if:\n\n*   Somehow we know that the input trees will be small enough.\n*   The tree is _balanced_, which means that, for each node, the left and right subtrees have roughly the same height. In a balanced tree, the height is guaranteed to be _logarithmic_ on the number of nodes (indeed, that is why balanced BSTs guarantee _O(log n)_ search time), so any tree that fits in RAM (or even disk) will require a tiny number of recursive calls.\n\nHowever, if we are not in either of the cases above, an iterative solution is safer.\n\nRecursive and iterative traversals have the same runtime complexity, so this is not a concern when choosing either (all the problems shown in this article can be solved in linear time using either).\n\nThe main approach for converting recursive implementations to iterative ones is to \"simulate\" the call stack with an actual stack where we push and pop the nodes explicitly. This works great \"out-of-the-box\" with preorder traversal.\n\nIncidentally, when implementing tree traversals we need to make an implementation choice about how to handle NULL pointers. We can be eager and filter them out before adding them to the stack, or we can be lazy and detect them once we extract them from the stack. Both are fine—what matters is to be deliberate and consistent about which approach we are using. I prefer the latter as it yields slightly shorter code, so I will use it in all the following examples. For comparison, here is the iterative preorder traversal with both approaches:\n\n```cpp\n//eager NULL checking\nvoid preorderTraversal(TreeNode* root) {\n  stack<TreeNode*> stk;\n  if (!root) return;\n  stk.push(root);\n  while (!stk.empty()) {\n    TreeNode* node = stk.top();\n    stk.pop();\n    process(node);\n    if (node->right) stk.push(node->right);\n    if (node->left) stk.push(node->left);\n  }\n}\n\n//lazy NULL checking\nvoid preorderTraversal(TreeNode* root) {\n  stack<TreeNode*> stk;\n  stk.push(root);\n  while (!stk.empty()) {\n    TreeNode* node = stk.top();\n    stk.pop();\n    if (!node) continue;\n    process(node);\n    stk.push(node->right);\n    stk.push(node->left);\n  }\n}\n\n```\n\nNote that **the right child is pushed to the stack before the left one**. This is because we want the left child to be above in the stack so that it is processed first.\n\n### Preorder traversal practice problems\n\n*   [https://leetcode.com/problems/invert-binary-tree/](https://leetcode.com/problems/invert-binary-tree/)\n*   [https://leetcode.com/problems/maximum-depth-of-binary-tree/](https://leetcode.com/problems/maximum-depth-of-binary-tree/)\n\nThis problem asks to find the depth of a binary tree (follow the link for the description and examples). It requires passing information from each node to its children. We can do this by changing the stack to `stack<pair<TreeNode*, int>>`, so that we can pass an `int` to each child, as in the solution below:\n\n```cpp\nint maxDepth(TreeNode* root) {\n    int res = 0;\n    stack<pair<TreeNode*, int>> stk;\n    stk.push({root, 1}); //node, depth\n    while (!stk.empty()) {\n        auto node = stk.top().first;\n        int depth = stk.top().second;\n        stk.pop();\n        if (!node) continue;\n        res = max(res, depth);\n        stk.push({node->left, depth+1});\n        stk.push({node->right, depth+1});\n    }\n    return res;\n}\n\n```\n\nIn the code above, the `{}` notation is used to create pairs (e.g., `{root, 0}`). If one is not familiar with pairs in C++, or is using a language without the equivalent, a simple alternative is to use two separate stacks, one for the nodes and one for the info.\n\nThe next two problems are similar:\n\n*   [https://leetcode.com/problems/minimum-depth-of-binary-tree/](https://leetcode.com/problems/minimum-depth-of-binary-tree/)\n*   [https://leetcode.com/problems/path-sum/](https://leetcode.com/problems/path-sum/)\n*   [https://leetcode.com/problems/symmetric-tree/](https://leetcode.com/problems/symmetric-tree/)\n\nA solution for the last one, this time using a stack with a pair of nodes:\n\n```cpp\nbool isSymmetric(TreeNode* root) {\n    if (!root) return true;\n    stack<pair<TreeNode*, TreeNode*>> stk;\n    stk.push({root->left, root->right});\n    while (!stk.empty()) {\n        TreeNode* l = stk.top().first;\n        TreeNode* r = stk.top().second;\n        stk.pop();\n        if (!l and !r) continue;\n        if (!l or !r or l->val != r->val) return false;\n        stk.push({l->left, r->right});\n        stk.push({l->right, r->left});\n    }\n    return true;\n}\n\n```\n\n## Iterative Postorder and Inorder Traversal\n\nWhile iterative preorder traversal is straightforward, with postorder and inorder we run into a complication: we cannot simply swap the order of the lines as with the recursive implementation. In other words, the following does _not_ yield a postorder traversal:\n\n```cpp\n...\nstk.push(node->right);\nstk.push(node->left);\nprocess(node);\n...\n\n```\n\nThe node is still processed before its children, which is not what we want.\n\n**The workaround, once again emulating the recursive implementation, is to visit each node twice.** We consider postorder traversal first. In the first visit, we only push the children onto the stack. In the second visit, we do the actual processing. The simplest way to do this is to enhance the stack with a **\"visit number flag\"**. Implementation-wise, we change the stack to `stack<pair<TreeNode*, int>>` so that we can pass the flag along with each node. The iterative postorder looks like this:\n\n```cpp\nvoid postorderTraversal(TreeNode* root) {\n  stack<pair<TreeNode*,int>> stk; //node, visit #\n  stk.push({root, 0});\n  while (!stk.empty()) {\n    TreeNode* node = stk.top().first;\n    int visit = stk.top().second;\n    stk.pop();\n    if (!node) continue;\n    if (visit == 0) {\n      stk.push({node, 1});\n      stk.push({node->right, 0});\n      stk.push({node->left, 0});\n    } else { //visit == 1\n      process(node);\n    }\n  }\n}\n\n```\n\nNote the order in which the nodes are added to the stack when `visit == 0`. The parent ends up under its children, with the left child on top. Since it is the first time that the children are added to the stack, their visit-number flag is 0. For the parent, it is 1. For simplicity, I also follow the convention to always immediately call pop after extracting the top element from the stack.\n\nThe same approach also works for inorder traversal (that's the point). Here is a version where we visit each node three times: one to push the left child, one to process the node, and one to push the right child.\n\n```cpp\n//3-visit version\nvoid inorderTraversal(TreeNode* root) {\n  stack<pair<TreeNode*,int>> stk;\n  stk.push({root, 0});\n  while (!stk.empty()) {\n    TreeNode* node = stk.top().first;\n    int visit = stk.top().second;\n    stk.pop();\n    if (!node) continue;\n    if (visit == 0) {\n      stk.push({node, 1});\n      stk.push({node->left, 0});\n    } else if (visit == 1) {\n      stk.push({node, 2});\n      process(node);\n    } else { //visit == 2\n      stk.push({node->right, 0});\n    }\n  }\n}\n\n```\n\nIn fact, the second and third visits can be merged together: processing the node does not modify the stack, so the two visits are followed one after the other anyway. Here is my preferred version:\n\n```cpp\n//2-visit version\nvoid inorderTraversal(TreeNode* root) {\n  stack<pair<TreeNode*,int>> stk;\n  stk.push({root, 0});\n  while (!stk.empty()) {\n    TreeNode* node = stk.top().first;\n    int visit = stk.top().second;\n    stk.pop();\n    if (!node) continue;\n    if (visit == 0) {\n      stk.push({node, 1});\n      stk.push({node->left, 0});\n    } else { //visit == 1\n      process(node);\n      stk.push({node->right, 0});\n    }\n  }\n}\n\n```\n\nFor completeness, here is the version found in most of my top Google hits (see [this](https://www.techiedelight.com/inorder-tree-traversal-iterative-recursive/) for a nice explanation):\n\n```cpp\nvoid inorderTraversal(TreeNode* root) {\n    stack<TreeNode*> stk;\n    TreeNode* curr = root;\n    while (curr or !stk.empty()) {\n        while (curr) {\n            stk.push(curr);\n            curr = curr->left;\n        }\n        curr = stk.top();\n        stk.pop();\n        process(curr);\n        curr = curr->right;\n    }\n}\n\n```\n\nWhile it is shorter, it cannot be easily converted to postorder traversal, so it is not as flexible. Also, I find it easier to follow the execution flow with the visit-number flag.\n\n### Inorder traversal practice problems\n\n*   [https://leetcode.com/problems/kth-smallest-element-in-a-bst/](https://leetcode.com/problems/kth-smallest-element-in-a-bst/)\n\nA solution (follow the link for the statement and examples):\n\n```cpp\nint kthSmallest(TreeNode* root, int k) {\n    int count = 1;\n    stack<pair<TreeNode*, int>> stk;\n    stk.push({root, 0});\n    while (!stk.empty()) {\n        auto node = stk.top().first;\n        int visit = stk.top().second;\n        stk.pop();\n        if (!node) continue;\n        if (visit == 0) {\n            stk.push({node, 1});\n            stk.push({node->left, 0});\n        } else { //visit == 1\n            if (count == k) return node->val;\n            count++;\n            stk.push({node->right, 0});\n        }\n    }\n    return -1;\n}\n\n```\n\n*   [https://leetcode.com/problems/validate-binary-search-tree/](https://leetcode.com/problems/validate-binary-search-tree/)\n\nA solution:\n\n```cpp\nbool isValidBST(TreeNode* root) {\n    int lastVal;\n    bool init = false;\n\n    stack<pair<TreeNode*, int>> stk;\n    stk.push({root, 0});\n    while (!stk.empty()) {\n        TreeNode* node = stk.top().first;\n        int visit = stk.top().second;\n        stk.pop();\n        if (!node) continue;\n        if (visit == 0) {\n            stk.push({node, 1});\n            stk.push({node->left, 0});\n        } else { //second visit\n            if (!init) {\n                init = true;\n                lastVal = node->val;\n            } else {\n                if (node->val <= lastVal) return false;\n                lastVal = node->val;\n            }\n            stk.push({node->right, 0});\n        }\n    }\n    return true;\n}\n\n```\n\n### Postorder traversal practice problems\n\n*   [https://leetcode.com/problems/balanced-binary-tree/](https://leetcode.com/problems/balanced-binary-tree/)\n\nThis problem asks to check if a binary tree is balanced. It requires passing information back from the children to the parent node in a postorder traversal. Passing information from the children to the parent is easy with recursion. It can be done both with return values or with parameters passed by reference. For this problem we need to pass two things: a `bool` indicating if the subtree is balanced, and an `int` indicating its height. I use a reference parameter for the latter (returning a `pair<bool,int>` would be cleaner).\n\n```cpp\nbool isBalancedRec(TreeNode* root, int& height) {\n  if (!root) {\n    height = 0;\n    return true;\n  }\n  int lHeight, rHeight;\n  bool lBal = isBalancedRec(root->left, lHeight);\n  bool rBal = isBalancedRec(root->right, rHeight);\n  height = max(lHeight, rHeight) + 1;\n  return lBal && rBal && abs(lHeight - rHeight) <= 1;\n}\n\nbool isBalanced(TreeNode* root) {\n  int height;\n  return isBalancedRec(root, height);\n}\n\n```\n\nPassing information from the children to the parent in an iterative implementation is more intricate. There are three general approaches:\n\n1.  Use a hash table mapping each node to the information.\n\nThis is the easiest way, but also the most expensive. While the asymptotic runtime is still linear, hash tables generally have significant constant factors.\n\n```cpp\nbool isBalanced(TreeNode* root) {\n    stack<pair<TreeNode*, int>> stk;\n    stk.push({root, 0});\n\n    unordered_map<TreeNode*, int> height;\n    height[NULL] = 0;\n\n    while (!stk.empty()) {\n        TreeNode* node = stk.top().first;\n        int visit = stk.top().second;\n        stk.pop();\n        if (!node) continue;\n        if (visit == 0) {\n            stk.push({node, 1});\n            stk.push({node->right, 0});\n            stk.push({node->left, 0});\n        } else { // visit == 1\n            int lHeight = height[node->left], rHeight = height[node->right];\n            if (abs(lHeight - rHeight) > 1) return false;\n            height[node] = max(lHeight, rHeight) + 1;\n        }\n    }\n    return true;\n}\n\n```\n\n2.  Add a field to the definition of the node structure for the information needed.\n\nThen, we can read it from the parent node by traversing the children's pointers. In Leetcode we cannot modify the `TreeNode` data structure so, to illustrate this approach, I build a new tree first with a new struct:\n\n```cpp\nstruct MyNode {\n    int val;\n    int height;\n    MyNode *left;\n    MyNode *right;\n    MyNode(TreeNode* node): val(node->val), height(-1), left(NULL), right(NULL) {\n        if (node->left) left = new MyNode(node->left);\n        if (node->right) right = new MyNode(node->right);\n    }\n};\n\nbool isBalanced(TreeNode* root) {\n    if (!root) return true;\n    MyNode* myRoot = new MyNode(root);\n    stack<pair<MyNode*, int>> stk;\n    stk.push({myRoot, 0});\n    while (!stk.empty()) {\n        MyNode* node = stk.top().first;\n        int visit = stk.top().second;\n        stk.pop();\n        if (!node) continue;\n        if (visit == 0) {\n            stk.push({node, 1});\n            stk.push({node->right, 0});\n            stk.push({node->left, 0});\n        } else { // visit == 1\n            int lHeight = 0, rHeight = 0;\n            if (node->left) lHeight = node->left->height;\n            if (node->right) rHeight = node->right->height;\n            if (abs(lHeight - rHeight) > 1) return false;\n            node->height = max(lHeight, rHeight) + 1;\n        }\n    }\n    return true;\n}\n\n```\n\n3.  Pass the information through an additional stack.\n\nThis is the most efficient, but one must be careful to keep both stacks in synch. When processing a node, that node first pops the information from its children, and then pushes its own info for its parent. Here is a solution (with eager NULL-pointer detection):\n\n```cpp\nbool isBalanced(TreeNode* root) {\n    if (!root) return true;\n    stack<pair<TreeNode*, int>> stk;\n    stk.push({root, 0});\n\n    stack<int> heights;\n\n    while (!stk.empty()) {\n        TreeNode* node = stk.top().first;\n        int visit = stk.top().second;\n        stk.pop();\n        if (visit == 0) {\n            stk.push({node, 1});\n            if (node->right) stk.push({node->right, 0});\n            if (node->left) stk.push({node->left, 0});\n        } else { // visit == 1\n            int rHeight = 0, lHeight = 0;\n            if (node->right) {\n                rHeight = heights.top();\n                heights.pop();\n            }\n            if (node->left) {\n                lHeight = heights.top();\n                heights.pop();\n            }\n            if (abs(lHeight - rHeight) > 1) return false;\n            heights.push(max(lHeight, rHeight) + 1);\n        }\n    }\n    return true;\n}\n\n```\n\n*   [https://leetcode.com/problems/diameter-of-binary-tree/](https://leetcode.com/problems/diameter-of-binary-tree/)\n\nThis problem also requires passing information from the children to the parent in a postorder traversal. Here is a solution using the third approach again, but this time with lazy NULL-pointer detection. Note that we push a 0 to the `depths` stack when we extract a NULL pointer from the main stack, and during processing we always do two pops regardless of the number of non-NULL children:\n\n```cpp\nint diameterOfBinaryTree(TreeNode* root) {\n    stack<pair<TreeNode*,int>> stk;\n    stk.push({root, 0});\n\n    stack<int> depths;\n    int res = 0;\n\n    while (!stk.empty()) {\n        TreeNode* node = stk.top().first;\n        int visit = stk.top().second;\n        stk.pop();\n        if (!node) {\n            depths.push(0);\n            continue;\n        }\n        if (visit == 0) {\n            stk.push({node, 1});\n            stk.push({node->right, 0});\n            stk.push({node->left, 0});\n        } else { //visit == 1\n            int rDepth = depths.top();\n            depths.pop();\n            int lDepth = depths.top();\n            depths.pop();\n            int depth = max(lDepth, rDepth) + 1;\n            depths.push(depth);\n            int dia = lDepth + rDepth;\n            res = max(res, dia);\n        }\n    }\n    return res;\n}\n\n```\n\n*   [https://leetcode.com/problems/binary-tree-tilt/](https://leetcode.com/problems/binary-tree-tilt/)\n*   [https://leetcode.com/problems/most-frequent-subtree-sum/](https://leetcode.com/problems/most-frequent-subtree-sum/)\n*   [https://leetcode.com/problems/maximum-product-of-splitted-binary-tree/](https://leetcode.com/problems/maximum-product-of-splitted-binary-tree/)\n\n## Traversals in n-ary Trees\n\nSo far, we have looked at binary trees. In an n-ary tree, each node has an arbitrary number of children.\n\n```cpp\nstruct Node {\n    int val;\n    vector<Node*> children;\n    Node(int val): val(val), children(0) {}\n};\n\n```\n\nFor n-ary trees, preorder traversal is also straightforward, and inorder traversal is not defined.\n\nFor postorder traversal, we can use a visit-number flag again. Two visits suffice for each node: one to push all the children into the stack, and another to process the node itself. I do not include the code here because it is very similar to the binary tree case.\n\nConsider a more complicated setting where we need to compute something at the node after visiting each child. Let's call this \"interleaved traversal\". I use `process(node, i)` as placeholder for the computation done before visiting the i-th child. Here is the recursive implementation and the corresponding iterative one using visit-number flags.\n\n```cpp\n//recursive\nvoid interleavedTraversal(Node* root) {\n  if (!root) return;\n  int n = root->children.size();\n  for (int i = 0; i < n; i++) {\n    process(root, i);\n    interleavedTraversal(root->children[i]);\n  }\n}\n\n//iterative\nvoid interleavedTraversal(Node* root) {\n  stack<pair<TreeNode*, int>> stk;\n  stk.push({root, 0});\n  while (!stk.empty()) {\n    TreeNode* node = stk.top().first;\n    int visit = stk.top().second;\n    stk.pop();\n    if (!node) continue;\n    int n = node->children.size();\n    if (visit < n) {\n      stk.push({node, visit+1});\n      process(node, visit);\n      stk.push({node->children[visit], 0});\n    }\n  }\n}\n\n```\n\n### N-ary tree practice problems\n\n*   [https://leetcode.com/problems/n-ary-tree-preorder-traversal/](https://leetcode.com/problems/n-ary-tree-preorder-traversal/)\n*   [https://leetcode.com/problems/n-ary-tree-postorder-traversal/](https://leetcode.com/problems/n-ary-tree-postorder-traversal/)\n\n## An Alternative Way of Passing the Visit Flag\n\nThe common framework to all our solutions has been to pass a visit-number flag along with the nodes on the stack. User \"heiswyd\" on leetcode posted [here](https://leetcode.com/problems/binary-tree-postorder-traversal/discuss/45582/A-real-Postorder-Traversal-.without-reverse-or-insert-4ms) an alternative way to pass the flag implicitly: initially, it pushes each node on the stack twice. Then, it can distinguish between the first visit and the second visit by checking whether the node that has just been extracted from the stack matches the node on top of the stack. This happens only when we extract the first of the two occurrences. Post-order traversal looks like this:\n\n```cpp\nvoid postorderTraversal(TreeNode* root) {\n  stack<TreeNode*> stk;\n  stk.push(root);\n  stk.push(root);\n  while (!stk.empty()) {\n    TreeNode* node = stk.top();\n    stk.pop();\n    if (!node) continue;\n    if (!stk.empty() and stk.top() == node) {\n      stk.push(node->right);\n      stk.push(node->right);\n      stk.push(node->left);\n      stk.push(node->left);\n    } else {\n      process(node);\n    }\n  }\n}\n\n```\n\nIt is cool, but I prefer passing the flag explicitly for clarity.",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/iterativetreetraversal?category=dsa",
      "author": "Nil Mamano",
      "user_id": ""
    }
  ]
}